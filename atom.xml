<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>watson&#39;blogs</title>
  
  
  <link href="https://watsonlu6.github.io/atom.xml" rel="self"/>
  
  <link href="https://watsonlu6.github.io/"/>
  <updated>2024-07-13T15:22:50.436Z</updated>
  <id>https://watsonlu6.github.io/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>云存储概述</title>
    <link href="https://watsonlu6.github.io/%E4%BA%91%E5%AD%98%E5%82%A8%E6%A6%82%E8%BF%B0/"/>
    <id>https://watsonlu6.github.io/%E4%BA%91%E5%AD%98%E5%82%A8%E6%A6%82%E8%BF%B0/</id>
    <published>2024-07-13T15:09:26.000Z</published>
    <updated>2024-07-13T15:22:50.436Z</updated>
    
    <content type="html"><![CDATA[<h2 id="云存储底层技术"><a href="#云存储底层技术" class="headerlink" title="云存储底层技术"></a>云存储底层技术</h2><p><img src="/images/%E4%BA%91%E5%AD%98%E5%82%A8%E5%BA%95%E5%B1%82%E6%8A%80%E6%9C%AF.png"></p><h4 id="云存储的概述"><a href="#云存储的概述" class="headerlink" title="云存储的概述"></a>云存储的概述</h4><p>云存储是指通过网络，将分布在不同地方的多种存储设备，通过应用软件集合起来共同对外提供数据存储和业务访问功能的一个系统。云存储是云计算系统中的一种新型网络存储技术。云存储对外提供的存储能力以统一、简单的数据服务接口来提供和展现。用户不用关心数据具体存放在哪个设备、哪个区域，甚至不知道数据到底是怎么保存的，他们只需要关心需要多少存储空间、什么时间能够拿到数据、数据存放的安全性和可用性如何即可。</p><h4 id="云存储的实现模式"><a href="#云存储的实现模式" class="headerlink" title="云存储的实现模式"></a>云存储的实现模式</h4><p>云存储的实现模式有多种，云存储的架构可以由传统的存储架构延伸而来，也可以采用全新的云计算架构。云存储的实现可以是软件的，也可以是硬件的。云存储的实现模式可以是块存储、文件存储和对象存储。</p><ul><li><p><strong>块存储</strong></p><ul><li>块存储：最接近底层的存储，可以对数据进行任意格式化操作，可以在上面运行数据库等性能要求高的应用。可以给虚拟机使用。</li></ul></li><li><p><strong>文件存储</strong></p><ul><li>文件存储：对数据以文件的形式进行存储，支持复杂的文件操作，适合共享文件和协作工作。典型应用场景：NAS。</li></ul></li><li><p><strong>对象存储</strong></p><ul><li>对象存储：将数据以对象形式存储，通过唯一的对象ID进行访问，适合存储大规模非结构化数据，支持RESTful API接口。典型应用场景：云存储服务，视频、图片存储。</li></ul></li></ul><h4 id="为什么需要多元化存储？"><a href="#为什么需要多元化存储？" class="headerlink" title="为什么需要多元化存储？"></a>为什么需要多元化存储？</h4><p>由于不同的应用场景对存储的需求不同，单一的存储类型无法满足所有需求。大规模存储系统需要支持多种存储类型和多种存储协议，比如NFS、iSCSI、HDFS、S3等。多元化存储可以更好地适应各种应用场景，提高存储系统的灵活性和适应性。</p><h4 id="文件如何通过分布式存储在许多服务器中"><a href="#文件如何通过分布式存储在许多服务器中" class="headerlink" title="文件如何通过分布式存储在许多服务器中"></a>文件如何通过分布式存储在许多服务器中</h4><p>分布式存储系统将数据分散存储在多个物理设备上。文件被切分成多个小块，存储在不同的服务器上。通过分布式哈希表（DHT）等算法确定数据块的位置，实现数据的快速定位和访问。通过数据复制和纠删码技术提高数据的可靠性和可用性。</p><h4 id="文件被读取时如何快速找到数据块，确保大目标的组合数据不会丢失？"><a href="#文件被读取时如何快速找到数据块，确保大目标的组合数据不会丢失？" class="headerlink" title="文件被读取时如何快速找到数据块，确保大目标的组合数据不会丢失？"></a>文件被读取时如何快速找到数据块，确保大目标的组合数据不会丢失？</h4><p>元数据服务器（MDS）存储文件系统的元数据，包括文件名、文件大小、数据块位置等信息。客户端请求文件时，首先查询MDS获取元数据，然后根据元数据访问对应的数据块。分布式文件系统中常用的元数据管理技术包括分布式哈希表（DHT）、目录树、名称节点（NameNode）等。</p><h4 id="如果文件丢失怎么办？"><a href="#如果文件丢失怎么办？" class="headerlink" title="如果文件丢失怎么办？"></a>如果文件丢失怎么办？</h4><p>由于分布式存储系统的特性，单一数据副本的丢失不会导致数据不可恢复。分布式存储系统采用数据冗余和副本机制，常见的冗余技术包括数据复制和纠删码。数据复制是将同一份数据存储在多个节点上，副本数通常为3个或更多。纠删码是一种冗余编码技术，通过增加校验数据，在数据块丢失的情况下，可以通过校验数据恢复原始数据。分布式存储系统在后台自动检测数据块的健康状态，发现数据丢失或损坏时，自动启动数据恢复机制，确保数据的完整性和可用性。</p><h4 id="存储系统的数据可靠性（就像RAID）以及可用性（如高可用性）是如何解决的？"><a href="#存储系统的数据可靠性（就像RAID）以及可用性（如高可用性）是如何解决的？" class="headerlink" title="存储系统的数据可靠性（就像RAID）以及可用性（如高可用性）是如何解决的？"></a>存储系统的数据可靠性（就像RAID）以及可用性（如高可用性）是如何解决的？</h4><p>存储系统的数据可靠性和可用性通过多种技术手段来保证。RAID技术通过数据条带化、镜像、奇偶校验等方法，提高单一存储设备的数据可靠性。分布式存储系统通过数据复制和纠删码技术，在多个节点上存储数据副本，提高数据的可靠性和可用性。高可用性通过冗余设计实现，常见的高可用架构包括双机热备、集群等。通过负载均衡技术，将用户请求分散到多个节点上，提高系统的可用性。</p><h4 id="写入的数据是如何被保护的？"><a href="#写入的数据是如何被保护的？" class="headerlink" title="写入的数据是如何被保护的？"></a>写入的数据是如何被保护的？</h4><p>数据写入时，采用多副本机制，确保数据的一致性和可靠性。写时复制（Copy-On-Write，COW）是一种常见的技术，通过在写入数据前复制一份旧数据，确保数据写入过程中的一致性。在分布式存储系统中，数据写入时，通常会先写入多个副本，只有所有副本写入成功后，才算写入成功。数据写入过程中的故障检测和处理机制，确保数据的可靠性和一致性。</p><h4 id="多人多设备协作时，如何保证远程协作时数据的一致性？"><a href="#多人多设备协作时，如何保证远程协作时数据的一致性？" class="headerlink" title="多人多设备协作时，如何保证远程协作时数据的一致性？"></a>多人多设备协作时，如何保证远程协作时数据的一致性？</h4><p>分布式存储系统通过分布式一致性协议（如Paxos、Raft）确保数据的一致性。在多个节点之间进行数据写入时，一致性协议保证数据的一致性和正确性。冲突检测和处理机制，在多人协作时，检测并解决数据冲突。分布式锁和事务机制，确保数据的一致性和完整性。</p><h4 id="节省存储空间"><a href="#节省存储空间" class="headerlink" title="节省存储空间"></a>节省存储空间</h4><p>存储系统采用数据压缩和数据去重技术，减少存储空间占用。数据压缩通过减少数据的冗余，提高存储空间的利用率。数据去重通过检测和删除重复数据，节省存储空间。在大规模存储系统中，数据压缩和去重技术可以显著降低存储成本，提高存储效率。</p><h4 id="避免存储固定的文件"><a href="#避免存储固定的文件" class="headerlink" title="避免存储固定的文件"></a>避免存储固定的文件</h4><p>存储系统采用分级存储和冷热数据分离策略，提高存储资源的利用率。根据数据的访问频率和重要性，将数据存储在不同的存储介质上。频繁访问的数据存储在高速存储设备上，减少访问延迟。较少访问的数据存储在低成本存储设备上，降低存储成本。通过冷热数据分离，优化存储资源的使用，提高存储系统的性能和效率。</p><h4 id="IO速度要有保证"><a href="#IO速度要有保证" class="headerlink" title="IO速度要有保证"></a>IO速度要有保证</h4><p>存储系统通过多种技术手段保证IO速度。使用高速缓存技术，将热点数据缓存到内存或SSD中，减少数据访问延迟。采用预取技术，在数据请求到达前提前加载数据，提高数据访问速度。使用QoS（Quality of Service）技术，为不同的应用场景和用户提供不同的IO优先级和带宽保障。负载均衡技术，将IO请求分散到多个存储节点上，避免单点瓶颈，提高IO性能。</p><h4 id="版本控制"><a href="#版本控制" class="headerlink" title="版本控制"></a>版本控制</h4><p>存储系统提供版本控制功能，允许用户对数据进行版本管理。在数据修改前，保存一份旧版本的数据，用户可以根据需要回滚到旧版本。版本控制功能确保数据的可追溯性和可恢复性，防止数据丢失和误操作。在分布式存储系统中，版本控制功能通过元数据管理和数据快照技术实现。元数据管理记录数据的版本信息和变更历史，数据快照技术保存数据的不同版本。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;云存储底层技术&quot;&gt;&lt;a href=&quot;#云存储底层技术&quot; class=&quot;headerlink&quot; title=&quot;云存储底层技术&quot;&gt;&lt;/a&gt;云存储底层技术&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/images/%E4%BA%91%E5%AD%98%E5%82%A8%E5%B</summary>
      
    
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="存储基础" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="存储基础" scheme="https://watsonlu6.github.io/tags/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>Linux存储栈</title>
    <link href="https://watsonlu6.github.io/Linux%E5%AD%98%E5%82%A8%E6%A0%88/"/>
    <id>https://watsonlu6.github.io/Linux%E5%AD%98%E5%82%A8%E6%A0%88/</id>
    <published>2024-07-13T12:24:56.000Z</published>
    <updated>2024-07-13T15:23:16.140Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Linux存储栈"><a href="#Linux存储栈" class="headerlink" title="Linux存储栈"></a>Linux存储栈</h1><p><img src="/images/linux_storage_stack.png"><br>Linux存储系统包括用户接口和存储设备接口两个部分，前者以流形式处理数据，后者以块形式处理数据，文件系统在中间起到承上启下的作用。应用程序通过系统调用发出写请求，文件系统定位请求位置并转换成块设备所需的块，然后发送到设备上。内存在此过程中作为磁盘缓冲，将上下两部分隔离成异步运行的两个过程，避免频繁的磁盘同步。当数据需要从页面缓存同步到磁盘时，请求被包装成包含多个bio的request，每个bio包含需要同步的数据页。磁盘在执行写操作时，需要通过IO请求调度合理安排顺序，减少磁头的频繁移动，提高磁盘性能。</p><ol><li><h5 id="用户视角的数据流接口"><a href="#用户视角的数据流接口" class="headerlink" title="用户视角的数据流接口"></a>用户视角的数据流接口</h5><p> 应用程序通过系统调用（如<code>write</code>、<code>read</code>等）与操作系统交互。这些调用使得数据以流的形式被处理。</p></li><li><h5 id="存储设备的块接口"><a href="#存储设备的块接口" class="headerlink" title="存储设备的块接口"></a>存储设备的块接口</h5><p> 数据在底层存储设备（如硬盘、SSD等）中以块（通常是512字节或4096字节）为单位进行读写操作。</p></li><li><h5 id="文件系统的中间角色"><a href="#文件系统的中间角色" class="headerlink" title="文件系统的中间角色"></a>文件系统的中间角色</h5><ul><li><strong>位置定位</strong>：文件系统负责将用户的读写请求定位到存储设备的具体块位置。</li><li><strong>数据转换</strong>：将数据流转换为存储设备所需的块结构，并将这些块组织成<code>bio</code>（block I&#x2F;O）请求。</li></ul></li><li><h5 id="内存作为缓冲"><a href="#内存作为缓冲" class="headerlink" title="内存作为缓冲"></a>内存作为缓冲</h5><ul><li><strong>页面缓存</strong>：内存中的页面缓存（Page Cache）用于暂时存储数据，以减少频繁的磁盘I&#x2F;O操作。</li><li><strong>异步运行</strong>：将用户操作与底层存储设备的实际写操作异步化，提升系统效率。对于用户态程序来说，数据尽量保留在内存中，这样可以减少频繁的数据同步。</li></ul></li><li><h5 id="I-O请求调度"><a href="#I-O请求调度" class="headerlink" title="I&#x2F;O请求调度"></a>I&#x2F;O请求调度</h5><ul><li><strong>请求封装</strong>：从页面缓存同步到磁盘的请求被封装成<code>request</code>，每个<code>request</code>包含多个<code>bio</code>，而每个<code>bio</code>又包含具体的数据页。</li><li><strong>调度策略</strong>：操作系统会对I&#x2F;O请求进行调度，优化执行顺序，尽量减少磁盘磁头的来回移动，提高磁盘的读写效率。</li></ul></li></ol><h5 id="Linux数据写入流程"><a href="#Linux数据写入流程" class="headerlink" title="Linux数据写入流程"></a>Linux数据写入流程</h5><ol><li><strong>应用程序发出写请求</strong>：比如，应用程序通过<code>write</code>系统调用写入数据。</li><li><strong>文件系统处理</strong>：文件系统接收请求，找到对应的文件位置，将数据写入页面缓存。</li><li><strong>内存缓冲处理</strong>：数据暂存在内存的页面缓存中，以等待后续的写入操作。</li><li><strong>请求调度与封装</strong>：页面缓存的数据需要同步到磁盘时，被封装成<code>bio</code>和<code>request</code>。</li><li><strong>I&#x2F;O调度执行</strong>：调度器优化I&#x2F;O请求的执行顺序，减少磁头移动，提高写入效率。</li><li><strong>数据写入磁盘</strong>：最终，数据从页面缓存同步到磁盘的指定位置，完成写操作。<br>通过以上流程，Linux存储系统在保证数据一致性的同时，最大限度地提高了性能和效率。</li></ol><h2 id="系统调用"><a href="#系统调用" class="headerlink" title="系统调用"></a>系统调用</h2><p><img src="/images/linux_storage_stack_1.png"><br>&emsp;&emsp;用户应用程序访问并使用内核所提供的各种服务的途径即是系统调用。在内核和应用程序交叉的地方，内核提供了一组系统调用接口，通过这组接口，应用程序可以访问系统硬件和各种操作系统资源。用户可以通过文件系统相关的调用请求系统打开问价、关闭文件和读写文件。<br>&emsp;&emsp;内核提供的这组系统调用称为系统调用接口层。系统调用接口把应用程序的请求传达给内核，待内核处理完请求后再将处理结果返回给应用程序。<br>&emsp;&emsp;32位Linux，CPU能访问4GB的虚拟空间，其中低3GB的地址是应用层的地址空间，高地址的1GB是留给内核使用的。内核中所有线程共享这1GB的地址空间，而每个进程可以有自己的独立的3GB的虚拟空间，互不干扰。<br>&emsp;&emsp;当一个进程运行的时候，其用到文件的代码段，数据段等都是映射到内存地址区域的，这个功能是通过系统调用mmap()来完成的。mmap()将文件（由文件句柄fd所指定）从偏移offset的位置开始的长度为length的一个块映射到内存区域中，从而把文件的某一段映射到进程的地址空间，这样程序就可以通过访问内存的方式访问文件了。与read()&#x2F;write()相比，使用mmap的方式对文件进行访问，带来的一个显著好处就是可以减少一次用户空间到内核空间的复制，在某些场景下，如音频、视频等大文件，可以带来性能的提升。</p><h2 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h2><p><img src="/images/linux_storage_stack_2.png"><br>&emsp;&emsp;Linux文件系统的体系结构是一个对复杂系统进行抽象化，通过使用一组通用的API函数，Linux就可以在多种存储设备上支持多种文件系统，使得它拥有了与其他操作系统和谐共存的能力。<br>&emsp;&emsp;Linux中文件的概念并不局限于普通的磁盘文件，而是由字节序列构成的信息载体，I&#x2F;O设备、socket等也被包括在内。因为有了文件的存在，所以需要衍生文件系统去进行组织和管理文件，而为了支持各种各样的文件系统，所以有了虚拟文件系统的出现。文件系统是一种对存储设备上的文件、数据进行存储和组织的机制。<br>&emsp;&emsp;虚拟文件系统通过在各种具体的文件系统上建立了一个抽象层，屏蔽了不同文件系统间的差异，通过虚拟文件系统分层架构，在对文件进行操作时，便不需要去关心相关文件所在的具体文件系统细节。通过系统调用层，可以在不同文件系统之间复制和移动数据，正是虚拟文件系统使得这种跨越不同存储设备和不同文件系统的操作成为了可能。</p><h4 id="虚拟文件系统象类型"><a href="#虚拟文件系统象类型" class="headerlink" title="虚拟文件系统象类型"></a>虚拟文件系统象类型</h4><ul><li><strong>超级块（Super Block）</strong><br>超级块对象代表了一个已经安装的文件系统，用于存储该文件系统的相关信息，如文件系统的类型、大小、状态等。对基于磁盘的文件系统， 这类对象通常存放在磁盘特定的扇区上。对于并非基于磁盘的文件系统，它们会现场创建超级块对象并将其保存在内存中。</li><li><strong>索引节点（Inode）</strong><br>索引节点对象代表存储设备上的一个实际物理文件，用于存储该文件的有关信息。Linux将文件的相关信息（如访问权限、大小、创建时间等）与文件本身区分开。文件的相关信息又被称为文件的元数据。</li><li><strong>目录项（Dentry)</strong><br>  目录项对象描述了文件系统的层次结构，一个路径的各个组成部分，不管是目录（虚拟文件系统将目录当作文件来处理）还是普通文件，都是一个目录项对象。</li><li><strong>文件</strong><br>  文件对象代表已经被进程打开的文件，主要用于建立进程和文件之间的对应关系。它由open()系统调用创建，由close()系统调用销毁，当且仅当进程访问文件期间存在于内存中，同一个物理文件可能存在多个对应的文件对象，但其对应的索引节点对象是唯一的。</li></ul><h2 id="Page-Cache"><a href="#Page-Cache" class="headerlink" title="Page Cache"></a>Page Cache</h2><p><img src="/images/linux_storage_stack_3.png"><br>&emsp;&emsp;Page Cache，通常也称为文件缓存，使用内存Page Cache文件的逻辑内容，从而提高对磁盘文件的访问速度。Page Cache是以物理页为单位对磁盘文件进行缓存的。<br>&emsp;&emsp;应用程序尝试读取某块数据的时候，会首先查找Page Cache，如果这块数据已经存放在Page Cache中，那么就可以立即返回给应用程序，而不需要再进行实际的物理磁盘操作。如果不能在Page Cache中发现要读取的数据，那么就需要先将数据从磁盘读取到Page Cache中，同样，对于写操作来说，应用程序也会将数据写到Page Cache中，再根据所采用的写操作机制，判断数据是否应该立即被写到磁盘上</p><h2 id="Direct-I-O和Buffered-I-O"><a href="#Direct-I-O和Buffered-I-O" class="headerlink" title="Direct I&#x2F;O和Buffered I&#x2F;O"></a>Direct I&#x2F;O和Buffered I&#x2F;O</h2><p><img src="/images/linux_storage_stack_4.png"><br>进程产生的IO路径主要有Direct I&#x2F;O和Buffered I&#x2F;O两条</p><ul><li><strong>标准I&#x2F;O：</strong> 也称为Buffered I&#x2F;O；Linux会将I&#x2F;O的数据缓存在Page Cache中，也就是说，数据会先被复制到内核的缓冲区，再从内核的缓冲区复制到应用程序的用户地址空间。在Buffered I&#x2F;O机制中，在没有CPU干预的情况下，可以通过DMA操作在磁盘和Page Cache之间直接进行数据的传输，在一定程度上分离了应用程序和物理设备，但是没有方法能直接在应用程序的地址空间和磁盘之间进行数据传输，数据在传输过程中需要在用户空间和Page Cache之间进行多次数据复制操作，这将带来较大的CPU开销。</li><li><strong>Direct I&#x2F;O：</strong> 可以省略使用Buffered I&#x2F;O中的内核缓冲区，数据可以直接在用户空间和磁盘之间进行传输，从而使得缓存应用程序可以避开复杂系统级别的缓存结构，执行自定义的数据读写管理，从而降低系统级别的管理对应用程序访问数据的影响。如果在块设备中执行Direct I&#x2F;O，那么进程必须在打开文件的时候将对文件的访问模式设置为O_DIRECT，这样就等于告诉Linux进程在接下来将使用Direct I&#x2F;O方式去读写文件，且传输的数据不经过内核中的Page Cache。Direct I&#x2F;O最主要的优点就是通过减少内核缓冲区和用户空间的数据复制次数，降低文件读写时所带来的CPU负载能力及内存带宽的占用率。如果传输的数据量很大，使用Direct IO的方式将会大大提高性能。然而，不经过内湖缓冲区直接进行磁盘的读写，必然会引起阻塞，因此通常Direct IO和AIO（异步IO）一起使用。</li></ul><h2 id="块层（Block-Layer）"><a href="#块层（Block-Layer）" class="headerlink" title="块层（Block Layer）"></a>块层（Block Layer）</h2><p><img src="/images/linux_storage_stack_5.png"><br>&emsp;&emsp;块设备访问时，需要在介质的不同区间前后移动，对于内核来说，管理块设备要比管理字符设备复杂得多。<br>&emsp;&emsp;系统调用read()触发相应的虚拟文件系统函数，虚拟文件系统判断请求是否已经在内核缓冲区里，如果不在，则判断如何执行读操作。如果内核必须从块设备上读取数据，就必须要确定数据在物理设备上的位置。这由映射层，即磁盘文件系统来完成。文件系统将文件访问映射为设备访问。<br>在通用块层中，使用bio结构体来描述一个I&#x2F;O请求在上层文件系统与底层物理磁盘之间的关系。而到了Linux驱动，则是使用request结构体来描述向块设备发出的I&#x2F;O请求的。对于慢速的磁盘而言，请求的处理速度很慢，这是内核就提供一种队列的机制把这些I&#x2F;O请求添加到队列中，使用request_queue结构体来描述。<br>&emsp;&emsp;bio和request是块层最核心的两个数据结构，其中，bio描述了磁盘里要真实操作的位置和Page Cache中的映射关系。作为Linux I&#x2F;O请求的基本单元，bio结构贯穿块层对I&#x2F;O请求处理的始终，每个bio对应磁盘里面一块连续的位置，bio结构中的bio_vec是一个bio的数据容器，专门用来保存bio的数据，包含一块数据所在页，以及页内的偏移及长度信息，通过这些信息就可以很清晰地描述数据具体什么位置。request用来描述单次I&#x2F;O请求，request_queue用来描述与设备相关的请求队列，每个块设备在块层都有一个request_queue与之对应，所有对该块设备的I&#x2F;O请求最后都会流经request_queue。块层正是借助bio、bio_vec、request、request_queue这几个结构将I&#x2F;O请求在内核I&#x2F;O子系统各个层次的处理过程联系起来。</p><ul><li><strong>I&#x2F;O调度算法：</strong> noop算法（不调度算法）、deadline算法（改良的电梯算法）、CFQ算法（完全公平调度算法，对于通用的服务器来说，CFQ是较好的选择，从Linux2.6.18版本开始，CFQ成为了默认的IO调度算法）。</li><li><strong>I&#x2F;O合并：</strong> 将符合条件的多个IO请求合并成单个IO请求进行一并处理，从而提升IO请求的效率。进程产生的IO路径主要有Direct I&#x2F;O和Buffered I&#x2F;O两条，无论哪一条路径，在bio结构转换为request结构进行IO调度前都需要进入Plug队列进行蓄流（部分Direct IO产生的请求不需要经过蓄流），所以对IO请求来说，能够进行合并的位置主要有Page Cache、Plug List、IO调度器3个。而在块层中，Plug将块层的IO请求聚集起来，使得零散的请求有机会进行合并和排序，最终达到高效利用存储设备的目的。每个进程都有一个私有的Plug队列，进程在向通用块层派发IO派发请求之前如果开始蓄流的功能，那么IO请求在被发送给IO调度器之前都被保存在Plug队列中，直到泄流的时候才被批量交给调度器。蓄流主要是为了增加请求合并的机会，bio在进入Plug队列之前会尝试与Plug队列保存的request进行合并。当应用需要发多个bio请求的时候，比较好的办法是先蓄流，而不是一个个单独发给最终的硬盘。</li></ul><h2 id="LVM"><a href="#LVM" class="headerlink" title="LVM"></a>LVM</h2><p>&emsp;&emsp;LVM，即Logical Volume Manager，逻辑卷管理器，是一种硬盘的虚拟化技术，可以允许用户的硬盘资源进行灵活的调整和动态管理。<br>&emsp;&emsp;LVM是Linux系统对于硬盘分区管理的一种机制，诞生是为了解决硬盘设备在创建分区后不易修改分区大小的缺陷。尽管对硬盘的强制性扩容和缩容理论上是可行的，但是却可能造成数据丢失。LVM技术是通过在硬盘分区和文件系统之间增加一个逻辑层，提供了一个抽象的卷组，就可以把多块硬盘设备、硬盘分区，甚至RAID整体进行卷则合并。并可以根据情况进行逻辑上的虚拟分割，这样一来，用户不用关心物理硬盘设备的底层架构和布局，就可以实现对硬盘分区设备的动态调整。<br>&emsp;&emsp;LVM通过在操作系统与物理存储资源之间引入逻辑卷（Logical Volume）的抽象，来解决传统磁盘分区管理工具的问题。LVM将众多不同的物理存储器资源（物理卷，Physical Volume）组成卷组（Volume Group），该卷组可以理解为普通系统的物理磁盘，但是卷组上不能创建或者安装文件系统，而是需要LVM先在卷组中创建一个逻辑卷，然后将ext3等文件系统安装在这个逻辑卷上，可以在不重新引导系统的前提下通过在卷组划分额外的空间，来为这个逻辑卷动态扩容。<br>LVM的架构体系中，有三个很重要的概念：</p><ul><li>PV，物理卷，即实际存在的硬盘、分区或者RAID</li><li>VG，卷组，是由多个物理卷组合形成的大的整体的卷组</li><li>LV，逻辑卷，是从卷组上分割出来的，可以使用使用的逻辑存储设备</li></ul><h2 id="条带化"><a href="#条带化" class="headerlink" title="条带化"></a>条带化</h2><p>&emsp;&emsp;大多数磁盘系统都对访问次数（每秒的 I&#x2F;O 操作，IOPS）和数据传输率（每秒传输的数据量，TPS）有限制。当达到这些限制时，后面需要访问磁盘的进程就需要等待，这时就是所谓的磁盘冲突。避免磁盘冲突是优化 I&#x2F;O 性能的一个重要目标，而 I&#x2F;O 性能的优化与其他资源（如CPU和内存）的优化有着很大的区别 ,I&#x2F;O 优化最有效的手段是将 I&#x2F;O 最大限度的进行平衡。条带化技术就是一种自动的将 I&#x2F;O 的负载均衡到多个物理磁盘上的技术，条带化技术就是将一块连续的数据分成很多小部分并把他们分别存储到不同磁盘上去。这就能使多个进程同时访问数据的多个不同部分而不会造成磁盘冲突，而且在需要对这种数据进行顺序访问的时候可以获得最大程度上的 I&#x2F;O 并行能力，从而获得非常好的性能。很多操作系统、磁盘设备供应商、各种第三方软件都能做到条带化。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Linux存储栈&quot;&gt;&lt;a href=&quot;#Linux存储栈&quot; class=&quot;headerlink&quot; title=&quot;Linux存储栈&quot;&gt;&lt;/a&gt;Linux存储栈&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/images/linux_storage_stack.png&quot;&gt;&lt;br</summary>
      
    
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="存储基础" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="存储基础" scheme="https://watsonlu6.github.io/tags/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://watsonlu6.github.io/hello-world/"/>
    <id>https://watsonlu6.github.io/hello-world/</id>
    <published>2024-07-13T08:29:18.282Z</published>
    <updated>2024-07-13T08:29:18.282Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
