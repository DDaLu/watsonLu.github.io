{"meta":{"title":"watson'blogs","subtitle":"","description":"","author":"John Doe","url":"https://watsonlu6.github.io","root":"/"},"pages":[],"posts":[{"title":"Linux存储栈","slug":"Linux存储栈","date":"2024-07-13T12:24:56.000Z","updated":"2024-07-13T13:06:46.760Z","comments":true,"path":"2024/07/13/Linux存储栈/","permalink":"https://watsonlu6.github.io/2024/07/13/Linux%E5%AD%98%E5%82%A8%E6%A0%88/","excerpt":"","text":"Linux存储栈Linux存储系统包括用户接口和存储设备接口两个部分，前者以流形式处理数据，后者以块形式处理数据，文件系统在中间起到承上启下的作用。应用程序通过系统调用发出写请求，文件系统定位请求位置并转换成块设备所需的块，然后发送到设备上。内存在此过程中作为磁盘缓冲，将上下两部分隔离成异步运行的两个过程，避免频繁的磁盘同步。当数据需要从页面缓存同步到磁盘时，请求被包装成包含多个bio的request，每个bio包含需要同步的数据页。磁盘在执行写操作时，需要通过IO请求调度合理安排顺序，减少磁头的频繁移动，提高磁盘性能。 用户视角的数据流接口 应用程序通过系统调用（如write、read等）与操作系统交互。这些调用使得数据以流的形式被处理。 存储设备的块接口 数据在底层存储设备（如硬盘、SSD等）中以块（通常是512字节或4096字节）为单位进行读写操作。 文件系统的中间角色 位置定位：文件系统负责将用户的读写请求定位到存储设备的具体块位置。 数据转换：将数据流转换为存储设备所需的块结构，并将这些块组织成bio（block I&#x2F;O）请求。 内存作为缓冲 页面缓存：内存中的页面缓存（Page Cache）用于暂时存储数据，以减少频繁的磁盘I&#x2F;O操作。 异步运行：将用户操作与底层存储设备的实际写操作异步化，提升系统效率。对于用户态程序来说，数据尽量保留在内存中，这样可以减少频繁的数据同步。 I&#x2F;O请求调度 请求封装：从页面缓存同步到磁盘的请求被封装成request，每个request包含多个bio，而每个bio又包含具体的数据页。 调度策略：操作系统会对I&#x2F;O请求进行调度，优化执行顺序，尽量减少磁盘磁头的来回移动，提高磁盘的读写效率。 Linux数据写入流程 应用程序发出写请求：比如，应用程序通过write系统调用写入数据。 文件系统处理：文件系统接收请求，找到对应的文件位置，将数据写入页面缓存。 内存缓冲处理：数据暂存在内存的页面缓存中，以等待后续的写入操作。 请求调度与封装：页面缓存的数据需要同步到磁盘时，被封装成bio和request。 I&#x2F;O调度执行：调度器优化I&#x2F;O请求的执行顺序，减少磁头移动，提高写入效率。 数据写入磁盘：最终，数据从页面缓存同步到磁盘的指定位置，完成写操作。通过以上流程，Linux存储系统在保证数据一致性的同时，最大限度地提高了性能和效率。 系统调用&emsp;&emsp;用户应用程序访问并使用内核所提供的各种服务的途径即是系统调用。在内核和应用程序交叉的地方，内核提供了一组系统调用接口，通过这组接口，应用程序可以访问系统硬件和各种操作系统资源。用户可以通过文件系统相关的调用请求系统打开问价、关闭文件和读写文件。&emsp;&emsp;内核提供的这组系统调用称为系统调用接口层。系统调用接口把应用程序的请求传达给内核，待内核处理完请求后再将处理结果返回给应用程序。&emsp;&emsp;32位Linux，CPU能访问4GB的虚拟空间，其中低3GB的地址是应用层的地址空间，高地址的1GB是留给内核使用的。内核中所有线程共享这1GB的地址空间，而每个进程可以有自己的独立的3GB的虚拟空间，互不干扰。&emsp;&emsp;当一个进程运行的时候，其用到文件的代码段，数据段等都是映射到内存地址区域的，这个功能是通过系统调用mmap()来完成的。mmap()将文件（由文件句柄fd所指定）从偏移offset的位置开始的长度为length的一个块映射到内存区域中，从而把文件的某一段映射到进程的地址空间，这样程序就可以通过访问内存的方式访问文件了。与read()&#x2F;write()相比，使用mmap的方式对文件进行访问，带来的一个显著好处就是可以减少一次用户空间到内核空间的复制，在某些场景下，如音频、视频等大文件，可以带来性能的提升。 文件系统&emsp;&emsp;Linux文件系统的体系结构是一个对复杂系统进行抽象化，通过使用一组通用的API函数，Linux就可以在多种存储设备上支持多种文件系统，使得它拥有了与其他操作系统和谐共存的能力。&emsp;&emsp;Linux中文件的概念并不局限于普通的磁盘文件，而是由字节序列构成的信息载体，I&#x2F;O设备、socket等也被包括在内。因为有了文件的存在，所以需要衍生文件系统去进行组织和管理文件，而为了支持各种各样的文件系统，所以有了虚拟文件系统的出现。文件系统是一种对存储设备上的文件、数据进行存储和组织的机制。&emsp;&emsp;虚拟文件系统通过在各种具体的文件系统上建立了一个抽象层，屏蔽了不同文件系统间的差异，通过虚拟文件系统分层架构，在对文件进行操作时，便不需要去关心相关文件所在的具体文件系统细节。通过系统调用层，可以在不同文件系统之间复制和移动数据，正是虚拟文件系统使得这种跨越不同存储设备和不同文件系统的操作成为了可能。 虚拟文件系统象类型 超级块（Super Block）超级块对象代表了一个已经安装的文件系统，用于存储该文件系统的相关信息，如文件系统的类型、大小、状态等。对基于磁盘的文件系统， 这类对象通常存放在磁盘特定的扇区上。对于并非基于磁盘的文件系统，它们会现场创建超级块对象并将其保存在内存中。 索引节点（Inode）索引节点对象代表存储设备上的一个实际物理文件，用于存储该文件的有关信息。Linux将文件的相关信息（如访问权限、大小、创建时间等）与文件本身区分开。文件的相关信息又被称为文件的元数据。 目录项（Dentry) 目录项对象描述了文件系统的层次结构，一个路径的各个组成部分，不管是目录（虚拟文件系统将目录当作文件来处理）还是普通文件，都是一个目录项对象。 文件 文件对象代表已经被进程打开的文件，主要用于建立进程和文件之间的对应关系。它由open()系统调用创建，由close()系统调用销毁，当且仅当进程访问文件期间存在于内存中，同一个物理文件可能存在多个对应的文件对象，但其对应的索引节点对象是唯一的。 Page Cache&emsp;&emsp;Page Cache，通常也称为文件缓存，使用内存Page Cache文件的逻辑内容，从而提高对磁盘文件的访问速度。Page Cache是以物理页为单位对磁盘文件进行缓存的。&emsp;&emsp;应用程序尝试读取某块数据的时候，会首先查找Page Cache，如果这块数据已经存放在Page Cache中，那么就可以立即返回给应用程序，而不需要再进行实际的物理磁盘操作。如果不能在Page Cache中发现要读取的数据，那么就需要先将数据从磁盘读取到Page Cache中，同样，对于写操作来说，应用程序也会将数据写到Page Cache中，再根据所采用的写操作机制，判断数据是否应该立即被写到磁盘上 Direct I&#x2F;O和Buffered I&#x2F;O进程产生的IO路径主要有Direct I&#x2F;O和Buffered I&#x2F;O两条 标准I&#x2F;O： 也称为Buffered I&#x2F;O；Linux会将I&#x2F;O的数据缓存在Page Cache中，也就是说，数据会先被复制到内核的缓冲区，再从内核的缓冲区复制到应用程序的用户地址空间。在Buffered I&#x2F;O机制中，在没有CPU干预的情况下，可以通过DMA操作在磁盘和Page Cache之间直接进行数据的传输，在一定程度上分离了应用程序和物理设备，但是没有方法能直接在应用程序的地址空间和磁盘之间进行数据传输，数据在传输过程中需要在用户空间和Page Cache之间进行多次数据复制操作，这将带来较大的CPU开销。 Direct I&#x2F;O： 可以省略使用Buffered I&#x2F;O中的内核缓冲区，数据可以直接在用户空间和磁盘之间进行传输，从而使得缓存应用程序可以避开复杂系统级别的缓存结构，执行自定义的数据读写管理，从而降低系统级别的管理对应用程序访问数据的影响。如果在块设备中执行Direct I&#x2F;O，那么进程必须在打开文件的时候将对文件的访问模式设置为O_DIRECT，这样就等于告诉Linux进程在接下来将使用Direct I&#x2F;O方式去读写文件，且传输的数据不经过内核中的Page Cache。Direct I&#x2F;O最主要的优点就是通过减少内核缓冲区和用户空间的数据复制次数，降低文件读写时所带来的CPU负载能力及内存带宽的占用率。如果传输的数据量很大，使用Direct IO的方式将会大大提高性能。然而，不经过内湖缓冲区直接进行磁盘的读写，必然会引起阻塞，因此通常Direct IO和AIO（异步IO）一起使用。 块层（Block Layer）&emsp;&emsp;块设备访问时，需要在介质的不同区间前后移动，对于内核来说，管理块设备要比管理字符设备复杂得多。&emsp;&emsp;系统调用read()触发相应的虚拟文件系统函数，虚拟文件系统判断请求是否已经在内核缓冲区里，如果不在，则判断如何执行读操作。如果内核必须从块设备上读取数据，就必须要确定数据在物理设备上的位置。这由映射层，即磁盘文件系统来完成。文件系统将文件访问映射为设备访问。在通用块层中，使用bio结构体来描述一个I&#x2F;O请求在上层文件系统与底层物理磁盘之间的关系。而到了Linux驱动，则是使用request结构体来描述向块设备发出的I&#x2F;O请求的。对于慢速的磁盘而言，请求的处理速度很慢，这是内核就提供一种队列的机制把这些I&#x2F;O请求添加到队列中，使用request_queue结构体来描述。&emsp;&emsp;bio和request是块层最核心的两个数据结构，其中，bio描述了磁盘里要真实操作的位置和Page Cache中的映射关系。作为Linux I&#x2F;O请求的基本单元，bio结构贯穿块层对I&#x2F;O请求处理的始终，每个bio对应磁盘里面一块连续的位置，bio结构中的bio_vec是一个bio的数据容器，专门用来保存bio的数据，包含一块数据所在页，以及页内的偏移及长度信息，通过这些信息就可以很清晰地描述数据具体什么位置。request用来描述单次I&#x2F;O请求，request_queue用来描述与设备相关的请求队列，每个块设备在块层都有一个request_queue与之对应，所有对该块设备的I&#x2F;O请求最后都会流经request_queue。块层正是借助bio、bio_vec、request、request_queue这几个结构将I&#x2F;O请求在内核I&#x2F;O子系统各个层次的处理过程联系起来。 I&#x2F;O调度算法： noop算法（不调度算法）、deadline算法（改良的电梯算法）、CFQ算法（完全公平调度算法，对于通用的服务器来说，CFQ是较好的选择，从Linux2.6.18版本开始，CFQ成为了默认的IO调度算法）。 I&#x2F;O合并： 将符合条件的多个IO请求合并成单个IO请求进行一并处理，从而提升IO请求的效率。进程产生的IO路径主要有Direct I&#x2F;O和Buffered I&#x2F;O两条，无论哪一条路径，在bio结构转换为request结构进行IO调度前都需要进入Plug队列进行蓄流（部分Direct IO产生的请求不需要经过蓄流），所以对IO请求来说，能够进行合并的位置主要有Page Cache、Plug List、IO调度器3个。而在块层中，Plug将块层的IO请求聚集起来，使得零散的请求有机会进行合并和排序，最终达到高效利用存储设备的目的。每个进程都有一个私有的Plug队列，进程在向通用块层派发IO派发请求之前如果开始蓄流的功能，那么IO请求在被发送给IO调度器之前都被保存在Plug队列中，直到泄流的时候才被批量交给调度器。蓄流主要是为了增加请求合并的机会，bio在进入Plug队列之前会尝试与Plug队列保存的request进行合并。当应用需要发多个bio请求的时候，比较好的办法是先蓄流，而不是一个个单独发给最终的硬盘。 LVM&emsp;&emsp;LVM，即Logical Volume Manager，逻辑卷管理器，是一种硬盘的虚拟化技术，可以允许用户的硬盘资源进行灵活的调整和动态管理。&emsp;&emsp;LVM是Linux系统对于硬盘分区管理的一种机制，诞生是为了解决硬盘设备在创建分区后不易修改分区大小的缺陷。尽管对硬盘的强制性扩容和缩容理论上是可行的，但是却可能造成数据丢失。LVM技术是通过在硬盘分区和文件系统之间增加一个逻辑层，提供了一个抽象的卷组，就可以把多块硬盘设备、硬盘分区，甚至RAID整体进行卷则合并。并可以根据情况进行逻辑上的虚拟分割，这样一来，用户不用关心物理硬盘设备的底层架构和布局，就可以实现对硬盘分区设备的动态调整。&emsp;&emsp;LVM通过在操作系统与物理存储资源之间引入逻辑卷（Logical Volume）的抽象，来解决传统磁盘分区管理工具的问题。LVM将众多不同的物理存储器资源（物理卷，Physical Volume）组成卷组（Volume Group），该卷组可以理解为普通系统的物理磁盘，但是卷组上不能创建或者安装文件系统，而是需要LVM先在卷组中创建一个逻辑卷，然后将ext3等文件系统安装在这个逻辑卷上，可以在不重新引导系统的前提下通过在卷组划分额外的空间，来为这个逻辑卷动态扩容。LVM的架构体系中，有三个很重要的概念： PV，物理卷，即实际存在的硬盘、分区或者RAID VG，卷组，是由多个物理卷组合形成的大的整体的卷组 LV，逻辑卷，是从卷组上分割出来的，可以使用使用的逻辑存储设备 条带化&emsp;&emsp;大多数磁盘系统都对访问次数（每秒的 I&#x2F;O 操作，IOPS）和数据传输率（每秒传输的数据量，TPS）有限制。当达到这些限制时，后面需要访问磁盘的进程就需要等待，这时就是所谓的磁盘冲突。避免磁盘冲突是优化 I&#x2F;O 性能的一个重要目标，而 I&#x2F;O 性能的优化与其他资源（如CPU和内存）的优化有着很大的区别 ,I&#x2F;O 优化最有效的手段是将 I&#x2F;O 最大限度的进行平衡。条带化技术就是一种自动的将 I&#x2F;O 的负载均衡到多个物理磁盘上的技术，条带化技术就是将一块连续的数据分成很多小部分并把他们分别存储到不同磁盘上去。这就能使多个进程同时访问数据的多个不同部分而不会造成磁盘冲突，而且在需要对这种数据进行顺序访问的时候可以获得最大程度上的 I&#x2F;O 并行能力，从而获得非常好的性能。很多操作系统、磁盘设备供应商、各种第三方软件都能做到条带化。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Hello World","slug":"hello-world","date":"2024-07-13T08:29:18.282Z","updated":"2024-07-13T08:29:18.282Z","comments":true,"path":"2024/07/13/hello-world/","permalink":"https://watsonlu6.github.io/2024/07/13/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]}