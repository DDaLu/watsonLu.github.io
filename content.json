{"meta":{"title":"watson'blogs","subtitle":"","description":"","author":"John Doe","url":"https://watsonlu6.github.io","root":"/"},"pages":[],"posts":[{"title":"Ceph线程池实现","slug":"Ceph线程池实现","date":"2021-08-27T14:26:42.000Z","updated":"2024-07-28T09:50:20.891Z","comments":true,"path":"Ceph线程池实现/","permalink":"https://watsonlu6.github.io/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"线程池和工作队列是紧密相连的，基本流程就是将任务送入到对应的工作队列中，线程池中的线程从工作队列中取出任务并进行处理。Ceph 为了支持高并发读写，源码设计中大量采用线程池来进行io的推进。Ceph的线程池实现了多种不同的工作队列。一般情况下，一个线程池对应一个类型的工作队列。在要求不高的情况下，也可以一个线程池对应多种类型的工作队列，让线程池处理不同类型的任务。 mutex的实现src&#x2F;common&#x2F;mutex.h condition variable的实现src&#x2F;common&#x2F;cond.h 线程的实现 Ceph中线程的在src&#x2F;common&#x2F;Thread.h中定义 线程编程接口中，一个线程在创建时调用pthread_create函数来传入entry函数，杀死线程调用pthread_kill函数，当线程被杀死之后，必须调用pthread_join函数来进行线程资源的回收，如果不调用此函数，就会出现类似zombie process。如果要想让系统自己回收线程资源，就要将线程与父线程分离即调用pthread_detach。通过接口对比，src&#x2F;common&#x2F;Thread.h中定义的class thread，实际上是Ceph自己封装了一个线程类，这个线程类其实就是对Linux线程接口的一层封装。 Ceph中所有要用的线程必须继承Thread类，通过查找发现如下一些线程： 1. Accepter.h (src\\msg)：class Accepter : public Thread &#x2F;&#x2F;用来socket bind的线程, accepter线程入口函数里定义了poll的网络通讯结构，用来放入管道 2. Admin_socket.h (src\\common)：class AdminSocket : public Thread 3. Ceph_context.cc (src\\common)：class CephContextServiceThread : public Thread 4. DispatchQueue.h (src\\msg): class DispatchThread : public Thread &#x2F;&#x2F;用来进行消息分发的线程， 在simpleMessenger中有dispatch_queue成员变量, 5. FileJournal.h (src\\os): class Writer : public Thread &#x2F;&#x2F;用来进行写数据到journal中的线程 6. FileJournal.h (src\\os): class WriteFinisher : public Thread &#x2F;&#x2F;当用aio异步模式写数据到journal完成后，此线程用来接管其他剩余操作 7. FileStore.h (src\\os): struct SyncThread : public Thread &#x2F;&#x2F;用来同步数据执行同步的线程，主要是将已经完成的journal的序列号写入到文件中 8. Finisher.h (src\\common): struct FinisherThread : public Thread &#x2F;&#x2F;公用的finisher线程，用来查看某些特定的操作是否结束，结束后进行后续处理工作 9. MDLog.h (src\\mds): class ReplayThread : public Thread 10. OSD.h (src\\osd): struct T_Heartbeat : public Thread &#x2F;&#x2F;维系osd进程之间互相心跳连接的线程 11. OutputDataSocket.h (src\\common):class OutputDataSocket : public Thread 12. Pipe.h (src\\msg): class Reader : public Thread &#x2F;&#x2F;用来处理所有对socket的读操作，由acepter线程将socket accept以后打入到SimpleMessenger::dispatch_queue中交由此线程处理 13. Pipe.h (src\\msg): class Writer : public Thread &#x2F;&#x2F;用来处理所有对socket的写操作，由acepter线程将socket accept以后打入到SimpleMessenger::dispatch_queue中交由此线程处理 14. Pipe.h (src\\msg): class DelayedDelivery: public Thread &#x2F;&#x2F;用来处理所有对socket的延时操作 15. Signal_handler.cc (src\\global)：struct SignalHandler : public Thread 16. SimpleMessenger.h (src\\msg): class ReaperThread : public Thread &#x2F;&#x2F;用来进行消息通信的主要线程 reaper是用来在通讯完成时拆除管道，其中成员有accepter线程（用来bind，accept socket文件放入管道），还有dispatch_queue线程 17. Throttle.cc (src\\test\\common): class Thread_get : public Thread 18. Timer.cc (src\\common)：class SafeTimerThread : public Thread 19. WorkQueue.h (src\\common): struct WorkThread : public Thread可以将这些线程分为四类线程 1. 普通类线程： 使用此类线程类直接申明继承自Thread，重写一个entry函数，在进程启动最初时，调用了create函数创建了线程，同时使用它的人必须自己定义消息队列。上面大部分线程都是此类，比如FileJournal::write_thread就是一个FileJournal::Writer类对象，它自己定义了消息队列FileJournal::writeq 2. SafeTimerThread类线程: 此类线程使用者可以直接申明一个SafeTimer成员变量，因为SafeTimer中已经封装了SafeTimerThread类和一个消息队列（成员是Context回调类），并完成了entry函数的逻辑流程。使用者使用方法，就是设置回调函数，通过SafeTimer::add_event_after函数将钩子埋入，等待规定时间到达后执行。 3. FinisherThread类线程: 此类线程使用者可以直接申明一个Finisher成员变量，因为Finsher中已经封装了FinisherThread类和一个消息队列（成员是Context回调类），并完成entry函数的逻辑流程。使用者使用方法，就是设置回调函数，通过Finisher::queue函数将钩子埋入，等待某类操作完成后执行。 4. ThreadPool内部线程： 这类线程由于是具体工作类线程，所以他们一般都是以线程池形式一下创建多个。ThreadPool类内部有多个线程set&lt;WorkThread*&gt;和多个消息队列vector&lt;WorkQueue_*&gt;组成。工作流程就是线程不断的轮询从队列中拿去数据进行操作。 可以看到Ceph线程的所有接口都只是对相应的Linux接口的封装。继承其的子类主要在于实现entry()函数： 线程池的实现 Ceph中线程池的在src&#x2F;common&#x2F;WorkQueue.h中定义 线程池和工作队列其实是密不可分的，从Ceph的代码中也可以看出来。让任务推入工作队列，而线程池中的线程负责从工作队列中取出任务进行处理。工作队列和线程池的关系，类似于狡兔和走狗的关系，正是因为有任务，所以才需要雇佣线程来完成任务，没有了狡兔，走狗也就失去了存在的意义。而线程必须要可以从工作队列中认领任务并完成，这就类似于猎狗要有追捕狡兔的功能。正因为两个数据结构拥有如此紧密的关系，因此，Ceph中他们的相关函数都位于WorkQueue.cc和WorkQueue.h中。void ThreadPool::start() 函数ThreadPool::start()用来启动线程池，其在加锁的情况下，调用函数start_threads()，start_threads()检查当前的线程数，如果小于配置的线程池线程数，就创建新的工作线程。 struct WorkThread : public Thread ThreadPool::worker() 线程池的关键在于线程的主函数做的事情。首先是工作线程。线程池中会有很多的WorkThread，它的基类就是Thread。线程的主函数为pool-&gt;worker，即ThreadPool::worker函数。其entry函数其实就是调用线程池的worker函数进行具体的工作。 ThreadPool::worker函数内定义了WorkThread类线程的操作逻辑。基本流程就是轮询所有WorkQueue_，当发现某种类型WorkQueue_中有数据时拿出，然后依次调用该WorkQueue_自己定义的函数_void_process和_void_process_finish等函数来顺序执行操作。（worker函数的主要实现其实很常规，就是遍历work_queues，从其中找出每一个消息队列实例，并调用WorkQueue_自己定义的函数_void_process和_void_process_finish等函数来顺序执行操作。） 线程池是支持动态调整线程个数的。所谓调整，有两种可能性，一种是线程个数增加，一种线程个数减少。当添加OSD的时候，数据会重分布，恢复的速度可以调节，其中一个重要的参数为osd-max-recovery-threads，该值修改可以实时生效。 ThreadPool::join_old_threads() 线程本身是一个loop，不停地处理WorkQueue中的任务，在一个loop的开头，线程个数是否超出了配置的个数，如果超出了，就需要自杀，所谓自杀即将自身推送到_old_threads中，然后跳出loop，直接返回了。线程池中的其他兄弟在busy-loop开头的join_old_threads函数会判断是否存在自杀的兄弟，如果存在的话，执行join，为兄弟收尸。 ThreadPool::start_threads() start_threads函数不仅仅可以用在初始化时启动所有工作线程，而且可以用于动态增加，它会根据配置要求的线程数_num_threads和当前线程池中线程的个数，来创建WorkThread，当然了，他会调整线程的io优先级。 ThreadPool::handle_conf_change() 线程池的线程个数如果不够用，也可以动态的增加，通过配置的变化来做到： ThreadPool::pause() 线程池的工作线程，绝大部分时间内，自然是busy－loop中处理工作队列上的任务，但是有一种场景是，需要让工作暂时停下来，停止工作，不要处理WorkQueue中的任务。线程池提供了一个标志为_pause,只要_pause不等于0，那么线程池中线程就在loop中就不会处理工作队列中的任务，而是空转。为了能够及时的醒来，也不是sleep，而是通过条件等待，等待执行的时间。 当下达pause指令的时候，很可能线程池中的某几个线程正在处理工作队列中的任务，这种情况下并不是立刻就能停下的，只有处理完手头的任务，在下一轮loop中检查_pause标志位才能真正地停下。那么pause指令就面临选择，要不要等工作线程WorkThread处理完手头的任务。pause函数是等，pauser_new函数并不等，pause_new函数只负责设置标志位，当其返回的时候，某几个线程可能仍然在处理工作队列中的任务。 struct WorkQueue_ 在ThreadPool这个类中，set&lt;WorkThread*&gt; _threads保存着线程池中的多个线程，vector&lt;WorkQueue_*&gt; work_queues保存着线程池中的待线程处理的消息队列。整个线程池的原理思想比较简单就是生成一定数目的线程，然后线程从队列中遍历获取队列实例，调用实例自带的处理函数_void_process和_void_process_finish处理。 ThreadPool中的WorkQueue_，这是一种抽象的类，只定义了一个队列应该有的一些特定的函数，这些函数几乎都是虚函数，目的是为了调用到自己三个子类BatchWorkQueue、WorkQueueVal、WorkQueue自己定义的函数。而在三个子类中对应函数_void_process、_void_process_finish中又分别调用了使用者自己继承它们而自己实现的具体操作函数如_process,_process_finish。存放在work_queues里面的WorkQueue_类： 这是一个纯虚基类，也就是说不同的线程池要实现自己的队列，继承WorkQueues_并且实现其接口。线程池已经有4个纯虚基类继承这个类： • BatchWorkQueue 批量处理队列 • WorkQueueVal 存值队列 • WorkQueue 存指针队列 • PointerWQ 存指针队列 add_work_queue()remove_work_queue()ThreadPool中的add_work_queue和remove_work_queue就是用来建立和移除与WorkQueue关联的函数 TPHandle 超时检查，每次线程函数执行时，都会设置一个grace超时时间，当线程执行超过该时间，就认为是unhealthy的状态。当执行时间超过suicide_grace时，OSD就会产生断言而导致自杀。heartbeat_handle_d记录了相关信息，并把该结构添加到HeartbeatMap的系统链表中保存。OSD会有一个定时器，定时检查是否超时。 线程池使用步骤 先创建线程池，然后创建WorkQueue的时候，将线程池作为参数传递给WorkQueue，就能建立关系。 1.声明线程池成员ThreadPool tp 2.声明队列类型ThreadPool::WorkQueue_wq 3.重写WorkQueue中对应函数_void_process,_void_process_finish 4.调用*_tp.add_work_queue(*_wq)将队列传入 基本线程池扩展在Ceph中有不少线程池会实现继承以上基类：ThreadPool op_tp 处理client请求 struct recovery_tp 处理recovery_tp操作 struct command_tp 处理命令行来的操作 ShardedThreadPool Ceph还实现了另外一种线程池ShardedThreadPool，这种线程池与上面的线程池不同之处在于这种线程池是多线程共享队列的方式。只有一个队列，多个线程同时对这个队列进行处理。 SharededWQshardedThreadPool类型线程池内部有个比较重要的消息队列SharededWQ，该队列将多种OP放入其中 Ceph 在实际使用中，会用到这种线程池","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph_rbd客户端实现","slug":"Ceph-rbd客户端实现","date":"2021-08-15T14:26:55.000Z","updated":"2024-07-28T09:48:58.097Z","comments":true,"path":"Ceph-rbd客户端实现/","permalink":"https://watsonlu6.github.io/Ceph-rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"Ceph RBD介绍随着云计算的发展，Ceph已经成为目前最为流行的分布式存储系统，俨然存储界的Linux操作系统。Ceph集块存储、文件存储和对象存储于一身，适用场景广泛，用户众多。RBD是 Ceph 分布式存储系统中提供的块存储服务，Ceph的块存储通过一个客户端模块实现，这个客户端可以直接从数据守护进程读写数据（不需要经过一个网关）。根据客户端整合生态系统的差异，使用Ceph的块设备有两种实现方式：librbd (用户态)和krbd (内核态)。RBD：RADOS Block Devices. Ceph block devices are thin-provisioned, resizable and store data striped over multiple OSDs in a Ceph cluster. 使用Ceph的块设备有两种路径（内核态与用户态）：(rbd map就是内核使用ceph块设备，调用librbd&#x2F;librados API访问ceph块设备是用户态) 通过Kernel Module(内核态RBD)：即创建了RBD设备后，把它映射到内核中（使用rbd map命令映射到操作系统上），成为一个虚拟的块设备，这时这个块设备同其他通用块设备一样，设备文件一般为&#x2F;dev&#x2F;rbd0，后续直接使用这个块设备文件就可以了，可以把&#x2F;dev&#x2F;rbd0格式化后挂载到某目录，也可以直接作为裸设备进行使用。krbd是一个内核模块。其在内核中以一个块设备的方式加以实现。这整个Ceph客户端都是以内核模块的方式实现（没有与之相关的用户态进程或者守护进程）。krbd在内核的源码目录源文件:drivers&#x2F;block&#x2F;rbd.c、drivers&#x2F;block&#x2F;rbd_types.h、net&#x2F;ceph&#x2F;、include&#x2F;linux&#x2F;ceph https://www.likecs.com/show-203739919.html https://github.com/torvalds/linux/blob/cfb92440ee71adcc2105b0890bb01ac3cddb8507/drivers/block/rbd.c https://github.com/torvalds/linux/tree/85c7000fda0029ec16569b1eec8fd3a8d026be73/include/linux/ceph 通过librbd(用户态RBD)：即创建了RBD设备后，使用librbd&#x2F;librados库访问和管理块设备。这种方式直接调用librbd提供的接口，实现对RBD设备的访问和管理，不会在客户端产生设备文件。应用方案有：SPDK+librbd&#x2F;librados https://github.com/ceph/ceph/tree/acf835db0376b1b71152949fdfec36e68f4a8474/src/librbd https://github.com/spdk/spdk/tree/cff525d336fb2c4c087413d4c53474b9e61cbdbe/module/bdev/rbd RBD 的块设备由于元数据信息少而且访问不频繁，故 RBD 在 Ceph 集群中不需要单独的守护进程将元数据加载到内存进行元数据访问加速，所有的元数据和数据操作直接与集群中的 Monitor 服务和 OSD 服务进行交互。 RBD 模块相关IO流图 客户端写数据osd过程： 采用的是 librbd 的形式，使用 librbd 创建一个块设备，向这个块设备中写入数据 在客户端本地同过调用 librados 接口，然后经过 pool，rbd，object，pg 进行层层映射（CRUSH 算法）,在 PG 这一层中，可以知道数据保存在哪几个 OSD 上，这几个 OSD 分为主从的关系 客户端与 primary OSD 建立 SOCKET 通信，将要写入的数据传给 primary OSD，由 primary OSD 再将数据发送给其他 replica OSD 数据节点。 IO 时序图librbd提供了针对image的数据读写和管理操作两种访问接口，其中数据读写请求入io_work_queue，然后由线程池中的线程将io请求以object粒度切分并分别调用rados层的aio接口（IoCtxImpl）下发，当所有的object请求完成时，调用librbd io回调（librbd::io::AioCompletion）完成用户层的数据io。而对image的管理操作通常需要涉及单个或多个对象的多次访问以及对内部状态的多次更新，其第一次访问将从用户线程调用至rados层 aio 接口或更新状态后入 op_work_queue 队列进行异步调用，当 rados aio 层回调或 Context 完成时再根据实现逻辑调用新的 rados aio 或构造 Context 回调，如此反复，最后调用应用层的回调完成管理操作请求。 此外为了支持多客户端共享访问 image，librbd 提供了构建于 rados watch&#x2F;notify 之上的通知、远程执行以及 exclusive lock 分布式锁机制。每个 librbd 客户端在打开 image 时（以非只读方式打开）都会 watch image 的 header 对象，从远程发往本地客户端的通知消息或者内部的 watch 错误消息会通过 RadosClient 的 Finisher 线程入 op_work_queue 队列进行异步处理。 RBD读写流程对于任何RBD客户端的读写都要经过以下步骤： 集群句柄创建、读取配置 集群句柄的创建即是librados:Rados的创建，初始化，读取配置 创建：librados::Rados rados; 初始化：librados::Rados::init(const char * const id) 主要是初始化librados::RadosClient 读取配置： librados::Rados::conf_read_file(const char * const path) const librados::Rados::conf_parse_argv(int argc, const char ** argv) const 集群连接 librados::Rados::connect() IO上下文环境初始化（pool创建读写等） librados::Rados::ioctx_create(const char *name, IoCtx &amp;io) 主要是IoCtxImpl即librados::IoCtx rbd创建 librbd::RBD rbd; RBD::create2(IoCtx&amp; io_ctx, const char *name, uint64_t size,uint64_t features, int *order) rbd的读写 librbd::Image image; RBD::open(IoCtx&amp; io_ctx, Image&amp; image, const char *name) Image::write(uint64_t ofs, size_t len, bufferlist&amp; bl) Image::read(uint64_t ofs, size_t len, bufferlist&amp; bl) IO上下文环境关闭 librbd::Image::close() librados::IoCtx::close() 集群句柄关闭 librados::Rados::shutdown() RBD源码介绍librbd以及librados都是属于ceph 的客户端，其提供ceph的接口向上提供块存储服务。librados提供客户端访问Ceph集群的原生态统一接口。其它接口或者命令行工具都基于该动态库实现。在librados中实现了Crush算法和网络通信等公共功能，数据请求操作在librados计算完成后可以直接与对应的OSD交互进行数据传输。librbd 是Ceph提供的在librados上封装的块存储接口的抽象。 librados主要的类是Rados和IoCtxlibrados::Rados负责初始化集群、读取配置、连接集群 librados::IoCtx负责创建IO上下文环境 librados::bufferlist负责读写缓存 librbd最主要的两个类是：RBD和Imagelibrbd::rbd主要负责 Image 的创建、删除、重命名、克隆映像等操作，包括对存储池的元数据的管理，针对部分操作提供异步接口 librbd::image负责image的读写(read&#x2F;write)，以及快照相关的操作等等。同时提供了相关异步操作的接口。 rbd Image的创建rbd卷的创建接口： 函数输入参数： io_ctx: 针对pool的上下文环境，对pool的操作都要首先建立一个相应的上下文环境 *name：rbd卷名字 size：rbd卷大小 features: rbd卷的特性 order: rbd卷的分块大小其具体实现在internal.cc中： 继续往下调用： 根据format格式调用不同的创建接口，现在主流采用新的format2，所用调用新的接口： 12int create_v2(IoCtx&amp; io_ctx, const char *imgname, uint64_t bid, uint64_t size,int order, uint64_t features, uint64_t stripe_unit,uint64_t stripe_count, uint8_t journal_order,uint8_t journal_splay_width, const std::string &amp;journal_pool,const std::string &amp;non_primary_global_image_id,const std::string &amp;primary_mirror_uuid,bool negotiate_features) 这个接口会做如下工作：创建rbd_id.{volume_name}的object： 然后想这个object写入block_name_prefix中的id号： 然后向rbd_directory写入卷名和id的一一映射。 创建名为rbd_header.id的object，并向这个object写入size,order,features,RBD_DATA_PREFIX等信息。 如果有条带化，则会设置条带化信息： 创建名为rbd_object_map.{id}的对象： rbd Image的打开 其实就是生成一个ImageCtx实例，调用其open接口。 rbd Image的写 rbd Image的读 rbd Image的快照 rbd Image的克隆 rbd Image的删除 rbd的读写 要使用librbd, 需要先安装下面两个包。可以通过yum安装, 也可以通过下载ceph源码编译后, 通过make install进行安装。 123$ yum list | grep librbdlibrbd1.x86_64 1:0.80.7-3.el7 baselibrbd1-devel.x86_64 1:0.80.7-3.el7 base 至于如何使用librbd来编程, 请参考下面的代码, 这是使用librbd的一般流程。编译时记得加上链接参数: g++ librbdtest.cpp -lrados -lrbd。更多函数的使用请参考 librbd.hpp。 另外 这里 有一些不错的示例。 12#include &lt;rbd/librbd.hpp&gt;#include &lt;rados/librados.hpp&gt;","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph_crush算法实现","slug":"Ceph-crush算法实现","date":"2021-08-02T14:26:17.000Z","updated":"2024-07-28T09:23:25.673Z","comments":true,"path":"Ceph-crush算法实现/","permalink":"https://watsonlu6.github.io/Ceph-crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"分布式存储系统的数据分布算法要解决数据如何分布到集群中的各个节点和磁盘上，其面临： 数据分布和负载均衡、灵活应对集群伸缩、大规模集群计算速率三方面的挑战。 数据分布和负载均衡：数据分布均衡，使数据能均匀地分布在各个节点和磁盘上，使数据访问的负载在各个节点和磁盘上。 灵活应对集群伸缩：系统可以方便地增加或者删除存储设备，当增加或删除存储设备后，能自动实现数据的均衡，并且迁移的数据尽可能减少。 大规模集群算法计算速率：要求数据分布算法维护的元数据相对较小，并且计算量不能太大。 在分布式存储系统中，数据分布算法由两种基本实现方法，一种是基于集中式的元数据查询的方式，如HDFS的实现：文件的分布信息是通过访问集中元数据服务器获得；另一种是基于哈希算法计算的方式。例如一致性哈希算法(DHT)。Ceph的数据分布算法CRUSH属于后者。CRUSH(Controlled Replication Under Scalable Hashing)，是一种基于哈希的数据分布算法。与另一种基于集中式的元数据查询的存储方式(文件的分布信息需要先通过访问集中元数据服务器获得)不同。CRUSH算法以数据唯一标识符、当前存储集群的拓扑结构以及数据分布策略作为CRUSH的输入，经过计算获得数据分布位置，直接与OSD进行通信，从而避免集中式查询操作，实现去中心化和高度并发。 Ceph 作为分布式存储系统，采用多节点多副本的数据存放方式，必然要解决数据如何分布到集群中各个节点和磁盘上。Ceph使用CRUSH数据分布算法。例如一个Ceph集群三副本，就存在着如何映射3个OSD存储这3个副本的数据，Ceph写数据时，即写object时，首先需要计算出object属于哪个PG，然后根据PG id 计算出存放的OSD位置。过程分两步：PG id的计算 ；OSD位置的计算。结合rbd的代码介绍这两个过程： 数据分片rbd的写接口（src&#x2F;linrbd&#x2F;librbd.cc）接口传入的参数是起始写位置（ofs）以及写数据大小（len）和要写入的数据（bl），调用io_work_queue-&gt;write()，生成Object写入请求对象，发送到ImageRequestWQ任务队列中，等待工作线程处理。现在看看ImageRequest的数据类型 因为Image的ImageWriteRequest继承AbstractImageWriteRequest类，重点关注AbstractImageWriteRequest类 发送写请求时调用void AbstractImageWriteRequest::send_request()函数，在这个函数进行切分数据，分成大小同等（可设定，一般为4M）的object(最后一块object可能大小小于块大小)。 file_to_extents就是将数据段切分各个object，具体怎么分割就不深入看源码了。然后调用send_object_requests()将分片各个object分别构造写请求 Op请求处理此后会构造objecter的Op请求，发送出去；转到src&#x2F;librados&#x2F;IoCtxImpl.cc，深入了解Op请求的处理。类IoCtxImpl是pool相关的上下文信息，一个pool对应一个IoCtxImpl对象，可以在该pool里创建、删除对象，完成对象数据读写等各种操作，包括同步和异步的实现。类IoCtxImpl把请求封装成ObjectOperation类。然后再添加pool的地址信息，封装成Obejcter::Op对象。Op封装了完成一个操作的相关的上下文信息，包括target地址信息、链接信息等。类IoCtxImpl的write&#x2F;read等同步操作函数通过调用operate()来调用op_submit()，类IoCtxImpl的aio_write&#x2F;aio_read&#x2F;aio_operate等异步函数直接调用了op_submit(），说明op_submit(）是object读写操作的入口。调用函数objeter-&gt;op_submit发送给相应的OSD，如果是同步操作，就等待操作完成。如果是异步操作，就不用等待，直接返回，当操作完成后，调用相应的回调函数通知。 Op封装了完成一个操作的相关的上下文信息，包括target地址信息、链接信息等。 发送数据op_submit在op_submit()调用_op_submit_with_budget()处理Throttle相关流量的限制在_op_submit_with_budget()中，如果osd_timeout大于0，就是设置定时器，当操作超时，就调用定时器回调函数op_ cancel取消操作，然后通过调用_op_submit(op, sul, ptid)。 _op_submit函数完成了关键的地址寻址和发送工作，比如_calc_target()、_get_session()、_send_op()等，调用函数_calc_target()计算对象的目标OSD；调用函数_get_session()获取目标OSD的链接，如果返回值为-EAGAIN，就升级为写锁，重新获取。检查当前的状态标志，如果当前是CEPH_OSDMAP_PAUSEWR或者OSD空间满，就暂时不发送，否则调用函数_prepare_osd_op准备请求的信息，调用函数_send_op发送出去。 对象寻址_calc_target重点详细分析下_calc_target函数：首先调用函数osdmap-&gt;get_pg_pool()根据t-&gt;base_oloc.pool获取pool信息，获取pg_pool_t对象；检查pi-&gt;last_force_op_resend是否强制重发，如果强制重发，force_resend设置为true；检查cache tier，如果是读操作，并且有读缓存，就设置t-&gt;target_oloc.pool为该pool的read_tier值；如果是写操作，并且有写缓存，就设置t-&gt;target_oloc.pool为该pool的write_tier值；调用函数osdmap-&gt;object_locator_to_pg()获取目标对象所在的PG；调用函数osdmap-&gt;pg_to_up_acting_osds()通过CRUSH算法，获取该PG对应的OSD列表，即pg_to_up_acting_osds()通过CRUSH算法计算OSD；判断读写操作：读操作，如果设置了CEPH_OSD_FLAG_BALANCE_READS标志，调用rand() 取余随机选择一个副本读取；读操作，如果设置了CEPH_OSD_FLAG_LOCALIZE_READS标志，尽可能从本地副本读取；写操作，target的OSD就设置为主OSD。 首先获取pool信息，判断是否有效： const pg_pool_t *pi = osdmap-&gt;get_pg_pool(t-&gt;base_oloc.pool);然后根据获取pgid，注意pgid是一个结构体pg_tpg_t 的结构如下： m_pool 是pool id， m_seed是函数根据object id算出来的哈希值，m_preferred赋值-1。接下来就是调用osdmap-&gt;pg_to_up_acting_osds()，获取该PG对应的OSD列表，即选择OSD： pg_to_up_acting_osds()函数在src\\osd\\OSDMap.cc中，函数功能是选出up osds以及 acting osds, 两个都是数组类型，大小为副本数 继续跟踪这个函数： 进入_pg_to_raw_osds： 上面函数crush-&gt;do_rule()就是真正调用crush算法计算出相应的osd列表。这里重点解释下参数pps：对象到PG的映射：任何程序通过客户端访问集群时，首先由客户端生成一个字符串形式的对象名，然后基于对象名和命名空间计算得出一个32位哈希值。针对此哈希值，对该存储池的PG总数量pg_num取模(掩码计算)，得到该对象所在的PG的id号。ps_t pps = pool.raw_pg_to_pps(pg); // placement ps 可以看出pps这是一个哈希值，这个哈希值根据pool id，函数中pg.ps()就是我们object哈希算出的m_seed： 调用CRUSH算法下面就是进入do_rule 进行CRUSH算法的处理了：src&#x2F;crush&#x2F;CrushWrapper.h调用crush_do_rule()函数 继续调用crush_do_rule()算法，执行CEUSH算法CRUSH算法：针对指定输入x(要计算PG的pg_id)，CRUSH将输出一个包含n个不同目标存储对象(例如磁盘)的集合(OSD列表)。CRUSH的计算过程使用x、cluster map、placement rule作为哈希函数输入。因此如果cluster map不发生变化(一般placement rule不会轻易变化)，那么结果就是确定的。算法输入需要3个输入参数： 输入x 即PG id的哈希值 crush_map即集群的拓扑结构，集群的层级化描述，形如”数据中心-&gt;机架-&gt;主机-&gt;磁盘”这样的层级拓扑。用树来表示，每个叶子节点都是真实的最小物理存储设备，称为devices；所有中间节点统称为bucket，每个bucket可以是一些devices的集合，也可以是低一级的buckets集合；根节点称为root，是整个集群的入口。 ruleno 即选择策略，就rule规则，这里用编号表示；它决定一个PG的对象副本如何选择(从定义的cluster map的拓扑结构中)的规则，以此完成数据映射。palcement rule可以包含多个操作，这些操作共有3种类型：take(root)、select(replicas, type)、emit(void) crush 算法输入需要3个输入参数： 输入x 即PG id的哈希值 crush_map即集群的拓扑结构 ruleno 即选择策略，就rule规则，这里用编号表示 可以通过集群输出crush_map: vim crush_map如下： 显示的结构和代码中的结构还是有着映射的关系： 其中crush_bucket:对应： crush_rule:对应于： 逐一对比分析其数据结构。这里分析下其选择OSD的过程： 12345int *a = scratch;int *b = scratch + result_max;int *c = scratch + result_max*2;w = a;o= b; a, b, c 分别指向 scratch向量的0, 1, 2的位置.w &#x3D; a; o &#x3D; b; w被用作一个先入先出队列来在CRUSH map中进行横向优先搜索(BFS traversal). o存储crush_choose_firstn选择的结果. c存储最终的OSD选择结果. crush_do_rule函数里面最重要的是函数里面的for循环，这个循环就是筛选osd的过程， for循环中： 首先从rule规则中当前执行的步骤，首次就执行第一条步骤： struct crush_rule_step *curstep = &amp;rule-&gt;steps[step]; 然后根据当前执行步骤的操作类型，选择不同的分支操作，首先一般是take操作，而且是take fault。即crush map树根节点。这个过程就是根据step 逐步选择bucket 知道知道叶子节点，即OSD。 这个过程中，crush_choose_firstn 函数, 递归的选择特定bucket或者设备,并且可以处理冲突,失败的情况. 如果当前是choose过程,通过调用crush_bucket_choose来直接选择. 如果当前是chooseleaf选择叶子节点的过程,该函数将递归直到得到叶子节点. 在for循环中的crush_choose_firstn()计算后如果结果不是OSD类型, o 交给w。以便于 w成为下次crush_choose_firstn的输入参数。在crush_choose_firstn()中，for(){}：副本选择循环判断条件rep是否等于副本数numrep，rep叠加。do{}while (retry_descent)：选择OSD冲突或故障域失效时循环，随机因子r改变。do{}while (retry_bucket)：进行bucket层级选择，当前item type不是OSD时循环，当前进行选择的bucket，即in改变。 在crush_choose_firstn()函数中有crush_bucket_choose函数，这个函数根据bucket类型选择不同的权重计算方法刷选出bucket。如果采用straw2，就会采用bucket_straw2_choose接口进行筛选。 bucket_straw2_choose()功能是通过调用伪随机算法计算伪随机数，以伪随机数最高的作为选择出的节点 generate_exponential_distribution()产生随机数的思想是：采用逆变换采样的思想，先调用crush_hash32_3()计算哈希值，然后取随机数的低16位。计算指数随机变量。作为参考，请参阅指数分布示例：https://en.wikipedia.org/wiki/Inverse_transform_sampling#Examples。 由于某种原因，略小于 0x10000 会产生更准确的分布……可能是舍入效果。 自然对数查找表映射 [0,0xffff]（对应实数 [1&#x2F;0x10000, 1] 到 [0, 0xffffffffffff]（对应实数 [-11.090355,0]）。除以 16.16 定点权重。 请注意，ln 值为负数，因此较大的权重意味着较大的（较小的负数）draw值。 CRUSH算法的一些缺陷： CRUSH算法提供了uniform、list和tree等bucket类型作为straw bucket类型的替代方案，但这些算法在添加或删除服务器时需要进行不必要的重排，这使它们不适合用于大规模存储系统。 CRUSH算法的查找函数需要进行O(log n)的二分查找，以找到与给定对象ID最接近的虚拟ID。这个计算对于系统中的每个对象都需要进行，因此在系统中有大量对象时，计算成本会很高。 CRUSH算法在重建过程中可能会出现瓶颈，因为它需要在placement groups中进行数据放置，这可能会导致数据重建速度变慢。 CRUSH算法的计算复杂度较高，需要进行大量的计算，这可能会影响系统的性能。 综上所述，CRUSH算法虽然是一种灵活的对象放置算法，但它也存在一些缺陷，需要进一步改进和优化。 由于CRUSH算法的计算复杂度较高，需要进行大量的计算，因此使用多线程来加速计算是一种可行的方法。具体来说，可以将CRUSH算法的计算任务分配给多个线程，每个线程负责计算一部分任务，然后将结果合并起来。这样可以充分利用多核处理器的计算能力，提高计算效率。但是，需要注意的是，多线程计算也会带来一些额外的开销，如线程间的同步和通信开销，因此需要进行合理的线程调度和优化，以达到最佳的性能提升效果。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph_Bufferlist的设计与使用","slug":"Ceph_Bufferlist的设计与使用","date":"2021-07-14T05:19:06.000Z","updated":"2024-07-27T14:37:12.169Z","comments":true,"path":"Ceph_Bufferlist的设计与使用/","permalink":"https://watsonlu6.github.io/Ceph_Bufferlist%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Ceph Bufferlist的设计与使用做为主要和磁盘、网络打交道的分布式存储系统，序列化是最基础的功能之一。当一个结构通过网络发送或写入磁盘时，它被编码为一串字节。可序列化结构具encode 和 decode方法，将结构体序列化后存入bufferlist和从bufferlist读出字节串反序列化出结构体。bufferlist是ceph的底层组件，用于存储二进制数据，其存储的数据可以直接写入磁盘，在代码中有很广泛的使用。 为什么要用bufferlist？ 为了免拷贝。发送数据时，传统的socket接口通常需要读取一段连续的内存。但是我们要发的数据内存不连续，所以以前的做法是申请一块大的内存，然后将不连续的内存内的数据拷贝到大内存块中，然后将大内存块地址给发送接口。但是找一块连续的大内存并不容易，系统可能会为此做各种腾挪操作，而将数据拷贝的大内存中，又是一个拷贝操作。RDMA的发送支持聚散表，不需要读取连续的内存。有bufferlist之后，我们可以通过bufferlist，将不连续的物理内存管理起来，形成一段“连续”的虚拟内存，然后将bufferlist的内存指针传递给聚散表，再把聚散表交给RDMA 发送接口即可。整个过程免去了内存拷贝操作。大大降低了CPU的消耗。 在ceph中经常需要将一个bufferlist编码(encode)到另一个bufferlist中，例如在msg发送消息的时候，通常msg拿到的osd等逻辑层传递给它的bufferlist，然后msg还需要给这个bufferlist加上消息头和消息尾，而消息头和消息尾也是用bufferlist表示的。这时候，msg通常会构造一个空的bufferlist，然后将消息头、消息尾、内容都encode到这个空的bufferlist。而bufferlist之间的encode实际只需要做ptr的copy，而不涉及到系统内存的申请和copy，效率较高。 补充： 传统内存访问需要通过CPU进行数据copy来移动数据，通过CPU将内存中的Buffer1移动到Buffer2中。 DMA(直接内存访问)是一种能力，允许在计算机主板上的设备直接把数据发送到内存中去，数据搬运不需要CPU的参与。 DMA模式：可以同DMA Engine之间通过硬件将数据从Buffer1移动到Buffer2，而不需要操作系统CPU的参与，大大降低了CPU Copy的开销。 RDMA是一种概念，在两个或者多个计算机进行通讯的时候使用DMA， 从一个主机的内存直接访问另一个主机的内存。RDMA是一种新的直接内存访问技术，RDMA让计算机可以直接存取其他计算机的内存，而不需要经过处理器的处理。RDMA将数据从一个系统快速移动到远程系统的内存中，而不对操作系统造成任何影响。 bufferlist的设计Bufferlist负责管理Ceph中所有的内存。整个Ceph中所有涉及到内存的操作，无论是msg分配内存接收消息，还是OSD构造各类数据结构的持久化表示（encode&#x2F;decode），再到实际磁盘操作，都将bufferlist作为基础。bufferlist对应的类为buffer::list(using bufferlist &#x3D; buffer::list;)，而buffer::list又基于buffer::ptr和buffer::raw实现，探讨buffer::list的实现，不能跳过它们。 123456789101112namespace ceph &#123; namespace buffer &#123; inline namespace v14_2_0 &#123; class ptr; class list; &#125; class hash; &#125; using bufferptr = buffer::ptr; using bufferlist = buffer::list; using bufferhash = buffer::hash;&#125; ceph::buffer是ceph非常底层的实现，负责管理ceph的内存。ceph::buffer的设计较为复杂，但本身没有任何内容，主要包含buffer::list、 buffer::ptr、 buffer::raw、 buffer::hash。这三个类都定义在src&#x2F;include&#x2F;buffer.h和src&#x2F;common&#x2F;buffer.cc中。 buffer::raw：负责维护物理内存的引用计数nref和释放操作。 buffer::ptr：指向buffer::raw的指针。 buffer::list：表示一个ptr的列表（std::list），相当于将N个ptr构成一个更大的虚拟的连续内存。 buffer::hash：一个或多个bufferlist的有效哈希。 buffer这三个类的相互关系可以用下面这个图来表示：图中蓝色的表示bufferlist，橙色表示bufferptr，绿色表示bufferraw。 在这个图中，实际占用的系统内存一共就三段，分别是raw0，raw1和raw2代表的三段内存。 raw0被ptr0，ptr1，ptr2使用 raw1被ptr3，ptr4，ptr6使用 raw2被ptr5，ptr7使用 而list0是由ptr0-5组成的，list1是由ptr6和ptr7组成的。 从这张图上我们就可以看出bufferlist的设计思路： 对于bufferlist来说，仅关心一个个ptr。bufferlist将ptr连在一起，当做是一段连续的内存使用。因此，可以通过bufferlist::iterator一个字节一个字节的迭代整个bufferlist中的所有内容，而不需要关心到底有几个ptr，更不用关心这些ptr到底和系统内存是怎么对应的；也可以通过bufferlist::write_file方法直接将bufferlist中的内容出到一个文件中；或者通过bufferlist::write_fd方法将bufferlist中的内容写入到某个fd中。 bufferraw负责管理系统内存的，bufferraw只关心一件事：维护其所管理的系统内存的引用计数，并且在引用计数减为0时——即没有ptr再使用这块内存时，释放这块内存。 bufferptr负责连接bufferlist和bufferraw。bufferptr关心的是如何使用内存。每一个bufferptr一定有一个bufferraw为其提供系统内存，然后ptr决定使用这块内存的哪一部分。bufferlist只用通过ptr才能对应到系统内存中，而bufferptr而可以独立存在，只是大部分ptr还是为bufferlist服务的，独立的ptr使用的场景并不是很多。通过引入ptr这样一个中间层次，bufferlist使用内存的方式可以非常灵活。 快速encode&#x2F;decode。在Ceph中经常需要将一个bufferlist编码（encode）到另一个bufferlist中，例如在msg发送消息的时候，通常msg拿到的osd等逻辑层传递给它的bufferlist，然后msg还需要给这个bufferlist加上消息头和消息尾，而消息头和消息尾也是用bufferlist表示的。这时候，msg通常会构造一个空的bufferlist，然后将消息头、消息尾、内容都encode到这个空的bufferlist。而bufferlist之间的encode实际只需要做ptr的copy，而不涉及到系统内存的申请和Copy，效率较高。 一次分配，多次使用。调用malloc之类的函数申请内存是非常重量级的操作。利用ptr这个中间层可以缓解这个问题，可以一次性申请一块较大的内存，也就是一个较大的bufferraw，然后每次需要内存的时候，构造一个bufferptr，指向这个bufferraw的不同部分。这样就不再需要向系统申请内存了。最后将这些ptr都加入到一个bufferlist中，就可以形成一个虚拟的连续内存。 减少内存分配次数和碎片。利用bufferptr这个中间层进行内存的多次使用，多个bufferptr可以引用同一段bufferraw的不同区域，这个bufferraw可以预先一次性申请较大一段连续内存，从而避免了多次申请内存以及内存碎片的产生。 buffer::rawraw的数据成员部分代码如下： 1234567891011class buffer::raw&#123;public: char *data; //数据指针 unsigned len; //数据长度 std::atomic&lt;unsigned&gt; nref&#123;0&#125;; //引用计数 int mempool; mutable ceph::spinlock crc_spinlock; //读写锁 map&lt;pair&lt;size_t, size_t&gt;, pair&lt;uint32_t, uint32_t&gt;&gt; crc_map; //crc校验信息 ......&#125;; 最基本的成员：data是指向具体数据的指针，len是数据的长度，nref是引用计数。而mempool是其对应的内存池的index，这个和data空间的分配有关，暂时不去管它。 data指向的数据有很多来源，直接通过malloc从内存分配只是最基础的一种，可能还来自mmap内存映射的空间，甚至可以通过pipe管道＋splice实现零拷贝获取空间。有些时候，分配的空间时，会提出对齐的要求，比如按页对齐等等。对于每一种数据来源，需要不同逻辑的数据分配和释放函数，所以raw对应了很多子类，分别表示不同的数据。 下列类都继承了buffer::raw，实现了对data对应内存空间的申请 类raw_malloc实现了用malloc函数分配内存空间的功能 类class buffer::raw_mmap_pages实现了通过mmap来把内存匿名映射到进程的地址空间 类class buffer::raw_posix_aligned调用了函数posix_memalign来申请内存地址对齐的内存空间。 类class buffer::raw_hack_aligned是在系统不支持内存对齐申请的情况下自己实现了内存地址的对齐 类class buffer::raw_pipe实现了pipe做为Buffer的内存空间 类class buffer::raw_char使用了C++的new操作符来申请空间 这是因为这些来源不同，要求不同，buffer::raw也就有了一些变体，举个例子，对应于malloc的raw子类为buffer::raw_malloc，构造和析构函数中实现了使用malloc进行数据分配和释放的逻辑： 123456789101112131415161718192021222324252627282930313233343536class buffer::raw_malloc : public buffer::raw&#123;public: MEMPOOL_CLASS_HELPERS(); explicit raw_malloc(unsigned l) : raw(l) &#123; if (len) &#123; data = (char *)malloc(len); if (!data) throw bad_alloc(); &#125; else &#123; data = 0; &#125; inc_total_alloc(len); inc_history_alloc(len); bdout &lt;&lt; &quot;raw_malloc &quot; &lt;&lt; this &lt;&lt; &quot; alloc &quot; &lt;&lt; (void *)data &lt;&lt; &quot; &quot; &lt;&lt; l &lt;&lt; &quot; &quot; &lt;&lt; buffer::get_total_alloc() &lt;&lt; bendl; &#125; raw_malloc(unsigned l, char *b) : raw(b, l) &#123; inc_total_alloc(len); bdout &lt;&lt; &quot;raw_malloc &quot; &lt;&lt; this &lt;&lt; &quot; alloc &quot; &lt;&lt; (void *)data &lt;&lt; &quot; &quot; &lt;&lt; l &lt;&lt; &quot; &quot; &lt;&lt; buffer::get_total_alloc() &lt;&lt; bendl; &#125; ~raw_malloc() override &#123; free(data); dec_total_alloc(len); bdout &lt;&lt; &quot;raw_malloc &quot; &lt;&lt; this &lt;&lt; &quot; free &quot; &lt;&lt; (void *)data &lt;&lt; &quot; &quot; &lt;&lt; buffer::get_total_alloc() &lt;&lt; bendl; &#125; raw *clone_empty() override &#123; return new raw_malloc(len); &#125;&#125;; 对应于malloc的raw子类为buffer::raw_mmap_pages，顾名思义，也能够猜到，这个数据的来源是通过mmap分配的匿名内存映射。因此析构的时候，毫不意外，掉用munmap解除映射，归还空间给系统： 12345678910111213141516171819class buffer::raw_mmap_pages : public buffer::raw &#123;public: explicit raw_mmap_pages(unsigned l) : raw(l) &#123; data = (char*)::mmap(NULL, len, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANON, -1, 0); if (!data) throw bad_alloc(); inc_total_alloc(len); inc_history_alloc(len); bdout &lt;&lt; &quot;raw_mmap &quot; &lt;&lt; this &lt;&lt; &quot; alloc &quot; &lt;&lt; (void *)data &lt;&lt; &quot; &quot; &lt;&lt; l &lt;&lt; &quot; &quot; &lt;&lt; buffer::get_total_alloc() &lt;&lt; bendl; &#125; ~raw_mmap_pages() &#123; ::munmap(data, len); dec_total_alloc(len); bdout &lt;&lt; &quot;raw_mmap &quot; &lt;&lt; this &lt;&lt; &quot; free &quot; &lt;&lt; (void *)data &lt;&lt; &quot; &quot; &lt;&lt; buffer::get_total_alloc() &lt;&lt; bendl; &#125; raw* clone_empty() &#123; return new raw_mmap_pages(len); &#125;&#125;; buffer::ptrbuffer::ptr是在buffer::raw系列的基础上，这个类也别名bufferptr， 这个类是raw这个类的包装升级版本，它的_raw就是指向buffer::raw类型的变量。成员部分如下（include&#x2F;buffer.h）： 123456class CEPH_BUFFER_API ptr&#123; raw *_raw; unsigned _off, _len; ......&#125;; 类buffer::ptr就是对于buffer::raw的一部分数据段，ptr是raw里的一个任意的数据段，_off是在_raw里的偏移量，_len是在ptr的长度。raw是真正存储数据的地方，而ptr只是指向某个raw中的一段的指针。其数据成员 _raw为指向raw的指针，_off表示数据起始偏移，_len表示数据长度。这边还有提一下ptr的append函数，直观上ptr不应该提供append函数，事实上ptr的append确实很局限，只有当ptr对应的raw区域后方有空闲空间的时候，才能append成功，至于空间不够的情况，应该是交给list等高层类来处理。代码如下： 123456789unsigned buffer::ptr::append(const char *p, unsigned l)&#123; assert(_raw); assert(l &lt;= unused_tail_length()); char *c = _raw-&gt;data + _off + _len; maybe_inline_memcpy(c, p, l, 32); _len += l; return _len + _off;&#125; buffer::ptr其他常见操作 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364buffer::ptr&amp; buffer::ptr::operator= (const ptr&amp; p)&#123; if (p._raw) &#123; p._raw-&gt;nref.inc(); bdout &lt;&lt; &quot;ptr &quot; &lt;&lt; this &lt;&lt; &quot; get &quot; &lt;&lt; _raw &lt;&lt; bendl; &#125; buffer::raw *raw = p._raw; release(); if (raw) &#123; _raw = raw; _off = p._off; _len = p._len; &#125; else &#123; _off = _len = 0; &#125; return *this;&#125;buffer::raw *buffer::ptr::clone()&#123; return _raw-&gt;clone();&#125;void buffer::ptr::swap(ptr&amp; other)&#123; raw *r = _raw; unsigned o = _off; unsigned l = _len; _raw = other._raw; _off = other._off; _len = other._len; other._raw = r; other._off = o; other._len = l;&#125;const char&amp; buffer::ptr::operator[](unsigned n) const&#123; assert(_raw); assert(n &lt; _len); return _raw-&gt;get_data()[_off + n];&#125;char&amp; buffer::ptr::operator[](unsigned n)&#123; assert(_raw); assert(n &lt; _len); return _raw-&gt;get_data()[_off + n];&#125;int buffer::ptr::cmp(const ptr&amp; o) const&#123; int l = _len &lt; o._len ? _len : o._len; if (l) &#123; int r = memcmp(c_str(), o.c_str(), l); if (r) return r; &#125; if (_len &lt; o._len) return -1; if (_len &gt; o._len) return 1; return 0;&#125; buffer::list类buffer::list是一个使用广泛的类，它是多个buffer::ptr的列表，也就是多个内存数据段的列表。多个bufferptr形成一个list，这就是bufferlist。简单来说，list就是一个ptr组成的链表：（include&#x2F;buffer.h） 12345678910class CEPH_BUFFER_API list&#123;// my private bitsstd::list&lt;ptr&gt; _buffers; //所有的ptrunsigned _len; //所有的ptr的数据总长度unsigned _memcopy_count; //当调用函数rebuild用来内存对齐时，需要内存拷贝的数据量ptr append_buffer; // 当有小的数据就添加到这个buffer里 mutable iterator last_p; //访问list的迭代器......&#125;; buffers是一个ptr的链表，_len是整个_buffers中所有的ptr的数据的总长度，_memcopy_count用于统计memcopy的字节数，append_buffer是用于优化append操作的缓冲区，可以看出bufferlist将数据以不连续链表的方式存储。 bufferlist的迭代器迭代器中提供的seek(unsigned o)和advance(int o)等函数中的o都是指bufferlist的偏移，而不是单个ptr内的偏移。 123456789101112template &lt;bool is_const&gt;class CEPH_BUFFER_API iterator_impl : public std::iterator&lt;std::forward_iterator_tag, char&gt;&#123;protected: bl_t *bl; list_t *ls; // meh.. just here to avoid an extra pointer dereference.. unsigned off; // in bl list_iter_t p; unsigned p_off; // in *p ......&#125;; 其数据成员的含义如下： bl：指针，指向bufferlist ls：指针，指向bufferlist的成员 _buffers p: 类型是std::list::iterator，用来迭代遍历bufferlist中的bufferptr p_off：当前位置在对应的bufferptr中的偏移量 off：当前位置在整个bufferlist中的偏移量 bufferlist常用函数librados只给出bufferlist API clear() 清空bufferlist中的内容 push_front(raw* &#x2F; ptr &amp;)push_back(raw* &#x2F; ptr &amp;) 在_buffers的前面或后面增加新的ptr rebuild()rebuild(ptr &amp;nb) 将bufferlist中buffers链表中所有的ptr中的数据存到一个ptr中，并将_buffers原有数据clear，然后将新的单个ptr push到_buffers中。 带参数时使用参数传入的ptr作为目标ptr，不带参数时自己创建一个ptr。 claim(list &amp;bl, unsigned int flags &#x3D; CLAIM_DEFAULT); 将bl的数据拿过来，替换原有的数据。调用后bl数据被清空。 claim_append(list &amp;bl, unsigned int flags &#x3D; CLAIM_DEFAULT);claim_prepend(list &amp;bl, unsigned int flags &#x3D; CLAIM_DEFAULT); 将bl的数据拿过来，splice到_buffers的尾部&#x2F;头部。 append(…) 将数据追加到_buffers尾部，已有ptr空间不够时，会自动分配新的ptr。 splice(unsigned off, unsigned len, list *claim_by &#x3D; 0) bl.splice(10,10,&amp;bl2); 将_buffers中总偏移off处长度为len的数据，move到claim_by对应的bufferlist的尾部。注意是move不是copy。 write(int off, int len, std::ostream &amp;out) 将_buffers中总偏移量off处长度为len的数据，写入到ostream。注意是copy，不是move。 push_front(ptr&amp; pb) 添加一个ptr到list头部 push_front(raw *r)添加一个raw到list头部中，先构造一个ptr，后添加list中 is_aligned(align)判断内存是否以参数align对齐，每一个ptr都必须以align对齐 read_fd()&#x2F;write_fd()把数据写入文件描述符或者从文件描述符读取数据 read_file()&#x2F;write_file()把数据写入文件或从文件读取数据的功能 write_stream() 内存对齐：有些情况下，需要内存地址对齐，例如当以directIO方式写入数据至磁盘时，需要内存地址按照内存页面大小（page）对齐，也即buffer::list的内存地址都需按照page对齐。函数rebuild用来完成对齐的功能。其实现的方法也比较简单，检查没有对齐的ptr，申请一块新对齐的内存，把数据拷贝过去，释放内存空间就可以了。 相关链接： http://bean-li.github.io/bufferlist-in-ceph/ https://www.jianshu.com/p/01e1f4e398df","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph序列化","slug":"Ceph数据序列化","date":"2021-07-10T04:43:01.000Z","updated":"2024-07-27T14:36:56.441Z","comments":true,"path":"Ceph数据序列化/","permalink":"https://watsonlu6.github.io/Ceph%E6%95%B0%E6%8D%AE%E5%BA%8F%E5%88%97%E5%8C%96/","excerpt":"","text":"Ceph 数据序列化Ceph 作为主要处理磁盘和网络的分布式存储系统，数据序列化是其最基本的功能之一。当一个结构通过网络发送或写入磁盘时，它会被编码为一串字节。可序列化的结构体具有 encode 和 decode 方法，用于将结构体序列化后存入 bufferlist，或从 bufferlist 读取字节串并反序列化为结构体。 在 Ceph 中，经常需要将一个 bufferlist 编码（encode）到另一个 bufferlist 中。例如，在 msg 发送消息时，msg 通常会接收到由 OSD 等逻辑层传递给它的 bufferlist，然后 msg 需要给这个 bufferlist 添加消息头和消息尾，而消息头和消息尾也是用 bufferlist 表示的。在这种情况下，msg 通常会构造一个空的 bufferlist，然后将消息头、消息尾和内容都编码到这个空的 bufferlist 中。 在 bufferlist 之间进行编码实际上只需要进行指针的复制，而不涉及系统内存的申请和复制，因此效率较高。encode 和 decode 方法的主要作用是方便 Ceph 不同模块之间的参数传输。 在Ceph代码中有很多例子，这里有一个例子。 12345678910111213141516171819class AcmeClass&#123; int member1; std::string member2; void encode(bufferlist &amp;bl) &#123; ENCODE_START(1, 1, bl); ::encode(member1, bl); ::encode(member2, bl); ENCODE_FINISH(bl); &#125; void decode(bufferlist::iterator &amp;bl) &#123; DECODE_START(1, bl); ::decode(member1, bl); ::decode(member2, bl); DECODE_FINISH(bl); &#125;&#125;; ENCODE_START宏写入标头 说明version和 compat_version（初值均为 1）。每当对encode进行更改时，version就会增加。仅当更改会影响decode时compat_version才会增加 - 比如新结构体只在尾部添加字段，不会影响旧结构体的解析，因此在结构末尾添加字段的更改不需要增加 compat_version。DECODE_START宏采用一个参数，指定encode代码可以处理的最新消息版本。这与消息中编码的 compat_version 进行比较，如果消息太新，则会抛出异常。因为对 compat_verison 的更改很少，所以在添加字段时通常不需要担心。 Ceph序列化的方式序列化（在 Ceph 中称为 encode）的目的是将数据结构表示为二进制流，以便通过网络传输或保存在磁盘等存储介质上。其逆过程称为反序列化（在 Ceph 中称为 decode）。例如，对于字符串“abc”，其序列化结果为7个字节（bytes）：03 00 00 00 61 62 63，其中前四个字节（03 00 00 00）表示字符串的长度为3个字符，后三个字节（61 62 63）分别是字符“abc”的 ASCII 码的十六进制表示。Ceph 采用 little-endian 的序列化方式，即低地址存放最低有效字节，因此32位整数0x12345678的序列化结果为78 56 34 12。 由于序列化在整个 Ceph 系统中是非常基础且常用的功能，Ceph 将其序列化方式设计为统一的结构，即任何支持序列化的数据结构都必须提供一对定义在全局命名空间中的序列化&#x2F;反序列化（encode&#x2F;decode）函数。例如，如果我们定义了一个结构体 inode，就必须在全局命名空间中定义以下两个方法： encode(struct inode, bufferlist bl); decode(struct inode, bufferlist::iterator bl); 在此基础上，序列化的使用变得非常简单。对于任意可序列化的类型 T 的实例 instance_T，可以通过如下语句将 instance_T 序列化并保存到 bufferlist 类的实例 instance_bufferlist 中。 bufferlist类（定义于include&#x2F;buffer.h）是ceph核心的缓存类，用于保存序列化结果、数据缓存、网络通信等，能够将bufferlist理解为一个可变长度的char数组。 如下代码演示了将一个时间戳以及一个inode序列化到一个bufferlist中。 12345utime_t timestamp;inode_t inode;bufferlist bl;::encode(timetamp, bl)::encode(inode, bl); 序列化后的数据能够经过反序列化方法读取，例如如下代码片断从一个bufferlist中反序列化一个时间戳和一个inode（前提是该bl中已经被序列化了一个utime_t和一个inode，不然会报错）。 123bufferlist::iterator bl;::decode(timetamp, bl)::decode(inode, bl); 各种数据类型的序列化Ceph为其全部用到数据类型提供了序列化方法或反序列化方法，这些数据类型包括了绝大部分基础数据类型（int、bool等）、结构体类型的序列化（ceph_mds_request_head等）、集合类型（vector、list、set、map等）、以及自定义的复杂数据类型（例如表示inode的inode_t等），如下分别介绍不一样数据类型的序列化实现方式。 1、基本数据类型的序列化基本数据类型的序列化结果基本就是该类型在内存中的表示形式。基本数据类型的序列化方法使用手工编写，定义在include&#x2F;encoding.h中，包括如下类型： 12345__u8, __s8, char, boolceph_le64, ceph_le32, ceph_le16,float, double,uint64_t, int64_t, uint32_t, int32_t, uint16_t, int16_t,string, char* 在手工编写encode方法过程当中，为了不重复代码，借助了WRITE_RAW_ENCODER和WRITE_INTTYPE_ENCODER两个宏。 2、结构体类型的序列化结构体类型的序列化方法与基本数据类型的序列化方法一致，即便用结构体的内存布局做为序列化的形式。在结构体定义完成后，经过调用WRITE_RAW_ENCODER宏函数生成结构体的全局encode方法，例如结构体ceph_mds_request_head相关结构实现以下。 1234567891011struct ceph_mds_request_head &#123; __le64 oldest_client_tid; __le32 mdsmap_epoch; __le32 flags; __u8 num_retry, num_fwd; __le16 num_releases; __le32 op; __le32 caller_uid, caller_gid; __le64 ino;&#125; __attribute__ ((packed));WRITE_RAW_ENCODER(ceph_mds_request_head) 其中： ceph_mds_request_head结构体定义在include&#x2F;ceph_fs.h . WRITE_RAW_ENCODER(ceph_mds_request_head)语句位于include&#x2F;types.h WRITE_RAW_ENCODER宏函数定义在include&#x2F;encoding.h WRITE_RAW_ENCODER宏函数其实是经过调用encode_raw实现的，而encode_raw调用bufferlist的append的方法，经过内存拷贝，将数据结构放入到bufferlist中。相关代码为： 12345678910template&lt;class T&gt;inline void encode_raw(const T&amp; t, bufferlist&amp; bl)&#123; bl.append((char*)&amp;t, sizeof(t));&#125;template&lt;class T&gt;inline void decode_raw(T&amp; t, bufferlist::iterator &amp;p)&#123; p.copy(sizeof(t), (char*)&amp;t);&#125; 3、集合数据类型的序列化集合数据类型序列化的基本思路包括两步： 序列化集合大小， 序列化集合内的全部元素 例如vector&amp; v的序列化方法：其中元素的序列化经过调用该元素的encode方法实现。 12345678template&lt;class T&gt;inline void encode(const std::vector&lt;T&gt;&amp; v, bufferlist&amp; bl)&#123; __u32 n = v.size(); encode(n, bl); for (typename std::vector&lt;T&gt;::const_iterator p = v.begin(); p != v.end(); ++p) encode(*p, bl);&#125; 经常使用集合数据类型的序列化已经由Ceph实现，位于include&#x2F;encoding.h中，包括如下集合类型：pair, triple, list, set, vector, map, multimap, hash_map, hash_set, deque。集合类型的序列化方法皆为基于泛型（模板类）的实现方式，适用于全部泛型派生类。 4、复杂数据类型的序列化除以上两种业务无关的数据类型外，其它数据类型的序列化实现包括两部分： 在类型内部现实encode方法，将类型内部的encode方法重定义为全局方法。如下以utime_t类为例：utime_t内部实现了encode和decode两个方法，WRITE_CLASS_ENCODER宏函数将这两个方法转化为全局方法。 1234567891011121314class utime_t &#123; struct &#123; __u32 tv_sec, tv_nsec; &#125; tv; void encode(bufferlist &amp;bl) const &#123; ::encode(tv.tv_sec, bl); ::encode(tv.tv_nsec, bl); &#125; void decode(bufferlist::iterator &amp;p) &#123; ::decode(tv.tv_sec, p); ::decode(tv.tv_nsec, p); &#125;&#125;;WRITE_CLASS_ENCODER(utime_t) 复杂数据结构内部的encode方法的实现方式一般是调用其内部主要数据结构的encode方法，例如utime_t类的encode方法其实是序列化内部的tv.tv_sec和tv.tv_nsec两个成员。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph相关数据结构","slug":"Ceph相关数据结构","date":"2021-07-02T15:33:02.000Z","updated":"2024-07-27T14:36:43.411Z","comments":true,"path":"Ceph相关数据结构/","permalink":"https://watsonlu6.github.io/Ceph%E7%9B%B8%E5%85%B3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","excerpt":"","text":"Ceph 相关数据结构要想深入到Ceph的源码底层，就必须对代码通用库里的一些关键，常见的数据结构进行学习，这样才能更好的理解源代码。从最高的逻辑层次为Pool的概念，然后是PG的概念。其次是OSDＭap记录了集群的所有的配置信息。数据结构OSDOp是一个操作上下文的封装。结构object_info_t保存了一个元数据信息和访问信息。对象ObjectState是在object_info_t基础上添加了一些内存的状态信息。SnapSetContext和ObjectContext分别保存了快照和对象上下文相关的信息。Session保存了一个端到端的链接相关的上下文。 PoolPool是整个集群层面定义的一个逻辑的存储池。对一个Pool可以设置相应的数据冗余类型，目前有副本和纠删码两种实现。数据结构pg_pool_t用于保存Pool的相关信息。Pool的数据结构如下：（src&#x2F;osd&#x2F;osd_types.h） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150struct pg_pool_t &#123; static const char *APPLICATION_NAME_CEPHFS; static const char *APPLICATION_NAME_RBD; static const char *APPLICATION_NAME_RGW; enum &#123; TYPE_REPLICATED = 1, // replication 副本 //TYPE_RAID4 = 2, // raid4 (never implemented) 从来没实现的raid4 TYPE_ERASURE = 3, // erasure-coded 纠删码 &#125;; enum &#123; FLAG_HASHPSPOOL = 1&lt;&lt;0, // hash pg seed and pool together (instead of adding) FLAG_FULL = 1&lt;&lt;1, // pool is full FLAG_EC_OVERWRITES = 1&lt;&lt;2, // enables overwrites, once enabled, cannot be disabled FLAG_INCOMPLETE_CLONES = 1&lt;&lt;3, // may have incomplete clones (bc we are/were an overlay) FLAG_NODELETE = 1&lt;&lt;4, // pool can&#x27;t be deleted FLAG_NOPGCHANGE = 1&lt;&lt;5, // pool&#x27;s pg and pgp num can&#x27;t be changed FLAG_NOSIZECHANGE = 1&lt;&lt;6, // pool&#x27;s size and min size can&#x27;t be changed FLAG_WRITE_FADVISE_DONTNEED = 1&lt;&lt;7, // write mode with LIBRADOS_OP_FLAG_FADVISE_DONTNEED FLAG_NOSCRUB = 1&lt;&lt;8, // block periodic scrub FLAG_NODEEP_SCRUB = 1&lt;&lt;9, // block periodic deep-scrub FLAG_FULL_QUOTA = 1&lt;&lt;10, // pool is currently running out of quota, will set FLAG_FULL too FLAG_NEARFULL = 1&lt;&lt;11, // pool is nearfull FLAG_BACKFILLFULL = 1&lt;&lt;12, // pool is backfillfull FLAG_SELFMANAGED_SNAPS = 1&lt;&lt;13, // pool uses selfmanaged snaps FLAG_POOL_SNAPS = 1&lt;&lt;14, // pool has pool snaps FLAG_CREATING = 1&lt;&lt;15, // initial pool PGs are being created &#125;; utime_t create_time; //Pool创建时间 uint64_t flags; ///&lt; FLAG_* Pool的相关标志 __u8 type; ///&lt; TYPE_* 类型 __u8 size, min_size; ///&lt;Pool的size和min_size，即副本数和至少保证的副本数 __u8 crush_rule; ///&lt; crush placement rule rule的编号 __u8 object_hash; ///&lt; hash mapping object name to ps 对象映射的hash函数 __u8 pg_autoscale_mode; ///&lt; PG_AUTOSCALE_MODE_ PG数自动增减模式private: __u32 pg_num = 0, pgp_num = 0; ///&lt; pg、pgp的数量 __u32 pg_num_pending = 0; ///&lt; pg_num we are about to merge down to __u32 pg_num_target = 0; ///&lt; pg_num we should converge toward __u32 pgp_num_target = 0; ///&lt; pgp_num we should converge towardpublic: map&lt;string,string&gt; properties; ///&lt; OBSOLETE string erasure_code_profile; ///&lt; name of the erasure code profile in OSDMap epoch_t last_change; ///&lt; most recent epoch changed, exclusing snapshot changes /// last epoch that forced clients to resend epoch_t last_force_op_resend = 0; /// last epoch that forced clients to resend (pre-nautilus clients only) epoch_t last_force_op_resend_prenautilus = 0; /// last epoch that forced clients to resend (pre-luminous clients only) epoch_t last_force_op_resend_preluminous = 0; /// metadata for the most recent PG merge pg_merge_meta_t last_pg_merge_meta; snapid_t snap_seq; ///&lt; seq for per-pool snapshot epoch_t snap_epoch; ///&lt; osdmap epoch of last snap uint64_t auid; ///&lt; who owns the pg uint64_t quota_max_bytes; ///&lt; maximum number of bytes for this pool uint64_t quota_max_objects; ///&lt; maximum number of objects for this pool /* * Pool snaps (global to this pool). These define a SnapContext for * the pool, unless the client manually specifies an alternate * context. */ map&lt;snapid_t, pool_snap_info_t&gt; snaps; /* * Alternatively, if we are defining non-pool snaps (e.g. via the * Ceph MDS), we must track @removed_snaps (since @snaps is not * used). Snaps and removed_snaps are to be used exclusive of each * other! */ interval_set&lt;snapid_t&gt; removed_snaps; unsigned pg_num_mask, pgp_num_mask; // Tier cache : Base Storage = N : 1 // ceph osd tier add &#123;data_pool&#125; &#123;cache pool&#125; set&lt;uint64_t&gt; tiers; ///&lt; pools that are tiers of us int64_t tier_of; ///&lt; pool for which we are a tier // Note that write wins for read+write ops // WriteBack mode, read_tier is same as write_tier. Both are cache pool. // Diret mode. cache pool is read_tier, not write_tier. // ceph osd tier set-overlay &#123;data_pool&#125; &#123;cache_pool&#125; int64_t read_tier; ///&lt; pool/tier for objecter to direct reads to int64_t write_tier; ///&lt; pool/tier for objecter to direct writes to // Set cache mode // ceph osd tier cache-mode &#123;cache-pool&#125; &#123;cache-mode&#125; cache_mode_t cache_mode; ///&lt; cache pool mode uint64_t target_max_bytes; ///&lt; tiering: target max pool size uint64_t target_max_objects; ///&lt; tiering: target max pool size // 目标脏数据率：当脏数据比例达到这个值，后台 agent 开始 flush 数据 uint32_t cache_target_dirty_ratio_micro; ///&lt; cache: fraction of target to leave dirty // 高目标脏数据率：当脏数据比例达到这个值，后台 agent 开始高速 flush 数据 uint32_t cache_target_dirty_high_ratio_micro; ///&lt; cache: fraction of target to flush with high speed // 数据满的比率：当数据达到这个比例时，认为数据已满，需要进行缓存淘汰 uint32_t cache_target_full_ratio_micro; ///&lt; cache: fraction of target to fill before we evict in earnest // 对象在 cache 中被刷入到 storage 层的最小时间 uint32_t cache_min_flush_age; ///&lt; minimum age (seconds) before we can flush // 对象在 cache 中被淘汰的最小时间 uint32_t cache_min_evict_age; ///&lt; minimum age (seconds) before we can evict // HitSet 相关参数 HitSet::Params hit_set_params; ///&lt; The HitSet params to use on this pool // 每间隔 hit_set_period 一段时间，系统重新产生一个新的 hit_set 对象来记录对象的h缓存统计信息 uint32_t hit_set_period; ///&lt; periodicity of HitSet segments (seconds) // 记录系统保存最近的多少个 hit_set 记录 uint32_t hit_set_count; ///&lt; number of periods to retain // hitset archive 对象的命名规则 bool use_gmt_hitset; ///&lt; use gmt to name the hitset archive object uint32_t min_read_recency_for_promote; ///&lt; minimum number of HitSet to check before promote on read uint32_t min_write_recency_for_promote; ///&lt; minimum number of HitSet to check before promote on write uint32_t hit_set_grade_decay_rate; ///&lt; current hit_set has highest priority on objects ///&lt; temperature count,the follow hit_set&#x27;s priority decay ///&lt; by this params than pre hit_set //当前hit_set在对象温度计数上具有最高优先级，后续hit_set的优先级比预hit_set衰减此参数 uint32_t hit_set_search_last_n; ///&lt; accumulate atmost N hit_sets for temperature 为温度累积最多N次hit_sets uint32_t stripe_width; ///&lt; erasure coded stripe size in bytes uint64_t expected_num_objects; ///&lt; expected number of objects on this pool, a value of 0 indicates ///&lt; user does not specify any expected value bool fast_read; ///&lt; whether turn on fast read on the pool or not pool_opts_t opts; ///&lt; options /// application -&gt; key/value metadata map&lt;string, std::map&lt;string, string&gt;&gt; application_metadata;private: vector&lt;uint32_t&gt; grade_table;public: uint32_t get_grade(unsigned i) const &#123; if (grade_table.size() &lt;= i) return 0; return grade_table[i]; &#125; void calc_grade_table() &#123; unsigned v = 1000000; grade_table.resize(hit_set_count); // hit_set_count记录系统保存最近的多少个 hit_set 记录 for (unsigned i = 0; i &lt; hit_set_count; i++) &#123; v = v * (1 - (hit_set_grade_decay_rate / 100.0)); grade_table[i] = v; &#125; &#125;&#125;; 数据结构pg_pool_t的成员变量和方法较多，不一一介绍了。 PGPG可以认为是一组对象的集合，该集合里的对象有共同特征：副本都分布在相同的OSD列表中。结构体pg_t只是一个PG的静态描述信息（只有三个成员变量），类PG及其子类ReplicatedPG都是和PG相关的处理。pg_t的数据结构如下：（src&#x2F;osd&#x2F;osd_types.h） 12345678struct pg_t &#123; uint64_t m_pool; //pg所在的pool uint32_t m_seed; //pg的序号 static const uint8_t calc_name_buf_size = 36; // max length for max values len(&quot;18446744073709551615.ffffffff&quot;) + future suffix len(&quot;_head&quot;) + &#x27;\\0&#x27; hobject_t get_hobj_start() const; hobject_t get_hobj_end(unsigned pg_num) const; static void generate_test_instances(list&lt;pg_t*&gt;&amp; o);&#125;; OSDMapOSDMap类定义了Ceph整个集群的全局信息。它由Monitor实现管理，并以全量或者增量的方式向整个集群扩散。每一个epoch对应的OSDMap都需要持久化保存在meta下对应对象的omap属性中。内部类Incremental以增量的形式保存了OSDMap新增的信息。OSDMap包含了四类信息：首先是集群的信息，其次是pool的信息，然后是临时PG相关信息，最后就是所有OSD的状态信息。OSDMap类的数据结构如下：（src&#x2F;osd&#x2F;OSDMap.h） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889class OSDMap &#123;public: MEMPOOL_CLASS_HELPERS(); typedef interval_set&lt; snapid_t, mempool::osdmap::flat_map&lt;snapid_t,snapid_t&gt;&gt; snap_interval_set_t; class Incremental &#123; public: MEMPOOL_CLASS_HELPERS(); //系统相关的信息 /// feature bits we were encoded with. the subsequent OSDMap /// encoding should match. uint64_t encode_features; uuid_d fsid; //当前集群的fsid值 epoch_t epoch; //当前集群的epoch值 new epoch; we are a diff from epoch-1 to epoch utime_t modified; //创建修改的时间戳 int64_t new_pool_max; //incremented by the OSDMonitor on each pool create int32_t new_flags; int8_t new_require_osd_release = -1; // full (rare) bufferlist fullmap; // in lieu of below. bufferlist crush;......private: //集群相关的信息 uuid_d fsid; //当前集群的fsid值 epoch_t epoch; //当前集群的epoch值 what epoch of the osd cluster descriptor is this utime_t created, modified; //创建、修改的时间戳 epoch start time int32_t pool_max; //最大的pool数量 the largest pool num, ever uint32_t flags; //一些标志信息 //OSD相关的信息 int num_osd; //OSD的总数量 not saved; see calc_num_osds int num_up_osd; //处于up状态的OSD的数量 not saved; see calc_num_osds int num_in_osd; //处于in状态的OSD的数量 not saved; see calc_num_osds int32_t max_osd; //OSD的最大数目 vector&lt;uint32_t&gt; osd_state; //OSD的状态 mempool::osdmap::map&lt;int32_t,uint32_t&gt; crush_node_flags; // crush node -&gt; CEPH_OSD_* flags mempool::osdmap::map&lt;int32_t,uint32_t&gt; device_class_flags; // device class -&gt; CEPH_OSD_* flags utime_t last_up_change, last_in_change; // These features affect OSDMap[::Incremental] encoding, or the // encoding of some type embedded therein (CrushWrapper, something // from osd_types, etc.). static constexpr uint64_t SIGNIFICANT_FEATURES = CEPH_FEATUREMASK_PGID64 | CEPH_FEATUREMASK_PGPOOL3 | CEPH_FEATUREMASK_OSDENC | CEPH_FEATUREMASK_OSDMAP_ENC | CEPH_FEATUREMASK_OSD_POOLRESEND | CEPH_FEATUREMASK_NEW_OSDOP_ENCODING | CEPH_FEATUREMASK_MSG_ADDR2 | CEPH_FEATUREMASK_CRUSH_TUNABLES5 | CEPH_FEATUREMASK_CRUSH_CHOOSE_ARGS | CEPH_FEATUREMASK_SERVER_LUMINOUS | CEPH_FEATUREMASK_SERVER_MIMIC | CEPH_FEATUREMASK_SERVER_NAUTILUS; struct addrs_s &#123; mempool::osdmap::vector&lt;std::shared_ptr&lt;entity_addrvec_t&gt; &gt; client_addrs; mempool::osdmap::vector&lt;std::shared_ptr&lt;entity_addrvec_t&gt; &gt; cluster_addrs; mempool::osdmap::vector&lt;std::shared_ptr&lt;entity_addrvec_t&gt; &gt; hb_back_addrs; mempool::osdmap::vector&lt;std::shared_ptr&lt;entity_addrvec_t&gt; &gt; hb_front_addrs; &#125;; std::shared_ptr&lt;addrs_s&gt; osd_addrs; //OSD的地址 entity_addrvec_t _blank_addrvec; mempool::osdmap::vector&lt;__u32&gt; osd_weight; //OSD的权重 16.16 fixed point, 0x10000 = &quot;in&quot;, 0 = &quot;out&quot; mempool::osdmap::vector&lt;osd_info_t&gt; osd_info; //OSD 的基本信息 std::shared_ptr&lt; mempool::osdmap::vector&lt;uuid_d&gt; &gt; osd_uuid; //OSD对应的uuid mempool::osdmap::vector&lt;osd_xinfo_t&gt; osd_xinfo; //OSD一些扩展信息 //PG相关的信息 std::shared_ptr&lt;PGTempMap&gt; pg_temp; // temp pg mapping (e.g. while we rebuild) std::shared_ptr&lt; mempool::osdmap::map&lt;pg_t,int32_t &gt; &gt; primary_temp; // temp primary mapping (e.g. while we rebuild) std::shared_ptr&lt; mempool::osdmap::vector&lt;__u32&gt; &gt; osd_primary_affinity; ///&lt; 16.16 fixed point, 0x10000 = baseline // remap (post-CRUSH, pre-up) mempool::osdmap::map&lt;pg_t,mempool::osdmap::vector&lt;int32_t&gt;&gt; pg_upmap; ///&lt; remap pg mempool::osdmap::map&lt;pg_t,mempool::osdmap::vector&lt;pair&lt;int32_t,int32_t&gt;&gt;&gt; pg_upmap_items; ///&lt; remap osds in up set //pool的相关信息 mempool::osdmap::map&lt;int64_t,pg_pool_t&gt; pools; //pool的id到pg_pool_t的映射 mempool::osdmap::map&lt;int64_t,string&gt; pool_name; //pool的id到pool的名字的映射 mempool::osdmap::map&lt;string,map&lt;string,string&gt; &gt; erasure_code_profiles; //pool的EC相关信息 mempool::osdmap::map&lt;string,int64_t&gt; name_pool; //pool的名字到pool的id的映射 Op结构体Op封装了完成一个操作的相关上下文信息，包括target地址信息(op_target_t)、链接信息(session)等 123456789101112131415161718192021222324252627282930313233343536//Op封装了完成一个操作的相关的上下文信息，包括target地址信息、链接信息等。 struct Op : public RefCountedObject &#123; OSDSession *session; //OSD相关的Session信息 int incarnation; //引用次数 op_target_t target; //地址信息 ConnectionRef con; // for rx buffer only uint64_t features; // explicitly specified op features vector&lt;OSDOp&gt; ops; // 对应多个操作的封装 snapid_t snapid; //快照的ID SnapContext snapc; //pool层级的快照信息 ceph::real_time mtime; bufferlist *outbl; //输出的bufferlist vector&lt;bufferlist*&gt; out_bl; //每个操作对应的bufferlist vector&lt;Context*&gt; out_handler; //每个操作对应的回调函数 vector&lt;int*&gt; out_rval; //每个操作对应的输出结果 int priority; Context *onfinish; uint64_t ontimeout; ceph_tid_t tid; int attempts; version_t *objver; epoch_t *reply_epoch; ceph::coarse_mono_time stamp; epoch_t map_dne_bound; int budget; /// true if we should resend this message on failure bool should_resend; /// true if the throttle budget is get/put on a series of OPs, /// instead of per OP basis, when this flag is set, the budget is /// acquired before sending the very first OP of the series and /// released upon receiving the last OP reply. bool ctx_budgeted; int *data_offset; osd_reqid_t reqid; // explicitly setting reqid ZTracer::Trace trace; op_target_t数据结构op_target_t封装了对象所在的PG，以及PG对应的OSD列表等地址信息。 12345678910111213141516171819202122232425262728293031//封装了对象所在的PG，以及PG对应的OSD列表等地址信息 struct op_target_t &#123; int flags = 0; //标志 epoch_t epoch = 0; ///&lt; latest epoch we calculated the mapping object_t base_oid; //读取的对象 object_locator_t base_oloc; //对象的pool信息 object_t target_oid; //最终读取的目标对象 object_locator_t target_oloc; //最终目标对象的pool信息 ///&lt; true if we are directed at base_pgid, not base_oid bool precalc_pgid = false; ///&lt; true if we have ever mapped to a valid pool bool pool_ever_existed = false; ///&lt; explcit pg target, if any pg_t base_pgid; pg_t pgid; ///&lt; last (raw) pg we mapped to spg_t actual_pgid; ///&lt; last (actual) spg_t we mapped to unsigned pg_num = 0; ///&lt; last pg_num we mapped to unsigned pg_num_mask = 0; ///&lt; last pg_num_mask we mapped to unsigned pg_num_pending = 0; ///&lt; last pg_num we mapped to vector&lt;int&gt; up; ///&lt; set of up osds for last pg we mapped to vector&lt;int&gt; acting; ///&lt; set of acting osds for last pg we mapped to int up_primary = -1; ///&lt; last up_primary we mapped to int acting_primary = -1; ///&lt; last acting_primary we mapped to int size = -1; ///&lt; the size of the pool when were were last mapped int min_size = -1; ///&lt; the min size of the pool when were were last mapped bool sort_bitwise = false; ///&lt; whether the hobject_t sort order is bitwise bool recovery_deletes = false; ///&lt; whether the deletes are performed during recovery instead of peering bool used_replica = false; bool paused = false; int osd = -1; ///&lt; the final target osd, or -1 epoch_t last_force_resend = 0; CRUSH Map123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146struct crush_rule_step &#123; __u32 op; //操作类型 __s32 arg1; //操作数1 __s32 arg2; //操作数2&#125;;enum crush_opcodes &#123; CRUSH_RULE_NOOP = 0, CRUSH_RULE_TAKE = 1, /* arg1 = value to start with */ CRUSH_RULE_CHOOSE_FIRSTN = 2, /* arg1 = num items to pick */ /* arg2 = type */ CRUSH_RULE_CHOOSE_INDEP = 3, /* same */ CRUSH_RULE_EMIT = 4, /* no args */ CRUSH_RULE_CHOOSELEAF_FIRSTN = 6, CRUSH_RULE_CHOOSELEAF_INDEP = 7, CRUSH_RULE_SET_CHOOSE_TRIES = 8, /* override choose_total_tries */ CRUSH_RULE_SET_CHOOSELEAF_TRIES = 9, /* override chooseleaf_descend_once */ CRUSH_RULE_SET_CHOOSE_LOCAL_TRIES = 10, CRUSH_RULE_SET_CHOOSE_LOCAL_FALLBACK_TRIES = 11, CRUSH_RULE_SET_CHOOSELEAF_VARY_R = 12, CRUSH_RULE_SET_CHOOSELEAF_STABLE = 13&#125;;/* * 用于指定相对于传递给 do_rule 的 max 参数的选择 num (arg1) */#define CRUSH_CHOOSE_N 0#define CRUSH_CHOOSE_N_MINUS(x) (-(x))/* * 规则掩码用于描述规则的用途。 * 给定规则集和输出集的大小，我们在规则列表中搜索匹配的 rule_mask。 */struct crush_rule_mask &#123; __u8 ruleset; //ruleId __u8 type; //多副本还是纠删码 __u8 min_size; //副本数大于等于时适用 __u8 max_size; //副本数小于等于时适用&#125;;struct crush_rule &#123; __u32 len; //steps数组的长度 struct crush_rule_mask mask; //releset相关的配置参数 struct crush_rule_step steps[0]; //step集合&#125;;#define crush_rule_size(len) (sizeof(struct crush_rule) + \\ (len)*sizeof(struct crush_rule_step))/* * A bucket is a named container of other items (either devices or * other buckets). * 桶是其他item（设备或其他存储桶）的命名容器 *//** * 使用三种算法中的一种来选择的，这些算法代表了性能和重组效率之间的权衡。 * 如果您不确定要使用哪种存储桶类型，我们建议您使用 ::CRUSH_BUCKET_STRAW2。 * 该表总结了在添加或删除item时每个选项的速度如何与映射稳定性相比较。 * Bucket Alg Speed Additions Removals * ------------------------------------------------ * uniform O(1) poor poor * list O(n) optimal poor * straw2 O(n) optimal optimal */enum crush_algorithm &#123; CRUSH_BUCKET_UNIFORM = 1, CRUSH_BUCKET_LIST = 2, CRUSH_BUCKET_TREE = 3, CRUSH_BUCKET_STRAW = 4, CRUSH_BUCKET_STRAW2 = 5,&#125;;extern const char *crush_bucket_alg_name(int alg);#define CRUSH_LEGACY_ALLOWED_BUCKET_ALGS ( \\ (1 &lt;&lt; CRUSH_BUCKET_UNIFORM) | \\ (1 &lt;&lt; CRUSH_BUCKET_LIST) | \\ (1 &lt;&lt; CRUSH_BUCKET_STRAW))struct crush_bucket &#123; __s32 id; //bucket的编号。小于0 /*!&lt; bucket identifier, &lt; 0 and unique within a crush_map */ __u16 type; //bucket的类型/*!&lt; &gt; 0 bucket type, defined by the caller */ __u8 alg; //使用的crush算法/*!&lt; the item selection ::crush_algorithm */ __u8 hash; //使用的hash算法/* which hash function to use, CRUSH_HASH_* */ __u32 weight; //权重 /*!&lt; 16.16 fixed point cumulated children weight */ __u32 size; //items的数量/*!&lt; size of the __items__ array */ __s32 *items; //子bucket/*!&lt; array of children: &lt; 0 are buckets, &gt;= 0 items */&#125;;struct crush_weight_set &#123; __u32 *weights; /*!&lt; 16.16 fixed point weights in the same order as items */ __u32 size; /*!&lt; size of the __weights__ array */&#125;;struct crush_choose_arg &#123; __s32 *ids; /*!&lt; values to use instead of items */ __u32 ids_size; /*!&lt; size of the __ids__ array */ struct crush_weight_set *weight_set; /*!&lt; weight replacements for a given position */ __u32 weight_set_positions; /*!&lt; size of the __weight_set__ array */&#125;;struct crush_choose_arg_map &#123; struct crush_choose_arg *args; /*!&lt; replacement for each bucket in the crushmap */ __u32 size; /*!&lt; size of the __args__ array */&#125;;struct crush_bucket_uniform &#123; struct crush_bucket h; /*!&lt; generic bucket information */ __u32 item_weight; /*!&lt; 16.16 fixed point weight for each item */&#125;;struct crush_bucket_list &#123; struct crush_bucket h; /*!&lt; generic bucket information */ __u32 *item_weights; /*!&lt; 16.16 fixed point weight for each item */ __u32 *sum_weights; /*!&lt; 16.16 fixed point sum of the weights */&#125;;struct crush_bucket_tree &#123; struct crush_bucket h; /* note: h.size is _tree_ size, not number of actual items */ __u8 num_nodes; __u32 *node_weights;&#125;;struct crush_bucket_straw &#123; struct crush_bucket h; __u32 *item_weights; /* 16-bit fixed point */ __u32 *straws; /* 16-bit fixed point */&#125;;struct crush_bucket_straw2 &#123; struct crush_bucket h; /*!&lt; generic bucket information */ __. /*!&lt; 16.16 fixed point weight for each item */&#125;;struct crush_map &#123; struct crush_bucket **buckets; **类型，所有的bucket都存在这里 /*! 一个大小为__max_rules__ 的crush_rule 指针数组。 * 如果规则被删除，数组的一个元素可能为NULL（没有API 可以这样做，但将来可能会有一个）。 * 规则必须使用crunch_add_rule() 添加。 */ struct crush_rule **rules; //**类型，多层嵌套的rules __s32 max_buckets; /*!&lt; the size of __buckets__ */ // bucket的总数 __u32 max_rules; /*!&lt; the size of __rules__ */ // rule的总数 __s32 max_devices; // osd的总数 __u32 choose_local_tries; //选择的总次数 __u32 choose_local_fallback_tries; __u32 choose_total_tries; __u32 chooseleaf_descend_once; __u8 chooseleaf_vary_r; __u8 chooseleaf_stable; /* 该值是在构建器解码或构建后计算的。 它在此处公开（而不是具有“构建 CRUSH 工作空间”功能），以便调用者可以保留静态缓冲区、在堆栈上分配空间，或者在需要时避免调用堆分配器。 工作空间的大小取决于映射，而传递给映射器的临时向量的大小取决于所需结果集的大小。尽管如此，没有什么能阻止调用者在一个膨胀 foop 中分配两个点并传递两个点。 */ size_t working_size;","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph源码编译调试","slug":"Ceph源码编译调试","date":"2021-06-20T07:24:13.000Z","updated":"2024-07-27T14:38:39.225Z","comments":true,"path":"Ceph源码编译调试/","permalink":"https://watsonlu6.github.io/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95/","excerpt":"","text":"对于一个ceph开发人员来说编译源码以及打rpm是其必备技能。无论是fix bug还是向社区提交pull request都离不开编译源码。 编译环境环境介绍 ceph version: N版 14.2.16 硬件环境：Centos7虚拟机 网络环境与源加速 12345678910111213141516171819202122232425262728293031323334353637383940414243# 额外软件源、生成新的缓存yum -y install centos-release-sclyum -y install epel-release yum clean all &amp;&amp; yum makecacheyum listyum update# 更换pip源，创建 .pip 目录mkdir ~/.pip cd ~/.pip vi pip.conf# 写入以下配置[global]index-url = https://mirrors.aliyun.com/pypi/simple/[install]trusted-host=mirrors.aliyun.com#配置yum源vim /etc/yum.repos.d/ceph.repo[norch]name=norchbaseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/enabled=1gpgcheck=0[x86_64]name=x86 64baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/enabled=1gpgcheck=0[SRPMS]name=SRPMSbaseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/SRPMS/enabled=1gpgcheck=0[aarch64]name=aarch64baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/aarch64/enabled=1gpgcheck=0 安装编译环境及依赖包 123456789101112131415yum -y install rdma-core-devel systemd-devel keyutils-libs-devel openldap-devel leveldb-devel snappy-devel lz4-devel curl-devel nss-develyum -y install libzstd zstd gcc cmake make git wgetyum -y install devtoolset-7-gcc devtoolset-7-gcc-c++ devtoolset-7-binutils # 安装gcc 7.2scl enable devtoolset-7 bash #临时生效source /opt/rh/devtoolset-7/enableecho &quot;source /opt/rh/devtoolset-7/enable&quot; &gt;&gt;/etc/profile #长期生效gcc -v #查看环境gcc版本wget https://github.com/Kitware/CMake/releases/download/v3.18.2/cmake-3.18.2.tar.gz #安装cmake3tar -zxvf cmake-3.18.2.tar.gzcd cmake-3.18.2 yum -y install ncurses-devel openssl-devel./bootstrapgmake &amp;&amp; gmake installln -s /usr/local/share/cmake /usr/bin/cmake -version 安装 ccache 加速编译 1234567891011121314151617181920# 下载安装包并解压mkdir /home/ccache cd /home/ccachewget https://github.com/ccache/ccache/releases/download/v4.0/ccache-4.0.tar.gztar -zxvf ccache-4.0.tar.gzcd ccache-4.0# 编译安装mkdir build cd buildcmake -DCMAKE_BUILD_TYPE=Release -DZSTD_FROM_INTERNET=ON ..make -j12make install# 修改配置mkdir -p /root/.config/ccache/ vi /root/.config/ccache/ccache.confmax_size = 16Gsloppiness = time_macrosrun_second_cpp = true 编译ceph代码1234567891011121314151617181920212223242526272829303132## 下载Ceph源码一mkdir /home/cephcd /home/cephgit clone git://github.com/ceph/ceph.git #(git clone https://github.com/ceph/ceph.git)cd cephgit checkout nautilus #切换分支，这里以 N 版本为例git submodule update --init --recursive #进入ceph目录，下载ceph代码依赖 ## 下载Ceph源码二wget https://mirrors.aliyun.com/ceph/debian-nautilus/pool/main/c/ceph/ceph_14.2.22.orig.tar.gztar -zxvf ceph_14.2.22.orig.tar.gzcd ceph_14.2.2./install-deps.sh #执行依赖安装脚本，ceph 自带的解决依赖的脚本## 修改cmake参数，因为后面需要使用gdb debug客户端程序，客户端程序会依赖librados库，所以我们必须以debug的模式去编译ceph，否则编译器会优化掉很多参数，导致很多信息缺失，需要修改一下ceph cmake的参数。如图所示vim do_cmake.sh $&#123;CMAKE&#125; -DCMAKE_C_FLAGS=&quot;-O0 -g3 -gdwarf-4&quot; -DCMAKE_CXX_FLAGS=&quot;-O0 -g3 -gdwarf-4&quot; -DBOOST_J=$(nproc) $ARGS &quot;$@&quot; ..# 可以看到这里修改了cmake的参数，增加了两个配置项，稍微解释一下# CMAKE_C_FLAGS=“-O0 -g3 -gdwarf-4” ： c 语言编译配置# CMAKE_CXX_FLAGS=“-O0 -g3 -gdwarf-4” ：c++ 编译配置# -O0 : 关闭编译器的优化，如果没有，使用GDB追踪程序时，大多数变量被优化,无法显示, 生产环境必须关掉# -g3 : 意味着会产生大量的调试信息# -gdwarf-4 : dwarf 是一种调试格式，dwarf-4 版本为4 ./do_cmake.sh -DWITH_MANPAGE=OFF -DWITH_BABELTRACE=OFF -DWITH_MGR_DASHBOARD_FRONTEND=OFF -DCMAKE_BUILD_TYPE=RelWithDebInfo -DWITH_CCACHE=ON --DWITH_PYTHON3=ON --DMGR_PYTHON_VERSION=3# 执行 cmake，解释一下，DWITH_MGR_DASHBOARD_FRONTEND=OFF 主要是因为 ceph dashboard 用到了一些国外的 nodejs源，国内无法下载，会导致编译失败超时。-DWITH_CCACHE=ON 如果你没有安装 步骤 2-2 的 ccache 的话，可以去掉这个参数。 cd buildmake -j20 #（线程数等于cpu core的2倍，可以提高编译的速度，20核CPU、32G内存的服务器） 修改do_cmake.sh编译进度 自此已经编译完ceph源代码！ 运行测试集群发行版的 ceph 安装包安装的集群默认是没有办法debug调试。这里推荐 ceph 内置的debug调试——vstart，非常方便模仿特殊场景进行debug调试。 12345678cd /home/watson/ceph/build # 进入build目录make vstart # 编译模拟启动环境（make help 查看有哪些target可以单独编译）MDS=0 RGW=1 ../src/vstart.sh -d -l -n --bluestore # (模拟启动，指令前半部分的MDS=0 RGW=1之类的就是设定你想要模拟的集群结构（集群的配置文件在ceph/build/ceph.conf）)# 启动完成后，可以在模拟集群环境下执行各种 ceph 指令(模拟集群所有的指令都在 build/bin 目录)bin/ceph -s # 查看 ceph 集群状态bin/radosgw-admin user list # 查看用户../src/stop.sh # 关闭测试集群 编译vstasrt环境启动vstart环境查看 ceph 集群状态查看Ceph用户 运行单元测试用例更改了代码准备提交到公司内部repo或者社区repo都需要先执行一下最小测试集，看看自己修改的代码有没有影响到别的模块(社区也会进行同样的测试)。 1234567cd buildmake #修改代码后先编译，可以模块编译man ctest #查看ctest的功能ctest -j20 #运行所有测试（使用所有处理器并行）ctest -R [regex matching test name(s)] #运行部分模块测试，使用 -R（正则表达式匹配）ctest -V -R [regex matching test name(s)] #使用 -V（详细）标志运行ctest -j20 -V -R [regex matching test name(s)] #运行正则表达式匹配的模块测试，显示详细信息，并发进行 注意：许多从 src&#x2F;test 构建的目标不是使用ctest运行的。以 “unittest” 开头的目标在其中运行make check，因此可以使用运行ctest。以 “ceph_test” 开头的目标不能，应该手动运行。发生故障时，请在 build&#x2F;Testing&#x2F;Temporary 中查找日志。 开发编译测试过程 1231. 编写保存源代码2. make -j20 unittest_crush #模块编译3. ctest -j20 -V -R unittest_crush #模块测试 通过librados客户端调试CRUSH算法编写客户端代码调用librados 库写入数据 运行librados代码 12yum install librados2-devel libradospp libradosstriper-devel -y #安装相关开发包（C/C++开发包）gcc -g rados_write.c -lrados -L/home/watson/ceph/build/lib -o rados_write -Wl,-rpath,/home/watson/ceph/build/lib #编译客户端程序 rados_write.c 这里解释一下gcc 几个参数，首先需要理解的是c程序在编译时依赖的库和运行时依赖库是分开指定的，也就是说，编译的时候使用的库，不一定就是运行时使用的库 g : 允许gdb调试 lrados : -l 指定依赖库的名字为rados L : 指定编译时依赖库的的路径， 如果不指定将在系统目录下寻找 o : 编译的二进制文件名 Wl : 指定编译时参数 rpath : 指定运行时依赖库的路径， 如果不指定将在系统目录下寻找 运行客户端程序 12./rados_writebin/rados ls -p default.rgw.meta #在集群中确认一下是否写入数据 运行rados_write程序确认写入数据 ceph的开发者模式是测试ceph功能和调试代码非常方便的途径，因为集群默认开启了debug模式，所有的日志都会详细的输出，并且为了调试的方便，在正式环境中的多线程多队列，在这都会简化。 使用GDB调试分析Object至OSD映射1234567yum install -y gdb #安装gdbgcc -g rados_write.c -lrados -L/home/watson/ceph/build/lib -o rados_write -Wl,-rpath,/home/watson/ceph/build/lib #编译客户端程序 rados_write.cgdb ./rados_write #使用gdb 调试 rados_write 程序#启动程序后，需要设置断点，这里选择的是 crush_do_rule 函数，因为这个函数是 object–&gt;到PG 流程的终点b crush_do_rule #在crush_do_rule 函数设置断点bt #查看当前的函数堆栈 gdb调试raodos_wirte程序设置调试断点查看当前函数栈 得到的函数流程如下 12345678910111213#0 crush_do_rule at /home/watson/ceph/src/crush/mapper.c:904#1 do_rule at /home/watson/ceph/src/crush/CrushWrapper.h:1570#2 OSDMap::_pg_to_raw_osds at /home/watson/ceph/src/osd/OSDMap.cc:2340#3 OSDMap::_pg_to_up_acting_osds at /home/watson/ceph/src/osd/OSDMap.cc:2586#4 pg_to_up_acting_osds at /home/watson/ceph/src/osd/OSDMap.h:1209#5 Objecter::_calc_target at /home/watson/ceph/src/osdc/Objecter.cc:2846#6 Objecter::_op_submit at /home/watson/ceph/src/osdc/Objecter.cc:2367#7 Objecter::_op_submit_with_budget at /home/watson/ceph/src/osdc/Objecter.cc:2284#8 Objecter::op_submit at /home/watson/ceph/src/osdc/Objecter.cc:2251#9 librados::IoCtxImpl::operate at /home/watson/ceph/src/librados/IoCtxImpl.cc:690#10 librados::IoCtxImpl::write at /home/watson/ceph/src/librados/IoCtxImpl.cc:623#11 rados_write at /home/watson/ceph/src/librados/librados_c.cc:1133#12 main at rados_write.c:73 不关心librados是如何封装请求，只关心object到pg的计算过程，所以这里决定从 Objecter::_calc_target 函数开始debug 整个过程，重新开始，然后再次设置断点。重新开始，计算 object的hash值 ps 1b Objecter::_calc_target #断点 卡住在断点处，现在我们打开tui模式跟踪代码， crtl + x + a 可以切换到tui界面这里按 n 逐行debug代码， 这里我想显示打印 pg_pool_t *p 和 op_target_t *t 的信息其中 pg_pool_t 是pool的结构体，包含pool相关的所有信息 1p *pi #查看pi的数据结构 而 op_target_t 则是整个写入操作封装的结构信息，包含对象的名字，写入pool的id继续 n 单步调试，这里我们会进去 osdmap-&gt;object_locator_to_pg 函数。然后一步一步调试……object到PG的函数流程图PG映射到OSD函数流程图crush_choose_firstn选择的过程 使用VScode远程调试Ceph以ceph osd部分为例，为您演示通过第三方社区提供的vscode 编辑软件，对ceph osd进行进行图形化单步调试以及配置操作。vscode是微软公司一个开源的编译器具备轻量的特点，通过插件安装方式提供了丰富的调试功能。通常 Linux环境的c&#x2F;c++软件开发使用GDB进行命令行调试，命令行操方式极其不方便。使用vscode 的图形化界面可替代gdb 命令行 ，整个开发调试过程更加便捷。Ceph源码路径在&#x2F;home&#x2F;watson&#x2F;ceph目录下，其编译运行文件在&#x2F;home&#x2F;watson&#x2F;ceph&#x2F;build&#x2F;bin当中。启动调试前需要停止本地的osd运行服务。下载安装windows的vscode和ssh在以下地址下载vscode: https://code.visualstudio.com/安装openssh (一般情况不用自己手动安装)如果需要远程开发，Windows机器也需要支持openssh，如果本机没有，会报错。可以到微软官网上下载ssh。在vscode安装Remote Development和Remote-SSH在安装完成之后，点击左侧的Remote-SSH选项卡，再将鼠标移向CONNECTIONS栏，点击出现的configure：填写linux服务器的ssh端口和用户名（如果是默认的22端口可不用填写）按下ctrl + s 保存 然后连接（&#x2F;home&#x2F;watson&#x2F;ceph&#x2F;）输入密码，总共有多次输入密码的流程留意窗口变化打开远程服务器的文件夹 远程连接遇到的问题以及技巧 因为ceph工程文件数量众多会出现无法在这个大型工作区中监视文件更改。请按照说明链接来解决此问题的问题。原因：工作区很大并且文件很多，导致VS Code文件观察程序的句柄达到上限。解决方法：编辑linux服务器中的 &#x2F;etc&#x2F;sysctl.conf；将以下一行添加到文件末尾，可以将限制增加到最大值 fs.inotify.max_user_watches=524288 保存之后终端窗口 输入sysctl -p可解决。远程调试首先前提Linux服务器已经安装了GDB，否则会提示出错。在ceph工程目录下添加launch.json文件。在最左上栏运行(R) -&gt; 添加配置 ，注意一定要在ceph当前工程目录。修改配置launch.json中的program、args选项。 12345678910111213141516171819202122232425262728launch.json&#123; // 使用 IntelliSense 了解相关属性。 // 悬停以查看现有属性的描述。 // 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387 &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ &#123; &quot;name&quot;: &quot;ceph-debug&quot;, &quot;type&quot;: &quot;cppdbg&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;program&quot;: &quot;$&#123;workspaceFolder&#125;/build/bin/unittest_crush&quot;, &quot;args&quot;: [&quot;-d&quot;, &quot;--cluster&quot;, &quot;ceph&quot;,&quot;--id&quot;, &quot;0&quot;, &quot;--setuser&quot;, &quot;root&quot;, &quot;--setgroup&quot;, &quot;root&quot;], &quot;stopAtEntry&quot;: false, &quot;cwd&quot;: &quot;$&#123;workspaceFolder&#125;&quot;, &quot;environment&quot;: [], &quot;externalConsole&quot;: false, &quot;MIMode&quot;: &quot;gdb&quot;, &quot;setupCommands&quot;: [ &#123; &quot;description&quot;: &quot;Enable pretty-printing for gdb&quot;, &quot;text&quot;: &quot;-enable-pretty-printing&quot;, &quot;ignoreFailures&quot;: true &#125; ] &#125; ]&#125; 按照下图点击就可以开始调试之路 报错记录报错1 1234RPC failed; result=35, HTTP code = 0 fatal: The remote end hung up unexpectedly无法克隆 &#x27;https://github.com/xxxx/xxxxxxxx.git&#x27; 到子模组路径 &#x27;xxxxxxxxx&#x27;解决： 通过设置Git的http缓存大小，解决了这个问题，在当前工程目录下运行如下命令： git config --global http.postBuffer 20M (如果20M不行就50M) 报错2 12编译出现了一个问题，卡在5%Built target rocksdb_ext这里 原因：国外网络太慢，下载boost_1_72_0.tar.bz2太慢了，换网络或者在先用本地下载再传到服务器上（ceph/build/boost/src目录下） 报错3 12No Package found for python-scipyvim ceph.spec.in 报错4 1234&quot;Error: Package: golang-github-prometheus-2.26.1-2.el7.x86_64 (epel) Requires: /usr/bin/systemd-sysusers&quot;, 去掉该需求vim ~/ceph-14.2.16/ceph.spec.in# 内容#BuildRequires: golang-github-prometheus","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph_librados_api使用","slug":"Ceph_librados_api使用","date":"2021-06-18T06:28:31.000Z","updated":"2024-07-27T14:37:33.694Z","comments":true,"path":"Ceph_librados_api使用/","permalink":"https://watsonlu6.github.io/Ceph_librados_api%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Librados API概述Ceph存储集群提供基本的存储服务，Ceph以独特的方式将对象、块和文件存储集成到一个存储系统中。基于RADOS，可以不限于RESTful或POSIX接口，使用librados API能够创建自定义的Ceph存储集群接口（除了块存储、对象存储和文件系统存储外）。librados API能够与Ceph存储集群中的两种类型的守护进程进行交互： Ceph Mon守护进程，维护集群映射的主副本 Ceph OSD守护进程，它将数据作为对象存储在存储节点上要使用 API，您需要一个正在运行的 Ceph 存储集群。（本教程教程使用ceph编译的vstart启动的开发编程环境）编译模拟启动环境1234make vstart #模拟启动MDS=0 RGW=1 ../src/vstart.sh -d -l -n --bluestore #模拟集群所有的指令都在 build/bin 目录bin/ceph -s #查看 ceph 集群状态../src/stop.sh #停止模拟集群 第 1 步：获取libradosCeph客户端应用必须绑定librados才能连接Ceph存储集群。在写使用librados的ceph客户端应用前，要安装librados及其依赖包。librados API本身是用C++实现，也有C、Python、Java和PHP的API。（本教程仅限于librados C&#x2F;C++API）获取C&#x2F;C++的librados 要在 Debian&#x2F;Ubuntu 发行版上安装C&#x2F;C++ 的librados开发支持文件，执行以下命令： sudo apt-get install librados-dev 要在 RHEL&#x2F;CentOS 发行版上安装C&#x2F;C++ 的librados开发支持文件，执行以下命令： sudo yum install librados2-devel 安装librados 后，可以在&#x2F;usr&#x2F;include&#x2F;rados 下找到 C&#x2F;C++所需的头文件 ls /usr/include/rados 第 2 步：配置集群句柄一个Ceph客户端，通过librados直接与OSD交互，来存储和取出数据。为了与OSD交互，客户端应用必须直接调用libradosAPI连接一个Ceph Monitor。一旦连接好以后，librados会从Monitor处取回一个Cluster map。当客户端的应用想读或者取数据的时候，它会创建一个I&#x2F;O上下文并且与一个pool绑定。通过这个I&#x2F;O上下文，客户端将Object的名字提供给librados，然后librados会根据Object的名字和Cluster map计算出相应的PG和OSD的位置。然后客户端就可以读或者写数据。客户端的应用无需知道这个集群的拓扑结构。Ceph存储集群手柄封装客户端配置，包括： 基于用户ID的rados_create() 或者基于用户名的rados_create2()(首选) cephx认证密钥 Mon ID和IP地址 日志记录级别 调试级别 因此，Ceph客户端应用程序使用Ceph群集的步骤： 创建一个集群句柄，客户端应用将使用该句柄连接到存储集群中； 使用该手柄进行连接。要连接到集群的客户端应用必须提供Mon地址，用户名和认证密钥（默认启用cephx）。提示：与不同的 Ceph 存储集群或与具有不同用户的同一个集群通信需要不同的集群句柄。RADOS 提供了多种设置所需值的方法。对于Mon和加密密钥设置，处理它们的一种简单方法是确保 Ceph 配置文件包含密钥环文件的密钥环路径和至少一个Mon地址（例如mon host）。例如:123[global]mon host = 192.168.1.1keyring = /etc/ceph/ceph.client.admin.keyring 创建句柄后，读取 Ceph 配置文件来配置句柄。可以将参数传递给客户端应用程序并使用解析命令行参数的函数（例如rados_conf_parse_argv()）或解析 Ceph 环境变量（例如rados_conf_parse_env()）来解析它们。 连接后，客户端应用程序可以调用仅使用集群句柄影响整个集群的函数。例如，一旦有了集群句柄，就可以： • 获取集群统计信息 • 使用池操作（存在、创建、列出、删除） • 获取和设置配置 Ceph 的强大功能之一是能够绑定到不同的池。每个池可能有不同数量的归置组、对象副本和复制策略。例如，可以将池设置为使用 SSD 存储常用对象的“热”池或使用纠删码的“冷”池。各种语言的librados 绑定的主要区别在于 C 与C++、Java 和 Python 的面向对象绑定之间。面向对象的绑定使用对象来表示集群句柄、IO 上下文、迭代器、异常等。 C调用librados 示例对于 C，使用管理员用户创建一个简单的集群句柄，配置它并连接到集群如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;errno.h&gt;#include &lt;rados/librados.h&gt;int main(int argc,const char* argv[])&#123; rados_t cluster; char cluster_name[] = &quot;ceph&quot;; char user_name[] = &quot;client.admin&quot;; char conf_flie[] = &quot;/home/watson/ceph/build/ceph.conf&quot;; uint64_t flags; int err; err = rados_create2(&amp;cluster,cluster_name,user_name,flags); if(err &lt; 0) &#123; fprintf(stderr,&quot;%s: Couldn&#x27;t create the cluster handle!%s\\n&quot;,argv[0],strerror(-err)); exit(0); &#125;else&#123; printf(&quot;Create a cluster handle!!!\\n&quot;); &#125; err = rados_conf_read_file(cluster,conf_flie); if(err &lt; 0) &#123; fprintf(stderr,&quot;%s: cannot read config file: %s\\n&quot;,argv[0],strerror(-err)); exit(0); &#125;else&#123; printf(&quot;Read the config flie\\n&quot;); &#125; err = rados_conf_parse_argv(cluster,argc,argv); if(err &lt; 0) &#123; fprintf(stderr,&quot;%s: cannot parse command line arguments: %s\\n&quot;,argv[0],strerror(-err)); exit(0); &#125;else&#123; printf(&quot;Read the command line arguments\\n&quot;); &#125; err = rados_connect(cluster); if(err &lt; 0) &#123; fprintf(stderr,&quot;%s: cannot connect to cluster: %s\\n&quot;,argv[0],strerror(-err)); exit(0); &#125;else&#123; printf(&quot;Connected to the cluster\\n&quot;); &#125; return 0;&#125; 使用-lrados编译客户端应用代码并链接到librados，如下： 1gcc ceph-client.c -lrados -o ceph-client ceph源码开发vstart环境下的编译，如下： 1gcc -g rados_write.c -lrados -L/home/watson/ceph/build/lib -o rados_write -Wl,-rpath,/home/watson/ceph/build/lib C++调用librados示例Ceph项目在ceph&#x2F;examples&#x2F;librados目录中提供了一个 C++ 示例。对于 C++，使用管理员用户的简单集群句柄需要初始化librados::Rados集群句柄对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;rados/librados.hpp&gt;/* *通过librados::Rados句柄处理整个RADOS系统层面以及pool层面的管理。*/int main(int argc,const char* argv[])&#123; int ret = 0; librados::Rados cluster; //定义一个操控集群的句柄对象 char cluster_name[] = &quot;ceph&quot;; //集群名字 char user_name[] = &quot;client.admin&quot;; //集群用户名 char conf_flie[] = &quot;/home/watson/ceph/build/ceph.conf&quot;; //集群配置文件 uint64_t flags; ret = cluster.init2(user_name,cluster_name,flags); //初始化句柄对象 if(ret &lt; 0) &#123; std::cerr&lt;&lt;&quot;Couldn&#x27;t initialize the cluster handle! error: &quot;&lt;&lt;ret&lt;&lt;std::endl; return 1; &#125;else&#123; std::cout&lt;&lt;&quot;Create a cluster handle.&quot;&lt;&lt;std::endl; &#125; ret = cluster.conf_read_file(conf_flie); //读配置文件获取Mon的信息 if(ret &lt; 0 ) &#123; std::cerr&lt;&lt;&quot;Couldn&#x27;t read the ceph configuration file! error&quot;&lt;&lt;ret&lt;&lt;std::endl; return 1; &#125;else&#123; std::cout&lt;&lt;&quot;Read the ceph configuration file.&quot;&lt;&lt;std::endl; &#125; ret = cluster.conf_parse_argv(argc,argv); //解析命令行输入的参数 if(ret &lt; 0) &#123; std::cerr&lt;&lt;&quot;Couldn&#x27;t parsed command line options!error&quot;&lt;&lt;ret&lt;&lt;std::endl; return 1; &#125;else&#123; std::cout&lt;&lt;&quot;Parsed command line options.&quot;&lt;&lt;std::endl; &#125; ret = cluster.connect(); //连接集群 if(ret &lt; 0 ) &#123; std::cerr&lt;&lt;&quot;Couldn&#x27;t connect to cluster! error&quot;&lt;&lt;ret&lt;&lt;std::endl; return 1; &#125;else&#123; std::cout&lt;&lt;&quot;Connected to the cluster.&quot;&lt;&lt;std::endl; &#125; cluster.pool_create(&quot;testpool&quot;); //创建存储池 std::list&lt;std::string&gt; poolList; cluster.pool_list(poolList); //获取存储池列表 for(auto iter : poolList)&#123; std::cout&lt;&lt;iter&lt;&lt;std::endl; &#125; return 0;&#125; 编译源码，然后，使用-lrados链接librados，如下： 12g++ -g -c ceph-client.cc -o ceph-client.o g++ -g ceph-client.o -lrados -o ceph-client ceph源码开发vstart环境下的编译，如下： 1g++ -g librados_rados.cpp -lrados -L/home/watson/ceph/build/lib -o librados_rados -Wl,-rpath,/home/watson/ceph/build/lib 第 3 步：创建 I&#x2F;O 上下文一旦客户端应用程序拥有集群句柄并连接到 Ceph 存储集群，就可以创建 I&#x2F;O 上下文并开始读取和写入数据。I&#x2F;O 上下文将连接绑定到特定池。用户必须具有适当的CAPS权限才能访问指定的池。例如，具有读取权限但没有写入权限的用户将只能读取数据。I&#x2F;O 上下文功能包括： 写入&#x2F;读取数据和扩展属性 列出并迭代对象和扩展属性 快照池、列表快照等RADOS 使客户端应用程序能够进行同步和异步交互。一旦应用程序具有 I&#x2F;O 上下文，读&#x2F;写操作只需要知道对象&#x2F;xattr 名称。librados中封装的 CRUSH 算法使用Cluster map来选择合适的 OSD。OSD 守护进程自动处理副本。librados库将对象映射到归置组。以下示例使用默认数据池。但是，也可以使用 API 列出池、确保它们存在或创建和删除池。对于写操作，示例说明了如何使用同步模式。对于读取操作，示例说明了如何使用异步模式。(提示：使用此 API 删除池时要小心。如果删除池，则该池和池中的所有数据都将丢失。)C创建Ceph IO上下文示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;errno.h&gt;#include &lt;stdlib.h&gt;#include &lt;rados/librados.h&gt;int main(int argc,const char* argv[])&#123; rados_t cluster; //集群句柄 rados_ioctx_t io; //io上下文 char cluster_name[] = &quot;ceph&quot;; char user_name[] = &quot;client.admin&quot;; char conf_flie[] = &quot;/home/watson/ceph/build/ceph.conf&quot;; char poolname[] = &quot;testpool&quot;; uint64_t flags; int err; /* 为了使示例代码更可观性，不对返回值判错，实际应用中需要进行判错，请养成良好习惯！ */ err = rados_create2(&amp;cluster,cluster_name,user_name,flags); err = rados_conf_read_file(cluster,conf_flie); err = rados_conf_parse_argv(cluster,argc,argv); err = rados_connect(cluster); if(err &lt; 0) //检查是否连接到集群上 &#123; fprintf(stderr,&quot;%s: Cannot connect to cluster: %s\\n&quot;,argv[0],strerror(-err)); exit(0); &#125;else&#123; printf(&quot;Connected to the cluster......\\n&quot;); &#125; //err = rados_pool_delete(cluster,poolname); int poolID = rados_pool_lookup(cluster,poolname); //通过poolname获取pool的ID，若池不存在返回-ENOENT if(poolID == -ENOENT) &#123; printf(&quot;this pool does not exist,and create the pool...... \\n&quot;); rados_pool_create(cluster,poolname); &#125; err = rados_ioctx_create(cluster,poolname,&amp;io); //初始化io上下文 char obj_name[] = &quot;obj&quot;; char obj_content[] = &quot;Hello librados&quot;; err = rados_write(io,obj_name,obj_content,strlen(obj_content),0); //往集群写入对象 if(err == 0) &#123; printf(&quot;rados_write success......\\n&quot;); &#125; char xattr[] = &quot;en_US&quot;; err = rados_setxattr(io,obj_name,&quot;lang&quot;,xattr,5); //给对象设置属性 if(err == 0) &#123; printf(&quot;Set object xattr success......\\n&quot;); &#125; rados_completion_t comp; err = rados_aio_create_completion(NULL,NULL,NULL,&amp;comp); //异步读 char read_ret[1024]; err = rados_aio_read(io,obj_name,comp,read_ret,sizeof(read_ret),0); rados_aio_wait_for_complete(comp); if( err == 0) &#123; printf(&quot;%s\\&#x27;s content is %s\\n&quot;,obj_name,read_ret); &#125;else&#123; printf(&quot;read_aio_read: err\\n&quot;); &#125; rados_aio_release(comp); err = rados_read(io,obj_name,read_ret,sizeof(read_ret),0); //同步读 if( err &gt; 0) &#123; printf(&quot;%s\\&#x27;s content is %s\\n&quot;,obj_name,read_ret); &#125;else&#123; printf(&quot;read_read: err\\n&quot;); &#125; char xattr_ret[100]; err = rados_getxattr(io,obj_name,&quot;lang&quot;,xattr_ret,6); //获取对象属性 if( err &gt; 0) &#123; printf(&quot;Read %s\\&#x27;s xattr \\&quot;lang\\&quot; is %s\\n&quot;,obj_name,xattr_ret); &#125;else&#123; printf(&quot;rados_getxattr: err\\n&quot;); &#125; err = rados_rmxattr(io,obj_name,&quot;lang&quot;); //删除对象属性 err = rados_remove(io,obj_name); //删除对象 rados_ioctx_destroy(io); //释放io上下文 rados_shutdown(cluster); //关闭集群句柄 return 0;&#125; C++创建Ceph IO上下文示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;rados/librados.hpp&gt;int main(int argc,const char* argv[])&#123; librados::Rados cluster; librados::IoCtx io_ctx; char cluster_name[] = &quot;ceph&quot;; char user_name[] = &quot;client.admin&quot;; char conf_flie[] = &quot;/home/watson/ceph/build/ceph.conf&quot;; char poolname[] = &quot;testpool&quot;; uint64_t flags; int ret; /* 为了使示例代码更可观性，不对返回值判错，实际应用中需要进行判错，请养成良好习惯！ */ ret = cluster.init2(user_name,cluster_name,flags); ret = cluster.conf_read_file(conf_flie); ret = cluster.conf_parse_argv(argc,argv); ret = cluster.connect(); if(ret &lt; 0 ) //测试集群连接情况 &#123; std::cerr&lt;&lt;&quot;Couldn&#x27;t connect to cluster! error&quot;&lt;&lt;ret&lt;&lt;std::endl; return 1; &#125;else&#123; std::cout&lt;&lt;&quot;Connected to the cluster.&quot;&lt;&lt;std::endl; &#125; int poolID = cluster.pool_lookup(poolname); //通过pool名检测是否存在pool if(poolID == -ENOENT) &#123; printf(&quot;this pool does not exist,and create the pool...... \\n&quot;); cluster.pool_create(poolname); &#125;else&#123; std::cout&lt;&lt;&quot;pool &quot;&lt;&lt;poolID&lt;&lt;&quot; is using......&quot;&lt;&lt;std::endl; &#125; ret = cluster.ioctx_create(poolname,io_ctx); //初始化io_ctx char obj_name[] = &quot;obj&quot;; librados::bufferlist bl; bl.append(&quot;Hello Librados!&quot;); ret = io_ctx.write_full(obj_name,bl); //往集群写入数据 if(ret &lt; 0) &#123; std::cerr&lt;&lt;&quot;Couldn&#x27;t write object! error&quot;&lt;&lt;ret&lt;&lt;std::endl; exit(EXIT_FAILURE); &#125;else&#123; std::cout&lt;&lt;&quot;Write success......&quot;&lt;&lt;std::endl; &#125; librados::bufferlist lang_bl; lang_bl.append(&quot;en_US&quot;); ret = io_ctx.setxattr(obj_name,&quot;lang&quot;,lang_bl); //给对象设置属性 if(ret &lt; 0) &#123; std::cerr&lt;&lt;&quot;Couldn&#x27;t write object xattr! error&quot;&lt;&lt;ret&lt;&lt;std::endl; exit(EXIT_FAILURE); &#125;else&#123; std::cout&lt;&lt;&quot;Set xattr success......&quot;&lt;&lt;std::endl; &#125; librados::bufferlist read_bl; //异步读 int read_len = 1024; librados::AioCompletion *read_completion = librados::Rados::aio_create_completion(); ret = io_ctx.aio_read(obj_name,read_completion,&amp;read_bl,read_len,0); if(ret &lt; 0) &#123; std::cerr&lt;&lt;&quot;Couldn&#x27;t read object! error&quot;&lt;&lt;ret&lt;&lt;std::endl; exit(EXIT_FAILURE); &#125; read_completion-&gt;wait_for_complete(); //等待异步完成 ret = read_completion-&gt;get_return_value(); //获取返回值 if(ret &lt; 0) &#123; std::cerr&lt;&lt;&quot;Couldn&#x27;t read object! error&quot;&lt;&lt;ret&lt;&lt;std::endl; exit(EXIT_FAILURE); &#125;else&#123; std::cout&lt;&lt;read_bl.c_str()&lt;&lt;std::endl; &#125; librados::bufferlist lang_res; ret = io_ctx.getxattr(obj_name,&quot;lang&quot;,lang_res); //获取属性 if(ret &lt; 0) &#123; std::cerr&lt;&lt;&quot;Couldn&#x27;t read object xattr! error&quot;&lt;&lt;ret&lt;&lt;std::endl; exit(EXIT_FAILURE); &#125;else&#123; std::cout&lt;&lt;lang_res.c_str()&lt;&lt;std::endl; &#125; ret = io_ctx.rmxattr(obj_name,&quot;lang&quot;); //删除对象属性 ret = io_ctx.remove(obj_name); //删除对象 io_ctx.close(); //关闭io cluster.shutdown(); //关闭集群句柄 return 0;&#125; 第 4 步：结束会话一旦客户端应用程序完成了 I&#x2F;O 上下文和集群句柄，应用程序应该关闭连接并关闭句柄。对于异步 I&#x2F;O，应用程序还应确保挂起的异步操作已完成。C结束会话示例 12rados_ioctx_destroy(io); rados_shutdown(cluster); C++结束会话示例 12io_ctx.close(); cluster.shutdown(); 补充：查看pool下的object对象 –all 显示所有namespace的object 1rados ls -p pool --all LIBRADOS常用接口 集群配置：提供了获取和设置配置值的方法，读取Ceph配置文件，并解析参数。 Rados.conf_get(option) Rados.conf_set(option, val) Rados.conf_read_file(path) Rados.conf_parse_argv(args) Rados.version() 连接管理：连接到集群、检查集群、检索集群的统计数据，并从集群断开连接。也可以断言集群句柄处于一个特定的状态（例如，”配置”，”连接”等等）。 Rados.connect(timeout) Rados.shutdown() Rados.get_fsid() Rados.get_cluster_stats() 池操作：列出可用的池，创建一个池，检查一个池是否存在，并删除一个池。 Rados.list_pools() Rados.create_pool(pool_name, crush_rule, auid) Rados.pool_exists(pool_name) Rados.delete_pool(pool_name) CLI 命令：Ceph CLI命令在内部使用以下librados Python绑定方法。 Rados.mon_command(cmd, inbuf, timeout, target) Rados.osd_command(osdid, cmd, inbuf, timeout) Rados.mgr_command(cmd, inbuf, timeout, target) Rados.pg_command(pgid, cmd, inbuf, timeout) I&#x2F;O上下文：为了将数据写入Ceph对象存储和从Ceph对象存储读取数据，必须创建一个输入&#x2F;输出上下文（ioctx）。Rados类提供了open_ioctx()和open_ioctx2()方法。其余的操作涉及调用Ioctx和其他类的方法。 Rados.open_ioctx(ioctx_name) Ioctx.require_ioctx_open() Ioctx.get_stats() Ioctx.get_last_version() Ioctx.close() 对象操作：同步或异步地读和写对象。一个对象有一个名称（或键）和数据。 Ioctx.aio_write(object_name, to_write, offset, oncomplete, onsafe) Ioctx.aio_write_full(object_name, to_write, oncomplete, onsafe) Ioctx.aio_append(object_name, to_append, oncomplete, onsafe) Ioctx.write(key, data, offset) Ioctx.write_full(key, data) Ioctx.aio_flush() Ioctx.set_locator_key(loc_key) Ioctx.aio_read(object_name, length, offset, oncomplete) Ioctx.read(key, length, offset) Ioctx.stat(key) Ioctx.trunc(key, size) Ioctx.remove_object(key) 对象扩展属性：在一个对象上设置扩展属性(XATTRs)。 Ioctx.set_xattr(key, xattr_name, xattr_value) Ioctx.get_xattrs(oid) XattrIterator.next() Ioctx.get_xattr(key, xattr_name) Ioctx.rm_xattr(key, xattr_name) 对象接口：从一个池中检索一个对象的列表，并对它们进行迭代。提供的对象接口使每个对象看起来像一个文件，可以对对象进行同步操作。对于异步操作，应该使用I&#x2F;O上下文的方法。 Ioctx.list_objects() ObjectIterator.next() Object.read(length&#x3D;1024 * 1024) Object.write(string_to_write) Object.get_xattrs() Object.get_xattr(xattr_name) Object.set_xattr(xattr_name, xattr_value) Object.rm_xattr(xattr_name) Object.stat() Object.remove()","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph_librados介绍","slug":"Ceph_librados介绍","date":"2021-06-05T06:15:14.000Z","updated":"2024-07-27T14:37:27.947Z","comments":true,"path":"Ceph_librados介绍/","permalink":"https://watsonlu6.github.io/Ceph_librados%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"Ceph Librados介绍Ceph Librados 概述一个Ceph客户端，通过librados直接与OSD交互，来存储和取出数据。为了与OSD交互，客户端应用必须直接调用librados，连接一个Ceph Monitor。一旦连接好以后，librados会从Monitor处取回一个Cluster map。当客户端的应用想读或者取数据的时候，它要创建一个I&#x2F;O上下文并且与一个pool绑定。通过这个I&#x2F;O上下文，客户端将Object的名字提供给librados，然后librados会根据Object的名字和Cluster map计算出相应的PG和OSD的位置。然后客户端就可以读或者写数据。客户端的应用无需知道这个集群的拓扑结构。 Ceph客户端主要是实现了接口，对外提供了访问的功能。上层可以通过接口访问Ceph存储。Ceph的客户端通过一套名为librados的接口进行集群的访问，这里的访问包括对集群的整体访问和对象的访问两类接口。这套接口（API）包括C、C++和Python常见语言的实现，接口通过网络实现对Ceph集群的访问。在用户层面，可以在自己的程序中调用该接口，从而集成Ceph集群的存储功能，或者在监控程序中实现对Ceph集群状态的监控。所谓集群的整体访问包括连接集群、创建存储池、删除存储池和获取集群状态等等。所谓对象访问是之对存储池中对象的访问，包括创建删除对象、向对象写数据或者追加数据和读对象数据等接口。 客户端基本架构概述librados客户端基本架构如下图所示，主要包括4层，分别是API层、IO处理层、对象处理层和消息收发层。 API层是一个抽象层，为上层提供统一的接口。API层提供的原生接口包括C和C++两种语言的实现外，还有Python的实现。 IO处理层用于实现IO的简单封装，其通过一个名为ObjectOperation类实现，该类主要包括的是读写操作的数据信息。之后在IO处理层在IoCtxImpl::operate函数中将ObjectOperation转换为Objecter::Op类的对象，并将该对象提交到对象处理层进行进一步的处理。 对象处理层包括了Ceph对象处理所需要的信息，包括通信管道、OSDMap和MonMap等内容。因此，在这里，根据对象的信息可以计算出对象存储的具体位置，最终找到客户端与OSD的连接信息（Session）。 消息收发层的接口会被对象处理层调用，此时消息会传递到本层，并且通过本层的线程池发送到具体的OSD。这里需要注意的是，消息收发层与服务端的消息收发公用Messager的代码。 核心流程图先根据配置文件调用librados创建Rados，接下来为这个Rados创建一个RadosClient，RadosClient包含3个主要模块(finisher、Messenger、Objecter)。再根据pool创建对应的ioctx，在ioctx中能够找到RadosClient。再调用OSDC生成对应的OSD请求，与OSD进行通信响应请求。这从大体上叙述了librados与osdc在整个Ceph中的作用。 具体细节可以按照该流程读对应源代码理解。在这个流程中需要注意的是_op_submit函数会调用_calc_target和_get_session两个函数，两个函数的作用分别是获取目的OSD和对应的Session（连接），这个是后面发送数据的基础。 Librados与OSDC的关系Librados与OSDC位于ceph客户端中比较底层的位置。 Librados模块是RADOS对象存储系统访问的接口，它提供了pool的创建、删除、对象的创建、删除、读写等基本操作接口。类RadosClient是librados模块的核心管理类，处理整个RADOS系统层面以及pool层面的管理。类ioctxlmpl实现单个pool层的对象读写等操作。 OSDC模块实现了请求的封装和通过网络模块发送请求的逻辑，其核心类Object完成对象的地址计算、消息的发送和处理超时等工作。librados模块包含两个部分，分别是RadosClient 模块和IoctxImpl。RadosClient处于最上层，是librados的核心管理类，管理着整个RADOS系统层面以及pool层面的管理。 Librados模块类RadosClientRadosClient处于最上层，是librados的核心管理类，管理着整个RADOS系统层面以及pool层面的管理。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103class librados::RadosClient : public Dispatcher //继承Dispatcher(消息分发类)&#123; //unique_ptr智能指针 std::unique_ptr&lt;CephContext,std::function&lt;void(CephContext*)&gt; &gt; cct_deleter;public: using Dispatcher::cct; const ConfigProxy&amp; conf; //配置文件private: enum &#123; DISCONNECTED, CONNECTING, CONNECTED, &#125; state; //Monitor的网络连接状态 MonClient monclient; //Monitor客户端 MgrClient mgrclient; //MGR客户端 Messenger *messenger; //网络消息接口 uint64_t instance_id; //rados客户端实例的ID //相关消息分发 Dispatcher类的函数重写 bool _dispatch(Message *m); bool ms_dispatch(Message *m) override; bool ms_get_authorizer(int dest_type, AuthAuthorizer **authorizer) override; void ms_handle_connect(Connection *con) override; bool ms_handle_reset(Connection *con) override; void ms_handle_remote_reset(Connection *con) override; bool ms_handle_refused(Connection *con) override; Objecter *objecter; //OSDC模块中用于发送封装好的OP消息 Mutex lock; Cond cond; SafeTimer timer; //定时器 int refcnt; //引用计算 version_t log_last_version; rados_log_callback_t log_cb; rados_log_callback2_t log_cb2; void *log_cb_arg; string log_watch; bool service_daemon = false; string daemon_name, service_name; map&lt;string,string&gt; daemon_metadata; int wait_for_osdmap(); Finisher finisher; //用于执行回调函数的finisher类 explicit RadosClient(CephContext *cct_); ~RadosClient() override; int ping_monitor(string mon_id, string *result); int connect(); //RadosClient的初始化函数、 连接 void shutdown(); int watch_flush(); int async_watch_flush(AioCompletionImpl *c); uint64_t get_instance_id(); int get_min_compatible_osd(int8_t* require_osd_release); int get_min_compatible_client(int8_t* min_compat_client,int8_t* require_min_compat_client); int wait_for_latest_osdmap(); //创建一个pool相关的上下文信息IoCtxImpl对象（根据pool名字或Id创建ioctx） int create_ioctx(const char *name, IoCtxImpl **io); int create_ioctx(int64_t, IoCtxImpl **io); int get_fsid(std::string *s); //用于查找pool int64_t lookup_pool(const char *name); bool pool_requires_alignment(int64_t pool_id); int pool_requires_alignment2(int64_t pool_id, bool *requires); uint64_t pool_required_alignment(int64_t pool_id); int pool_required_alignment2(int64_t pool_id, uint64_t *alignment); int pool_get_name(uint64_t pool_id, std::string *name, bool wait_latest_map = false); //用于列出所有的pool int pool_list(std::list&lt;std::pair&lt;int64_t, string&gt; &gt;&amp; ls); //用于获取pool的统计信息 int get_pool_stats(std::list&lt;string&gt;&amp; ls, map&lt;string,::pool_stat_t&gt; *result,bool *per_pool); //用于获取系统的统计信息 int get_fs_stats(ceph_statfs&amp; result); bool get_pool_is_selfmanaged_snaps_mode(const std::string&amp; pool); //pool的同步创建 int pool_create(string&amp; name, int16_t crush_rule=-1); //pool的异步创建 int pool_create_async(string&amp; name, PoolAsyncCompletionImpl *c,int16_t crush_rule=-1); int pool_get_base_tier(int64_t pool_id, int64_t* base_tier); //同步删除pool int pool_delete(const char *name); //异步删除pool int pool_delete_async(const char *name, PoolAsyncCompletionImpl *c); int blacklist_add(const string&amp; client_address, uint32_t expire_seconds); //处理Mon相关命令,调用monclient.start_mon_command 把命令发送给Mon处理 int mon_command(const vector&lt;string&gt;&amp; cmd, const bufferlist &amp;inbl,bufferlist *outbl, string *outs); void mon_command_async(………); int mon_command(int rank,const vector&lt;string&gt;&amp; cmd, const bufferlist &amp;inbl,bufferlist *outbl, string *outs); int mon_command(string name,const vector&lt;string&gt;&amp; cmd, const bufferlist &amp;inbl,bufferlist *outbl, string *outs); int mgr_command(const vector&lt;string&gt;&amp; cmd, const bufferlist &amp;inbl, bufferlist *outbl, string *outs); //处理OSD相关命令 int osd_command(int osd, vector&lt;string&gt;&amp; cmd, const bufferlist&amp; inbl,bufferlist *poutbl, string *prs); //处理PG相关命令 int pg_command(pg_t pgid, vector&lt;string&gt;&amp; cmd, const bufferlist&amp; inbl,bufferlist *poutbl, string *prs); void handle_log(MLog *m); int monitor_log(const string&amp; level, rados_log_callback_t cb,rados_log_callback2_t cb2, void *arg); void get(); bool put(); void blacklist_self(bool set); std::string get_addrs() const; int service_daemon_register( const std::string&amp; service, ///&lt; service name (e.g., &#x27;rgw&#x27;) const std::string&amp; name, ///&lt; daemon name (e.g., &#x27;gwfoo&#x27;) const std::map&lt;std::string,std::string&gt;&amp; metadata); ///&lt; static metadata about daemon int service_daemon_update_status(std::map&lt;std::string,std::string&gt;&amp;&amp; status); mon_feature_t get_required_monitor_features() const; int get_inconsistent_pgs(int64_t pool_id, std::vector&lt;std::string&gt;* pgs);&#125;; 类IoctxImpl类IoctxImpl是对于其中的某一个pool进行管理，如对 对象的读写等操作的控制。该类是pool的上下文信息，一个pool对应一个IoctxImpl对象。librados中所有关于io操作的API都设计在librados::IoCtx中，接口的真正实现在ioCtxImpl中，它的处理过程如下： 把请求封装成ObjectOperation类(osdc类中) 把相关的pool信息添加到里面，封装成Object::Op对象 调用响应的函数object-&gt;op_submit发送给相应的OSD 操作完成后，调用相应的回调函数。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138struct librados::IoCtxImpl &#123; std::atomic&lt;uint64_t&gt; ref_cnt = &#123; 0 &#125;; RadosClient *client; int64_t poolid; snapid_t snap_seq; ::SnapContext snapc; uint64_t assert_ver; version_t last_objver; uint32_t notify_timeout; object_locator_t oloc; Mutex aio_write_list_lock; ceph_tid_t aio_write_seq; Cond aio_write_cond; xlist&lt;AioCompletionImpl*&gt; aio_write_list; map&lt;ceph_tid_t, std::list&lt;AioCompletionImpl*&gt; &gt; aio_write_waiters; Objecter *objecter; IoCtxImpl(); IoCtxImpl(RadosClient *c, Objecter *objecter,int64_t poolid, snapid_t s); void dup(const IoCtxImpl&amp; rhs); void set_snap_read(snapid_t s); int set_snap_write_context(snapid_t seq, vector&lt;snapid_t&gt;&amp; snaps); void get(); void put(); void queue_aio_write(struct AioCompletionImpl *c); void complete_aio_write(struct AioCompletionImpl *c); void flush_aio_writes_async(AioCompletionImpl *c); void flush_aio_writes(); int64_t get_id(); string get_cached_pool_name(); int get_object_hash_position(const std::string&amp; oid, uint32_t *hash_position); int get_object_pg_hash_position(const std::string&amp; oid, uint32_t *pg_hash_position); ::ObjectOperation *prepare_assert_ops(::ObjectOperation *op); // snaps int snap_list(vector&lt;uint64_t&gt; *snaps); int snap_lookup(const char *name, uint64_t *snapid); int snap_get_name(uint64_t snapid, std::string *s); int snap_get_stamp(uint64_t snapid, time_t *t); int snap_create(const char* snapname); int selfmanaged_snap_create(uint64_t *snapid); void aio_selfmanaged_snap_create(uint64_t *snapid, AioCompletionImpl *c); int snap_remove(const char* snapname); int rollback(const object_t&amp; oid, const char *snapName); int selfmanaged_snap_remove(uint64_t snapid); void aio_selfmanaged_snap_remove(uint64_t snapid, AioCompletionImpl *c); int selfmanaged_snap_rollback_object(const object_t&amp; oid,::SnapContext&amp; snapc, uint64_t snapid); // io int nlist(Objecter::NListContext *context, int max_entries); uint32_t nlist_seek(Objecter::NListContext *context, uint32_t pos); uint32_t nlist_seek(Objecter::NListContext *context, const rados_object_list_cursor&amp; cursor); rados_object_list_cursor nlist_get_cursor(Objecter::NListContext *context); void object_list_slice(……); int create(const object_t&amp; oid, bool exclusive); int write(const object_t&amp; oid, bufferlist&amp; bl, size_t len, uint64_t off); int append(const object_t&amp; oid, bufferlist&amp; bl, size_t len); int write_full(const object_t&amp; oid, bufferlist&amp; bl); int writesame(const object_t&amp; oid, bufferlist&amp; bl,size_t write_len, uint64_t offset); int read(const object_t&amp; oid, bufferlist&amp; bl, size_t len, uint64_t off); int mapext(const object_t&amp; oid, uint64_t off, size_t len,std::map&lt;uint64_t,uint64_t&gt;&amp; m); int sparse_read(const object_t&amp; oid, std::map&lt;uint64_t,uint64_t&gt;&amp; m,bufferlist&amp; bl, size_t len, uint64_t off); int checksum(……); int remove(const object_t&amp; oid); int remove(const object_t&amp; oid, int flags); int stat(const object_t&amp; oid, uint64_t *psize, time_t *pmtime); int stat2(const object_t&amp; oid, uint64_t *psize, struct timespec *pts); int trunc(const object_t&amp; oid, uint64_t size); int cmpext(const object_t&amp; oid, uint64_t off, bufferlist&amp; cmp_bl); int tmap_update(const object_t&amp; oid, bufferlist&amp; cmdbl); int exec(const object_t&amp; oid, const char *cls, const char *method, bufferlist&amp; inbl, bufferlist&amp; outbl); int getxattr(const object_t&amp; oid, const char *name, bufferlist&amp; bl); int setxattr(const object_t&amp; oid, const char *name, bufferlist&amp; bl); int getxattrs(const object_t&amp; oid, map&lt;string, bufferlist&gt;&amp; attrset); int rmxattr(const object_t&amp; oid, const char *name); int operate(const object_t&amp; oid, ::ObjectOperation *o, ceph::real_time *pmtime, int flags=0); int operate_read(const object_t&amp; oid, ::ObjectOperation *o, bufferlist *pbl, int flags=0); int aio_operate(……); int aio_operate_read(…………); struct C_aio_stat_Ack : public Context &#123;…… &#125;; struct C_aio_stat2_Ack : public Context &#123;…… &#125;; struct C_aio_Complete : public Context &#123;…… &#125;; int aio_read(……); int aio_read(………); int aio_sparse_read(………); int aio_cmpext(const object_t&amp; oid, AioCompletionImpl *c, uint64_t off,bufferlist&amp; cmp_bl); int aio_cmpext(………); int aio_write(………); int aio_append(const object_t &amp;oid, AioCompletionImpl *c,const bufferlist&amp; bl, size_t len); int aio_write_full(const object_t &amp;oid, AioCompletionImpl *c,const bufferlist&amp; bl); int aio_writesame(const object_t &amp;oid, AioCompletionImpl *c,const bufferlist&amp; bl, size_t write_len, uint64_t off); int aio_remove(const object_t &amp;oid, AioCompletionImpl *c, int flags=0); int aio_exec(………); int aio_exec(………); int aio_stat(const object_t&amp; oid, AioCompletionImpl *c, uint64_t *psize, time_t *pmtime); int aio_stat2(const object_t&amp; oid, AioCompletionImpl *c, uint64_t *psize, struct timespec *pts); int aio_getxattr(const object_t&amp; oid, AioCompletionImpl *c,const char *name, bufferlist&amp; bl); int aio_setxattr(const object_t&amp; oid, AioCompletionImpl *c, const char *name, bufferlist&amp; bl); int aio_getxattrs(const object_t&amp; oid, AioCompletionImpl *c,map&lt;string, bufferlist&gt;&amp; attrset); int aio_rmxattr(const object_t&amp; oid, AioCompletionImpl *c,const char *name); int aio_cancel(AioCompletionImpl *c); int hit_set_list(uint32_t hash, AioCompletionImpl *c,std::list&lt; std::pair&lt;time_t, time_t&gt; &gt; *pls); int hit_set_get(uint32_t hash, AioCompletionImpl *c, time_t stamp,bufferlist *pbl); int get_inconsistent_objects(………); int get_inconsistent_snapsets(………); void set_sync_op_version(version_t ver); int watch(………); int watch(………); int aio_watch(……); int aio_watch(……); int watch_check(uint64_t cookie); int unwatch(uint64_t cookie); int aio_unwatch(uint64_t cookie, AioCompletionImpl *c); int notify(……); int notify_ack(const object_t&amp; oid, uint64_t notify_id, uint64_t cookie,bufferlist&amp; bl); int aio_notify(………); int set_alloc_hint(……); version_t last_version(); void set_assert_version(uint64_t ver); void set_notify_timeout(uint32_t timeout); int cache_pin(const object_t&amp; oid); int cache_unpin(const object_t&amp; oid); int application_enable(const std::string&amp; app_name, bool force); void application_enable_async(const std::string&amp; app_name, bool force,PoolAsyncCompletionImpl *c); int application_list(std::set&lt;std::string&gt; *app_names); int application_metadata_get(const std::string&amp; app_name,const std::string &amp;key,std::string* value); int application_metadata_set(const std::string&amp; app_name,const std::string &amp;key,const std::string&amp; value); int application_metadata_remove(const std::string&amp; app_name,const std::string &amp;key); int application_metadata_list(const std::string&amp; app_name, std::map&lt;std::string, std::string&gt; *values);&#125;; librados主要接口 集群句柄创建librados::Rados对象是用来操纵ceph集群的句柄，使用init来创建RadosClient，之后读取指定的ceph配置文件，获取monitor的ip和端口号。RadosClient里面有与monitor通信的MonClient和用于与OSD通信的Messenger。 集群连接初始化集群句柄之后，就可以使用这个句柄来连接集群了RadosClient::connect完成了连接操作： a. 调用monclient.build_inital_monmap，从配置文件种检查是否有初始化的monitor的地址信息 b. 创建网络通信模块messenger，并设置相关的Policy信息 c. 创建Objecter对象并初始化 d. 调用monclient.init()函数初始化monclient e. Timer定时器初始化，Finisher对象初始化 IO上下文环境初始化使用句柄创建好存储池后，还需要创建与存储池相关的IO上下文句柄rados.ioctx_create(pool_name, io_ctx) 对象读写创建对象并写入数据：io_ctx.create_full(object_name,bl)读取对象中的数据到bufferlist中，对象读取有同步读取和异步读取两种接口：io_ctx.read和io_ctx.aio_read a. 同步读取：io_ctx.read(object_name,read_bl,read_len,0) b. 异步读取：需要指定完成读取数据后的回调，用于检查读取是否完成 librados::AioCompletion *read_completion &#x3D; librados::Rados::aio_create_completion(); io_ctx.aio_read(object_name,read_completion,&amp;read_buff,read_len,0) read_completion-&gt;wait_for_complete() 同时还要获取返回值，得到读取对象的字节数 IO上下文关闭io_ctx.close() 集群句柄关闭rados.shutdown()上述功能通过Rados和IoCtx两个类实现，两个类的主要函数如下图所示（这里仅是示例，实际接口数量要多很多，具体参考源代码）。 Ceph官方的示例代码为了了解如何使用这些API，这里给出一些代码片段。具体完整的代码大家可以参考Ceph官方的示例代码。 12345678910111213141516171819202122librados::IoCtx io_ctx;const char *pool_name = &quot;test&quot;;cluster.ioctx_create(pool_name, io_ctx); /* 创建进行IO处理的上下文，其实就是用于访问Ceph的对象 *//* 同步写对象 */librados::bufferlist bl;bl.append(&quot;Hello World!&quot;); /* 对象的内容 */ret = io_ctx.write_full(&quot;itworld123&quot;, bl); /*写入对象itworld123*//* 向对象添加属性，这里的属性与文件系统中文件的扩展属性类似。 */librados::bufferlist attr_bl;attr_bl.append(&quot;en_US&quot;);io_ctx.setxattr(&quot;itworld123&quot;, &quot;test_attr&quot;, attr_bl);/* 异步读取对象内容 */librados::bufferlist read_buf;int read_len = 1024;librados::AioCompletion *read_completion = librados::Rados::aio_create_completion(); /* 创建一个异步完成类对象 */io_ctx.aio_read(&quot;itworld123&quot;, read_completion, &amp;read_buf, read_len, 0); /* 发送读请求 */read_completion-&gt;wait_for_complete(); /* 等待请求完成 */read_completion-&gt;get_return_value(); librados::bufferlist attr_res;io_ctx.getxattr(&quot;itworld123&quot;, &quot;test_attr&quot;, attr_res); /* 读取对象属性 */io_ctx.rmxattr(&quot;itworld123&quot;, &quot;test_attr&quot;); /* 删除对象的属性 */io_ctx.remove(&quot;itworld123&quot;); /* 删除对象 */","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph数据读写过程","slug":"Ceph数据读写过程","date":"2021-05-19T09:06:37.000Z","updated":"2024-07-27T14:31:10.525Z","comments":true,"path":"Ceph数据读写过程/","permalink":"https://watsonlu6.github.io/Ceph%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%86%99%E8%BF%87%E7%A8%8B/","excerpt":"","text":"Ceph数据映射过程在一个大规模分布式存储系统中，需要解决两个核心问题：“我应该把数据写到哪里？”和“我之前把数据存储在了哪里？”。这就引出了数据寻址的问题。Ceph 的寻址流程可以如下描述。 File：此处的File就是用户需要存储或访问的文件。对于一个基于Ceph开发的对象存储应用而言，这个File也就对应于应用中的“对象” ，也就是用户直接操作的“对象” Object： 在 RADOS（Reliable Autonomic Distributed Object Store）中，”对象” 是指系统中存储的基本单位。与 File 的不同之处在于，Object 的最大尺寸受到 RADOS 的限制，通常为 2MB 或 4MB。这一限制是为了优化底层存储的管理和组织。因此，当上层应用向 RADOS 存储一个较大的 File 时，需要将其拆分成多个统一大小的 Object（最后一个 Object 的大小可能不同）进行存储。 PG（Placement Group）： 顾名思义，PG 用于组织对象的存储并映射其位置。具体来说，一个 PG 负责管理多个对象，而每个对象只能映射到一个 PG 中，即 PG 和对象之间是“一对多”的映射关系。同时，一个 PG 会被映射到多个 OSD（Object Storage Device）上，通常 n 至少为 2，而在生产环境中，n 通常至少为 3。每个 OSD 上会承载大量的 PG，可能达到数百个。PG 的数量设置直接影响数据的分布均匀性，因此在实际配置中需要谨慎考虑。 OSD（Object Storage Device）： OSD 是 Ceph 中用于存储数据的对象存储设备。OSD 的数量对系统的数据分布均匀性有直接影响，因此不宜过少。为了充分发挥 Ceph 系统的优势，通常需要配置至少数百个 OSD。 File → Object映射 这个映射过程的目的是将用户操作的 File 转换为 RADOS 能够处理的 Object。这个过程相对简单，本质上就是按照 Object 的最大尺寸对 File 进行切分，类似于磁盘阵列中的条带化（striping）过程。这种切分有两个主要好处： 将大小不定的 File 转换为具有一致最大尺寸的 Object，使得 RADOS 能够更高效地管理这些数据。 将对单一 File 的串行处理转变为对多个 Object 的并行处理，从而提高处理效率。 每一个切分后的 Object 将获得一个唯一的 Object ID (oid)，其生成方式非常简单，是一种线性映射。具体来说，ino 表示待操作 File 的元数据，可以简单理解为该 File 的唯一 ID；ono 则是由该 File 切分产生的某个 Object 的序号。而 oid 就是将这个序号简单地附加在该 File 的 ID 之后得到的。举个例子，如果一个 ID 为 filename 的 File 被切分成了 3 个 Object，那么其 Object 的序号依次为 0、1 和 2，最终得到的 oid 就依次为 filename0、filename1 和 filename2。 这里有一个隐含的问题，即 ino 的唯一性必须得到保证，否则后续的映射将无法正确进行。 Object → PG 映射 当一个 File 被映射为一个或多个 Object 后，需要将每个 Object 独立地映射到一个 PG 中。这个过程相对简单，具体计算过程如下： 1Hash(oid) &amp; mask -&gt; pgid 这个计算过程分为两步： 使用 Ceph 系统指定的静态哈希算法计算 oid 的哈希值，将 oid 转换为一个近似均匀分布的伪随机值。 将这个伪随机值与 mask 进行按位与操作，得到最终的 PG 序号 (pgid)。 根据 RADOS 的设计，PG 的总数为 m（m 应该为 2 的整数幂），则 mask 的值为 m - 1。哈希值计算和按位与操作的结果就是从所有 m 个 PG 中近似均匀地随机选择一个。这种机制保证了在大量 Object 和大量 PG 存在的情况下，Object 和 PG 之间的映射近似均匀。由于 Object 是由 File 切分而来，大部分 Object 的尺寸相同，因此这一映射最终保证了各个 PG 中存储的 Object 的总数据量的近似均匀性。 这里强调“大量”是因为，只有在 Object 和 PG 数量较多时，这种伪随机关系的近似均匀性才有效，Ceph 的数据存储均匀性才能得到保障。为了确保这一点，一方面，Object 的最大尺寸应该被合理配置，以使得相同数量的 File 能被切分成更多的 Object；另一方面，Ceph 建议 PG 的总数应为 OSD 总数的数百倍，以确保有足够数量的 PG 供映射使用。 PG → OSD 映射 第三次映射是将作为对象逻辑组织单元的 PG 映射到实际存储单元 OSD 上。RADOS 使用了一种称为 CRUSH（Controlled Replication Under Scalable Hashing）的算法，将 pgid 代入其中，然后得到一组包含 n 个 OSD。这 n 个 OSD 共同负责存储和维护一个 PG 中的所有对象。通常，n 的值根据实际应用中的可靠性需求而定，在生产环境下通常为 3。具体到每个 OSD，由其上运行的 OSD Daemon 负责执行映射到本地的对象在本地文件系统中的存储、访问、元数据维护等操作。 与对象到 PG 的映射中采用的哈希算法不同，CRUSH 算法的结果并非绝对不变，而会受到其他因素的影响，主要有两个： 当前系统状态：即集群运行图。当系统中的 OSD 状态或数量发生变化时，集群运行图可能会改变，这将影响 PG 与 OSD 之间的映射关系。 存储策略配置：这与数据的安全性相关。系统管理员可以通过策略配置指定承载同一个 PG 的 3 个 OSD 分别位于数据中心的不同服务器或机架上，从而提高存储的可靠性。 因此，只有在系统状态和存储策略都不发生变化时，PG 和 OSD 之间的映射关系才是固定的。在实际使用中，策略配置通常一经设定就不会改变。而系统状态的变化可能是由于设备损坏或存储集群规模的扩大。好在 Ceph 提供了对这些变化的自动化支持，因此，即便 PG 与 OSD 之间的映射关系发生变化，也不会对应用产生影响。实际上，Ceph 利用 CRUSH 算法的动态特性，可以根据需要将一个 PG 动态迁移到不同的 OSD 组合上，从而自动实现高可靠性和数据分布再平衡等特性。 选择 CRUSH 算法而非其他哈希算法的原因有两点： 可配置性：CRUSH 算法具有可配置特性，可以根据管理员的配置参数决定 OSD 的物理位置映射策略。 稳定性：CRUSH 算法具有特殊的“稳定性”，即当系统中加入新的 OSD 导致系统规模增大时，大部分 PG 与 OSD 之间的映射关系不会改变，只有少部分 PG 的映射关系会发生变化并引发数据迁移。这种特性使得系统在扩展时能够保持相对稳定，避免了普通哈希算法可能带来的大规模数据迁移问题。 至此为止，Ceph通过3次映射，完成了从File到Object、Object到PG、PG再到OSD的整个映射过程。从整个过程可以看到，这里没有任何的全局性查表操作需求。至于唯一的全局性数据结构：集群运行图。它的维护和操作都是轻量级的，不会对系统的可扩展性、性能等因素造成影响。 接下来的一个问题是:为什么需要引人PG并在Object与OSD之间增加一层映射呢？可以想象一下，如果没有 PG 这一层的映射，会是什么情况？在这种情况下，需要采用某种算法将 Object 直接映射到一组 OSD 上。如果这种算法是某种固定映射的哈希算法，这就意味着一个 Object 将被固定映射在一组 OSD 上。当其中一个或多个 OSD 损坏时，Object 无法自动迁移到其他 OSD 上（因为映射函数不允许），而当系统为了扩容新增 OSD 时，Object 也无法被再平衡到新的 OSD 上（同样因为映射函数不允许）。这些限制违背了 Ceph 系统高可靠性和高自动化的设计初衷。 即便使用一个动态算法（如 CRUSH 算法）来完成这一映射，似乎可以避免静态映射带来的问题。但这样会导致各个 OSD 处理的本地元数据量大幅增加，计算复杂度和维护工作量也会大幅上升。 例如，在 Ceph 的现有机制中，一个 OSD 通常需要与其他承载同一个 PG 的 OSD 交换信息，以确定各自是否工作正常或是否需要进行维护。由于每个 OSD 承载约数百个 PG，而每个 PG 通常有 3 个 OSD，因此，在一定时间内，一个 OSD 大约需要进行数百次至数千次的信息交换。 然而，如果没有 PG 存在，一个 OSD 需要与其他承载同一个 Object 的 OSD 交换信息。由于每个 OSD 可能承载高达数百万个 Object，在同样时间内，一个 OSD 大约需要进行数百万次甚至数千万次的信息交换。这种状态维护成本显然过高。 综上所述，引入 PG 有至少两方面的好处：一方面，实现了 Object 和 OSD 之间的动态映射，为 Ceph 的可靠性和自动化等特性的实现提供了可能；另一方面，有效简化了数据的存储组织，大大降低了系统的维护和管理成本。 Ceph数据读写过程Ceph的读&#x2F;写操作采用Primary-Replica模型，客户端只向Object所对应OSD set的Primary OSD发起读&#x2F;写请求，这保证了数据的强一致性。当Primary OSD收到Object的写请求时，它负责把数据发送给其他副本，只有这个数据被保存在所有的OSD上时，Primary OSD才应答Object的写请求，这保证了副本的一致性。 写入数据这里以Object写入为例，假定一个PG被映射到3个OSD上。Object写入流程如图所示。 当某个客户端需要向Ceph集群写入一个File时，首先需要在本地完成前面所述的寻址流程，将File变为一个Object，然后找出存储该Object的一组共3个OSD，这3个OSD具有各自不同的序号，序号最靠前的那个OSD就是这一组中的Primary OSD，而后两个则依次Secondary OSD和Tertiary OSD。找出3个OSD后，客户端将直接和Primary OSD进行通信，发起写入操作(步骤1)。 Primary OSD收到请求后，分别向Secondary OSD和Tertiary OSD发起写人操作(步骤2和步骤3)。当Secondary OSD和Tertiary OSD各自完成写入操作后，将分别向Primary OSD发送确认信息(步骤4和步骤5)。当Primary OSD确认其他两个OSD的写入完成后，则自己也完成数据写入，并向客户端确认Object写入操作完成(步骤6)。之所以采用这样的写入流程，本质上是为了保证写入过程中的可靠性，尽可能避免出现数据丢失的情况。同时，由于客户端只需要向Primary OSD发送数据，因此在互联网使用场景下的外网带宽和整体访问延迟又得到了一定程度的优化。当然，这种可靠性机制必然导致较长的延迟，特别是，如果等到所有的OSD都将数据写入磁盘后再向客户端发送确认信号，则整体延迟可能难以忍受。因此， Ceph可以分两次向客户端进行确认。当各个OSD都将数据写入内存缓冲区后，就先向客户端发送一次确认，此时客户端即可以向下执行。待各个OSD都将数据写入磁盘后，会向客户端发送一个最终确认信号，此时客户端可以根据需要删除本地数据。分析上述流程可以看出，在正常情况下，客户端可以独立完成OSD寻址操作，而不必依赖于其他系统模块。因此，大量的客户端可以同时和大量的OSD进行并行操作。同时，如果一个File被切分成多个Object，这多个Object也可被并行发送至多个OSD上。从OSD的角度来看，由于同一个OSD在不同的PG中的角色不同，因此，其工作压力也可以被尽可能均匀地分担，从而避免单个OSD变成性能瓶颈。 读取数据如果需要读取数据，客户端只需完成同样的寻址过程，并直接和Primary OSD联系。在目前的Ceph设计中，被读取的数据默认由Primary OSD提供，但也可以设置允许从其他OSD中获取，以分散读取压力从而提高性能。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph体系架构","slug":"Ceph体系架构","date":"2021-05-07T02:19:42.000Z","updated":"2024-07-27T14:30:54.244Z","comments":true,"path":"Ceph体系架构/","permalink":"https://watsonlu6.github.io/Ceph%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/","excerpt":"","text":"Ceph 官方定义Ceph is a unified, distributed storage system designed for excellent performance, reliability and scalability.(Ceph 是一种为优秀的性能、可靠性和可扩展性而设计的统一的、分布式的存储系统。) Ceph 设计思路 充分发挥存储设备自身的计算能力。 采用具有计算能力的设备作为存储系统的存储节点。 去除所有的中心点。 解决单点故障点和当系统规模扩大时出现的规模和性能瓶颈问题。 Ceph的设计哲学 每个组件必须可扩展 不存在单点故障 解决方案必须是基于软件的 可摆脱专属硬件的束缚即可运行在常规硬件上 推崇自我管理 Ceph体系结构首先作为一个存储系统，Ceph在物理上必然包含一个存储集群，以及这个存储集群的应用或客户端。Ceph客户端又需要一定的协议与Ceph存储集群进行交互，Ceph的逻辑层次演化如图所示。OSD：主要功能包括存储数据，处理数据的复制、恢复、回补、平衡数据分布，并将一些相关数据提供给Ceph Monitor。一个Ceph的存储集群，至少需要两个Ceph OSD来实现active+clean健康状态和有效的保存数据的双副本。一旦应用程序向ceph集群发出写操作，数据就以对象的形式存储在OSD中，OSD是Ceph集群中存储实际用户数据的唯一组件。通常，一个OSD守护进程绑定到集群中的一个物理磁盘。因此，通常来说，Ceph集群中物理磁盘的总数与在每个物理磁盘上存储用户数据的OSD守护进程的总数相同。 MON：Ceph的监控器，主要功能是维护整个集群健康状态，提供一致性的决策。 MDS：主要保存的是Ceph文件系统的元数据。（Ceph的块存储和对象存储都不需要Ceph MDS） RADOS：Ceph基于可靠的、自动化的、分布式的对象存储(Reliabl,Autonomous,Distributed Object Storage, RADOS )提供了一个可无限扩展的存储集群，RADOS是Ceph最为关键的技术，它是一个支持海量存储对象的分布式对象存储系统。RADOS层本身就是一个完整的对象存储系统，事实上，所有存储在Ceph系统中的用户数据最终都是由这一层来存储。RADOS层确保数据始终保持一致，他执行数据复制、故障检测和恢复，以及跨集群节点的数据迁移和再平衡。 RADOS集群主要由两种节点组成：为数众多的OSD，负责完成数据存储和维护；若干个Monitor，负责完成系统状态检测和维护。OSD和Monion之间互相传递节点的状态信息，共同得出系统的总体运行状态，并保存在一个全局数据结构中，即所谓的集群运行图(Cluster Map )里。集群运行图与RADOS提供的特定算法相配合，便实现了Ceph的许多优秀特性。 Librados：Librados库实际上是对RADOS进行抽象和封装，并向上层提供API，支持PHP、Ruby、Java、Python、C和C++编程语言。它为Ceph存储集群（RADOS）提供了本机接口，并为其他服务提供基础，如RBD、RGW和CephFS，这些服务构建在Librados之上，Librados还支持从应用程序直接访问RADOS，没有HTTP开销。 RBD：RBD提供了一个标准的块设备接口，常用于在虚拟化的场景下为虚拟机创建存储卷，Red Hat已经将RBD驱动集成在QEMU&#x2F;KVM中，以提高虚拟机的访问性能。 RADOS GW：Ceph对象网关RADOS GW提供对象存储服务，是一个构建在Librados库之上的对象存储接口，为应用访问Ceph集群提供了一个与Amazon S3和OpenStack Swift兼容的RESTful风格的 网关。 Ceph FS：Ceph文件系统提供了一个符合posix标准的文件系统，它使用Ceph存储集群在文件系统上存储用户数据。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"块存储_文件系统存储_对象存储的区别","slug":"块存储-文件存储-对象存储的区别","date":"2021-04-23T08:29:15.000Z","updated":"2024-07-27T14:30:41.200Z","comments":true,"path":"块存储-文件存储-对象存储的区别/","permalink":"https://watsonlu6.github.io/%E5%9D%97%E5%AD%98%E5%82%A8-%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"定义角度 块存储指以扇区为基础，一个或者连续的扇区组成一个块，也叫物理块。它是在文件系统与块设备(例如：磁盘驱动器)之间。利用多个物理硬盘的并发能力。关注的是写入偏移位置。 文件系统存储文件系统存储也称为文件级存储或基于文件的存储，数据会以单条信息的形式存储在文件夹中。当需要访问该数据时，计算机需要知道相应的查找路径，存储在文件中的数据会根据数量有限的元数据来进行整理和检索，这些元数据会告诉计算机文件所在的确切位置。它就像是数据文件的库卡目录。 对象存储对象存储，也称为基于对象的存储，是一种扁平结构，其中的文件被拆分成多个部分并散布在多个硬件间。在对象存储中，数据会被分解为称为“对象”的离散单元，并保存在单个存储库中，而不是作为文件夹中的文件或服务器上的块来保存。 使用角度 块存储生活中常见的块存储设备（也叫“块设备”）比如，插在你本地电脑上的U盘、硬盘，你电脑连接的iSCSI等。从使用上来说，块级的存储如果是第一次使用，那么必须需要进行一次格式化的操作，创建出一个文件系统，然后才可以使用。例如新买的U盘、硬盘、或者新发现的iSCSI设备等，首次使用的时候都需要进行一次格式化操作，创建出一个文件系统，然后才可以将你的文件拷贝到U盘、硬盘、或者新发现的iSCSI设备中。 文件系统存储文件系统存储是最常见的一种文件内系统，我们日常对操作系使用中，基本上能够直接接触到的都就是这种，能够直接访问的C、D、E盘，电脑里的一个目录，网上邻居的空间都是文件级的存储。块级的存储设备经过格式化以及挂载（win 会自动挂载）之后，你就将一个块级的存储变成了文件级的存储。 对象存储对象存储一般来说并不是给我们人直接去使用的，从使用者角度来说，它更适合用用于给程序使用。平时最常见的一般就是百度网盘，其后端对接的就是对象存储。还有就网页上的图片、视频，其本身也是存储在对象存储的文件系统中的。如果要直接使用对象级的存储，你会发现对象级的存储本身是非常的简单的（但是对人来说不方便），它只有简单的几种命令如上传、下载、删除，并且你只需要知道某个文件的编号（如：”d5t35e6tdud725dgs6u2hdsh27dh27d7” 这不是名字）就可以直接对它进行上传、下载、删除等操作，不需要像文件级那样，直到文件的具体的路径（如:D:\\photo\\1.jpg），并且他也只有这几种操作，如果你想编辑文件，那只能将文件下载下来编辑好之后在进行上传（这也是它对人来说不方便的原因之一） 技术角度块级、文件级、对象级技术上的区别，首先要明白两个概念第一，无论是那个级别的存储系统，其数据都是会存储在物理的存储设备上的，这些存储设备现在常见的基本上就两种机械硬盘、固态硬盘。第二，任何数据都是由两部”数据“分组成的，一部分是”数据本身”(下文中“数据”指”数据本身“)，另一部分就是这些“数据”的”元数据“。所谓的”元数据”就是用来描述”数据”的”数据”。包括数据所在的位置，文件的长度（大小），文件的访问权限、文件的时间戳（创建时间、修改时间….），元数据本身也是数据。 块存储对于块级来说，如果要通过块设备来访问一段数据的话，你自己需要知道这些数据具体是存在于那个存储设备上的位置上，例如如果你要从块设备上读取一张照片，你就要高速存储设备：我要从第2块硬盘中的从A位置开始到B位置的数据，硬盘的驱动就会将这个数据给你。读取照片的过程中照片的具体位置就是元数据，也就是说块级的存储中要求程序自己保存元数据。 文件系统存储如果需要自己保存元数据的话就太麻烦了，上文也说了，元数据本身也是数据，实际上元数据也是存储在硬盘上的，那么如何访问元数据这个数据呢其实，文件级的元数据是存储在固定位置的，存储的位置和方式是大家事先约定好的，这个约定就叫做文件系统，例如EXT4、FAT32、XFS、NTFS等。借助于这些约定，我们就不用自己去维护一个表去记录每一份数据的具体存储位置了。我们只需要直到我们存储的文件的路径和名字就好了，例如我们想要 D:\\1.jpg 这个文件，那么你只需要告诉文件系统 D:\\1.jpg 这个位置就可以了，去硬盘的哪里找D:\\1.jpg 数据的真身，就是文件系统的工作了 对象存储对象级存储，文件级的元数据实际上是和数据放在一起的，就像一本书每本书都有一个目录，这个目录描述的是这本书上内容的索引，目录就是书内容的“元数据”，而对象存储，会有一本书只放目录（元数据），其他更多的书只有内容，并且内容都是被拆分好的一段一段的，就是说你会看每本书上面的内容完全是混在在一起的，这一页的前两行是书A的某句话，后面就跟的是书D的某句话，如果只放目录（元数据）那本书，你根本不知道这里写的是啥。对象及存储将一切的文件都视作对象，并且将对象按照固定的”形式”组合或拆分的存储在存储设备中，并且将数据的元数据部分完全的独立出来，进行单独的管理。 对比 从距离（io路径） 上来说（相对于传统的存储），块存储的使用者距离最底层实际存储数据的存储设备是最近的，对象级是最远的。 从使用上来说，块存储需要使用者自己直到数据的真是位置，需要自己管理记录这些数据，所以使用上是最复杂的，而对象存储的接口最简单，基本上只有上传、下载、删除，并且不需要自己保存元数据，也不需要直到文件的索引路径，所以使用上是最简单的。但是从方便角度来讲还是文件存储最方便。 从性能上来说，综合的来讲（在特定的应用场合）性能最好的是块存储，它主要用在数据库、对延时要求非常高的场景中，对象存储多用于互联网，因为扩展性好，容量可以做的非常的大。对于人类来说，如果不借助特定的客户端、APP，使用文件存储是最友好最简单的。 应用场景 块存储： 要求高性能的应用，如数据库需要高IO，用块存储比较合适。 文件系统存储： 需局域网共享的应用，如文件共享，视频处理，动画渲染&#x2F;高性能计算。 对象存储： 互联网领域的存储，如点播&#x2F;视频监控的视频存储、图片存储、网盘存储、静态网页存储等，以及异地备份存储&#x2F;归档等。 为什么块级的存储性能最好&emsp;&emsp;首先要明确一点，要明确，每次在发生数据读取访问的时候，实际上对应系统的底层是发生了多次IO的（主要是要对元数据进行访问），例如，你要打开文件1.txt ，操作系统回去进行文件是否存在的查询，以及读写权限的查询等操作，这些操作实际上都是对于元数据的访问。&emsp;&emsp;然后，相对于其它的存储方式，块存储的元数据是有操作系统自己管理的，也就是说整个文件系统（元数据）是存在在操做系统的内存中的，这样操作系统在进行元数据管理的时候可以直和自己的内存打交道。而文件系统存储和对象存储，它的文件系统是存在于另一台服务器上的，这样在进行元数据访问时就需要从网络进行访问，这样要比从内存访问慢得多。&emsp;&emsp;总结来讲，就是块级存储的元数据在系统本机中，在进行元数据访问（每次读写文件实际都会在操作系统底层发生），会更快，因为其它的级别的存储元数据都要通过网络访问。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"存储基础","slug":"云存储/存储基础","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"存储基础","slug":"存储基础","permalink":"https://watsonlu6.github.io/tags/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}]},{"title":"云存储概述","slug":"云存储概述","date":"2021-04-18T15:09:26.000Z","updated":"2024-07-27T14:30:28.553Z","comments":true,"path":"云存储概述/","permalink":"https://watsonlu6.github.io/%E4%BA%91%E5%AD%98%E5%82%A8%E6%A6%82%E8%BF%B0/","excerpt":"","text":"云存储概述 云存储的概述云存储是指通过网络，将分布在不同地方的多种存储设备，通过应用软件集合起来共同对外提供数据存储和业务访问功能的一个系统。云存储是云计算系统中的一种新型网络存储技术。云存储对外提供的存储能力以统一、简单的数据服务接口来提供和展现。用户不用关心数据具体存放在哪个设备、哪个区域，甚至不知道数据到底是怎么保存的，他们只需要关心需要多少存储空间、什么时间能够拿到数据、数据存放的安全性和可用性如何即可。 云存储的实现模式云存储的实现模式有多种，云存储的架构可以由传统的存储架构延伸而来，也可以采用全新的云计算架构。云存储的实现可以是软件的，也可以是硬件的。云存储的实现模式可以是块存储、文件存储和对象存储。 块存储 块存储：最接近底层的存储，可以对数据进行任意格式化操作，可以在上面运行数据库等性能要求高的应用。可以给虚拟机使用。 文件存储 文件存储：对数据以文件的形式进行存储，支持复杂的文件操作，适合共享文件和协作工作。典型应用场景：NAS。 对象存储 对象存储：将数据以对象形式存储，通过唯一的对象ID进行访问，适合存储大规模非结构化数据，支持RESTful API接口。典型应用场景：云存储服务，视频、图片存储。 为什么需要多元化存储？由于不同的应用场景对存储的需求不同，单一的存储类型无法满足所有需求。大规模存储系统需要支持多种存储类型和多种存储协议，比如NFS、iSCSI、HDFS、S3等。多元化存储可以更好地适应各种应用场景，提高存储系统的灵活性和适应性。 文件如何通过分布式存储在许多服务器中分布式存储系统将数据分散存储在多个物理设备上。文件被切分成多个小块，存储在不同的服务器上。通过分布式哈希表（DHT）等算法确定数据块的位置，实现数据的快速定位和访问。通过数据复制和纠删码技术提高数据的可靠性和可用性。 文件被读取时如何快速找到数据块，确保大目标的组合数据不会丢失？元数据服务器（MDS）存储文件系统的元数据，包括文件名、文件大小、数据块位置等信息。客户端请求文件时，首先查询MDS获取元数据，然后根据元数据访问对应的数据块。分布式文件系统中常用的元数据管理技术包括分布式哈希表（DHT）、目录树、名称节点（NameNode）等。 如果文件丢失怎么办？由于分布式存储系统的特性，单一数据副本的丢失不会导致数据不可恢复。分布式存储系统采用数据冗余和副本机制，常见的冗余技术包括数据复制和纠删码。数据复制是将同一份数据存储在多个节点上，副本数通常为3个或更多。纠删码是一种冗余编码技术，通过增加校验数据，在数据块丢失的情况下，可以通过校验数据恢复原始数据。分布式存储系统在后台自动检测数据块的健康状态，发现数据丢失或损坏时，自动启动数据恢复机制，确保数据的完整性和可用性。 存储系统的数据可靠性（就像RAID）以及可用性（如高可用性）是如何解决的？存储系统的数据可靠性和可用性通过多种技术手段来保证。RAID技术通过数据条带化、镜像、奇偶校验等方法，提高单一存储设备的数据可靠性。分布式存储系统通过数据复制和纠删码技术，在多个节点上存储数据副本，提高数据的可靠性和可用性。高可用性通过冗余设计实现，常见的高可用架构包括双机热备、集群等。通过负载均衡技术，将用户请求分散到多个节点上，提高系统的可用性。 写入的数据是如何被保护的？数据写入时，采用多副本机制，确保数据的一致性和可靠性。写时复制（Copy-On-Write，COW）是一种常见的技术，通过在写入数据前复制一份旧数据，确保数据写入过程中的一致性。在分布式存储系统中，数据写入时，通常会先写入多个副本，只有所有副本写入成功后，才算写入成功。数据写入过程中的故障检测和处理机制，确保数据的可靠性和一致性。 多人多设备协作时，如何保证远程协作时数据的一致性？分布式存储系统通过分布式一致性协议（如Paxos、Raft）确保数据的一致性。在多个节点之间进行数据写入时，一致性协议保证数据的一致性和正确性。冲突检测和处理机制，在多人协作时，检测并解决数据冲突。分布式锁和事务机制，确保数据的一致性和完整性。 节省存储空间存储系统采用数据压缩和数据去重技术，减少存储空间占用。数据压缩通过减少数据的冗余，提高存储空间的利用率。数据去重通过检测和删除重复数据，节省存储空间。在大规模存储系统中，数据压缩和去重技术可以显著降低存储成本，提高存储效率。 避免存储固定的文件存储系统采用分级存储和冷热数据分离策略，提高存储资源的利用率。根据数据的访问频率和重要性，将数据存储在不同的存储介质上。频繁访问的数据存储在高速存储设备上，减少访问延迟。较少访问的数据存储在低成本存储设备上，降低存储成本。通过冷热数据分离，优化存储资源的使用，提高存储系统的性能和效率。 IO速度要有保证存储系统通过多种技术手段保证IO速度。使用高速缓存技术，将热点数据缓存到内存或SSD中，减少数据访问延迟。采用预取技术，在数据请求到达前提前加载数据，提高数据访问速度。使用QoS（Quality of Service）技术，为不同的应用场景和用户提供不同的IO优先级和带宽保障。负载均衡技术，将IO请求分散到多个存储节点上，避免单点瓶颈，提高IO性能。 版本控制存储系统提供版本控制功能，允许用户对数据进行版本管理。在数据修改前，保存一份旧版本的数据，用户可以根据需要回滚到旧版本。版本控制功能确保数据的可追溯性和可恢复性，防止数据丢失和误操作。在分布式存储系统中，版本控制功能通过元数据管理和数据快照技术实现。元数据管理记录数据的版本信息和变更历史，数据快照技术保存数据的不同版本。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"存储基础","slug":"云存储/存储基础","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"存储基础","slug":"存储基础","permalink":"https://watsonlu6.github.io/tags/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}]},{"title":"Linux存储栈","slug":"Linux存储栈","date":"2021-04-06T12:24:56.000Z","updated":"2024-07-27T03:55:23.469Z","comments":true,"path":"Linux存储栈/","permalink":"https://watsonlu6.github.io/Linux%E5%AD%98%E5%82%A8%E6%A0%88/","excerpt":"","text":"Linux存储栈Linux存储系统包括用户接口和存储设备接口两个部分，前者以流形式处理数据，后者以块形式处理数据，文件系统在中间起到承上启下的作用。应用程序通过系统调用发出写请求，文件系统定位请求位置并转换成块设备所需的块，然后发送到设备上。内存在此过程中作为磁盘缓冲，将上下两部分隔离成异步运行的两个过程，避免频繁的磁盘同步。当数据需要从页面缓存同步到磁盘时，请求被包装成包含多个bio的request，每个bio包含需要同步的数据页。磁盘在执行写操作时，需要通过IO请求调度合理安排顺序，减少磁头的频繁移动，提高磁盘性能。 用户视角的数据流接口 应用程序通过系统调用（如write、read等）与操作系统交互。这些调用使得数据以流的形式被处理。 存储设备的块接口 数据在底层存储设备（如硬盘、SSD等）中以块（通常是512字节或4096字节）为单位进行读写操作。 文件系统的中间角色 位置定位：文件系统负责将用户的读写请求定位到存储设备的具体块位置。 数据转换：将数据流转换为存储设备所需的块结构，并将这些块组织成bio（block I&#x2F;O）请求。 内存作为缓冲 页面缓存：内存中的页面缓存（Page Cache）用于暂时存储数据，以减少频繁的磁盘I&#x2F;O操作。 异步运行：将用户操作与底层存储设备的实际写操作异步化，提升系统效率。对于用户态程序来说，数据尽量保留在内存中，这样可以减少频繁的数据同步。 I&#x2F;O请求调度 请求封装：从页面缓存同步到磁盘的请求被封装成request，每个request包含多个bio，而每个bio又包含具体的数据页。 调度策略：操作系统会对I&#x2F;O请求进行调度，优化执行顺序，尽量减少磁盘磁头的来回移动，提高磁盘的读写效率。 Linux数据写入流程 应用程序发出写请求：比如，应用程序通过write系统调用写入数据。 文件系统处理：文件系统接收请求，找到对应的文件位置，将数据写入页面缓存。 内存缓冲处理：数据暂存在内存的页面缓存中，以等待后续的写入操作。 请求调度与封装：页面缓存的数据需要同步到磁盘时，被封装成bio和request。 I&#x2F;O调度执行：调度器优化I&#x2F;O请求的执行顺序，减少磁头移动，提高写入效率。 数据写入磁盘：最终，数据从页面缓存同步到磁盘的指定位置，完成写操作。通过以上流程，Linux存储系统在保证数据一致性的同时，最大限度地提高了性能和效率。 系统调用&emsp;&emsp;用户应用程序访问并使用内核所提供的各种服务的途径即是系统调用。在内核和应用程序交叉的地方，内核提供了一组系统调用接口，通过这组接口，应用程序可以访问系统硬件和各种操作系统资源。用户可以通过文件系统相关的调用请求系统打开问价、关闭文件和读写文件。&emsp;&emsp;内核提供的这组系统调用称为系统调用接口层。系统调用接口把应用程序的请求传达给内核，待内核处理完请求后再将处理结果返回给应用程序。&emsp;&emsp;32位Linux，CPU能访问4GB的虚拟空间，其中低3GB的地址是应用层的地址空间，高地址的1GB是留给内核使用的。内核中所有线程共享这1GB的地址空间，而每个进程可以有自己的独立的3GB的虚拟空间，互不干扰。&emsp;&emsp;当一个进程运行的时候，其用到文件的代码段，数据段等都是映射到内存地址区域的，这个功能是通过系统调用mmap()来完成的。mmap()将文件（由文件句柄fd所指定）从偏移offset的位置开始的长度为length的一个块映射到内存区域中，从而把文件的某一段映射到进程的地址空间，这样程序就可以通过访问内存的方式访问文件了。与read()&#x2F;write()相比，使用mmap的方式对文件进行访问，带来的一个显著好处就是可以减少一次用户空间到内核空间的复制，在某些场景下，如音频、视频等大文件，可以带来性能的提升。 文件系统&emsp;&emsp;Linux文件系统的体系结构是一个对复杂系统进行抽象化，通过使用一组通用的API函数，Linux就可以在多种存储设备上支持多种文件系统，使得它拥有了与其他操作系统和谐共存的能力。&emsp;&emsp;Linux中文件的概念并不局限于普通的磁盘文件，而是由字节序列构成的信息载体，I&#x2F;O设备、socket等也被包括在内。因为有了文件的存在，所以需要衍生文件系统去进行组织和管理文件，而为了支持各种各样的文件系统，所以有了虚拟文件系统的出现。文件系统是一种对存储设备上的文件、数据进行存储和组织的机制。&emsp;&emsp;虚拟文件系统通过在各种具体的文件系统上建立了一个抽象层，屏蔽了不同文件系统间的差异，通过虚拟文件系统分层架构，在对文件进行操作时，便不需要去关心相关文件所在的具体文件系统细节。通过系统调用层，可以在不同文件系统之间复制和移动数据，正是虚拟文件系统使得这种跨越不同存储设备和不同文件系统的操作成为了可能。 虚拟文件系统象类型 超级块（Super Block）超级块对象代表了一个已经安装的文件系统，用于存储该文件系统的相关信息，如文件系统的类型、大小、状态等。对基于磁盘的文件系统， 这类对象通常存放在磁盘特定的扇区上。对于并非基于磁盘的文件系统，它们会现场创建超级块对象并将其保存在内存中。 索引节点（Inode）索引节点对象代表存储设备上的一个实际物理文件，用于存储该文件的有关信息。Linux将文件的相关信息（如访问权限、大小、创建时间等）与文件本身区分开。文件的相关信息又被称为文件的元数据。 目录项（Dentry) 目录项对象描述了文件系统的层次结构，一个路径的各个组成部分，不管是目录（虚拟文件系统将目录当作文件来处理）还是普通文件，都是一个目录项对象。 文件 文件对象代表已经被进程打开的文件，主要用于建立进程和文件之间的对应关系。它由open()系统调用创建，由close()系统调用销毁，当且仅当进程访问文件期间存在于内存中，同一个物理文件可能存在多个对应的文件对象，但其对应的索引节点对象是唯一的。 Page Cache&emsp;&emsp;Page Cache，通常也称为文件缓存，使用内存Page Cache文件的逻辑内容，从而提高对磁盘文件的访问速度。Page Cache是以物理页为单位对磁盘文件进行缓存的。&emsp;&emsp;应用程序尝试读取某块数据的时候，会首先查找Page Cache，如果这块数据已经存放在Page Cache中，那么就可以立即返回给应用程序，而不需要再进行实际的物理磁盘操作。如果不能在Page Cache中发现要读取的数据，那么就需要先将数据从磁盘读取到Page Cache中，同样，对于写操作来说，应用程序也会将数据写到Page Cache中，再根据所采用的写操作机制，判断数据是否应该立即被写到磁盘上 Direct I&#x2F;O和Buffered I&#x2F;O进程产生的IO路径主要有Direct I&#x2F;O和Buffered I&#x2F;O两条 标准I&#x2F;O： 也称为Buffered I&#x2F;O；Linux会将I&#x2F;O的数据缓存在Page Cache中，也就是说，数据会先被复制到内核的缓冲区，再从内核的缓冲区复制到应用程序的用户地址空间。在Buffered I&#x2F;O机制中，在没有CPU干预的情况下，可以通过DMA操作在磁盘和Page Cache之间直接进行数据的传输，在一定程度上分离了应用程序和物理设备，但是没有方法能直接在应用程序的地址空间和磁盘之间进行数据传输，数据在传输过程中需要在用户空间和Page Cache之间进行多次数据复制操作，这将带来较大的CPU开销。 Direct I&#x2F;O： 可以省略使用Buffered I&#x2F;O中的内核缓冲区，数据可以直接在用户空间和磁盘之间进行传输，从而使得缓存应用程序可以避开复杂系统级别的缓存结构，执行自定义的数据读写管理，从而降低系统级别的管理对应用程序访问数据的影响。如果在块设备中执行Direct I&#x2F;O，那么进程必须在打开文件的时候将对文件的访问模式设置为O_DIRECT，这样就等于告诉Linux进程在接下来将使用Direct I&#x2F;O方式去读写文件，且传输的数据不经过内核中的Page Cache。Direct I&#x2F;O最主要的优点就是通过减少内核缓冲区和用户空间的数据复制次数，降低文件读写时所带来的CPU负载能力及内存带宽的占用率。如果传输的数据量很大，使用Direct IO的方式将会大大提高性能。然而，不经过内湖缓冲区直接进行磁盘的读写，必然会引起阻塞，因此通常Direct IO和AIO（异步IO）一起使用。 块层（Block Layer）&emsp;&emsp;块设备访问时，需要在介质的不同区间前后移动，对于内核来说，管理块设备要比管理字符设备复杂得多。&emsp;&emsp;系统调用read()触发相应的虚拟文件系统函数，虚拟文件系统判断请求是否已经在内核缓冲区里，如果不在，则判断如何执行读操作。如果内核必须从块设备上读取数据，就必须要确定数据在物理设备上的位置。这由映射层，即磁盘文件系统来完成。文件系统将文件访问映射为设备访问。在通用块层中，使用bio结构体来描述一个I&#x2F;O请求在上层文件系统与底层物理磁盘之间的关系。而到了Linux驱动，则是使用request结构体来描述向块设备发出的I&#x2F;O请求的。对于慢速的磁盘而言，请求的处理速度很慢，这是内核就提供一种队列的机制把这些I&#x2F;O请求添加到队列中，使用request_queue结构体来描述。&emsp;&emsp;bio和request是块层最核心的两个数据结构，其中，bio描述了磁盘里要真实操作的位置和Page Cache中的映射关系。作为Linux I&#x2F;O请求的基本单元，bio结构贯穿块层对I&#x2F;O请求处理的始终，每个bio对应磁盘里面一块连续的位置，bio结构中的bio_vec是一个bio的数据容器，专门用来保存bio的数据，包含一块数据所在页，以及页内的偏移及长度信息，通过这些信息就可以很清晰地描述数据具体什么位置。request用来描述单次I&#x2F;O请求，request_queue用来描述与设备相关的请求队列，每个块设备在块层都有一个request_queue与之对应，所有对该块设备的I&#x2F;O请求最后都会流经request_queue。块层正是借助bio、bio_vec、request、request_queue这几个结构将I&#x2F;O请求在内核I&#x2F;O子系统各个层次的处理过程联系起来。 I&#x2F;O调度算法： noop算法（不调度算法）、deadline算法（改良的电梯算法）、CFQ算法（完全公平调度算法，对于通用的服务器来说，CFQ是较好的选择，从Linux2.6.18版本开始，CFQ成为了默认的IO调度算法）。 I&#x2F;O合并： 将符合条件的多个IO请求合并成单个IO请求进行一并处理，从而提升IO请求的效率。进程产生的IO路径主要有Direct I&#x2F;O和Buffered I&#x2F;O两条，无论哪一条路径，在bio结构转换为request结构进行IO调度前都需要进入Plug队列进行蓄流（部分Direct IO产生的请求不需要经过蓄流），所以对IO请求来说，能够进行合并的位置主要有Page Cache、Plug List、IO调度器3个。而在块层中，Plug将块层的IO请求聚集起来，使得零散的请求有机会进行合并和排序，最终达到高效利用存储设备的目的。每个进程都有一个私有的Plug队列，进程在向通用块层派发IO派发请求之前如果开始蓄流的功能，那么IO请求在被发送给IO调度器之前都被保存在Plug队列中，直到泄流的时候才被批量交给调度器。蓄流主要是为了增加请求合并的机会，bio在进入Plug队列之前会尝试与Plug队列保存的request进行合并。当应用需要发多个bio请求的时候，比较好的办法是先蓄流，而不是一个个单独发给最终的硬盘。 LVM&emsp;&emsp;LVM，即Logical Volume Manager，逻辑卷管理器，是一种硬盘的虚拟化技术，可以允许用户的硬盘资源进行灵活的调整和动态管理。&emsp;&emsp;LVM是Linux系统对于硬盘分区管理的一种机制，诞生是为了解决硬盘设备在创建分区后不易修改分区大小的缺陷。尽管对硬盘的强制性扩容和缩容理论上是可行的，但是却可能造成数据丢失。LVM技术是通过在硬盘分区和文件系统之间增加一个逻辑层，提供了一个抽象的卷组，就可以把多块硬盘设备、硬盘分区，甚至RAID整体进行卷则合并。并可以根据情况进行逻辑上的虚拟分割，这样一来，用户不用关心物理硬盘设备的底层架构和布局，就可以实现对硬盘分区设备的动态调整。&emsp;&emsp;LVM通过在操作系统与物理存储资源之间引入逻辑卷（Logical Volume）的抽象，来解决传统磁盘分区管理工具的问题。LVM将众多不同的物理存储器资源（物理卷，Physical Volume）组成卷组（Volume Group），该卷组可以理解为普通系统的物理磁盘，但是卷组上不能创建或者安装文件系统，而是需要LVM先在卷组中创建一个逻辑卷，然后将ext3等文件系统安装在这个逻辑卷上，可以在不重新引导系统的前提下通过在卷组划分额外的空间，来为这个逻辑卷动态扩容。LVM的架构体系中，有三个很重要的概念： PV，物理卷，即实际存在的硬盘、分区或者RAID VG，卷组，是由多个物理卷组合形成的大的整体的卷组 LV，逻辑卷，是从卷组上分割出来的，可以使用使用的逻辑存储设备 条带化&emsp;&emsp;大多数磁盘系统都对访问次数（每秒的 I&#x2F;O 操作，IOPS）和数据传输率（每秒传输的数据量，TPS）有限制。当达到这些限制时，后面需要访问磁盘的进程就需要等待，这时就是所谓的磁盘冲突。避免磁盘冲突是优化 I&#x2F;O 性能的一个重要目标，而 I&#x2F;O 性能的优化与其他资源（如CPU和内存）的优化有着很大的区别 ,I&#x2F;O 优化最有效的手段是将 I&#x2F;O 最大限度的进行平衡。条带化技术就是一种自动的将 I&#x2F;O 的负载均衡到多个物理磁盘上的技术，条带化技术就是将一块连续的数据分成很多小部分并把他们分别存储到不同磁盘上去。这就能使多个进程同时访问数据的多个不同部分而不会造成磁盘冲突，而且在需要对这种数据进行顺序访问的时候可以获得最大程度上的 I&#x2F;O 并行能力，从而获得非常好的性能。很多操作系统、磁盘设备供应商、各种第三方软件都能做到条带化。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"存储基础","slug":"云存储/存储基础","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"存储基础","slug":"存储基础","permalink":"https://watsonlu6.github.io/tags/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}]},{"title":"Hello World","slug":"hello-world","date":"2021-04-01T15:09:26.000Z","updated":"2024-07-27T03:58:36.598Z","comments":true,"path":"hello-world/","permalink":"https://watsonlu6.github.io/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"},{"name":"存储基础","slug":"云存储/存储基础","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"},{"name":"存储基础","slug":"存储基础","permalink":"https://watsonlu6.github.io/tags/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}]}