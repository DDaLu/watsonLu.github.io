{"meta":{"title":"watson'blogs","subtitle":"","description":"","author":"John Doe","url":"https://watsonlu6.github.io","root":"/"},"pages":[],"posts":[{"title":"Ceph体系架构","slug":"Ceph体系架构","date":"2024-07-27T02:19:42.000Z","updated":"2024-07-27T02:20:10.481Z","comments":true,"path":"Ceph体系架构/","permalink":"https://watsonlu6.github.io/Ceph%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/","excerpt":"","text":"Ceph 官方定义Ceph is a unified, distributed storage system designed for excellent performance, reliability and scalability.(Ceph 是一种为优秀的性能、可靠性和可扩展性而设计的统一的、分布式的存储系统。) Ceph 设计思路 充分发挥存储设备自身的计算能力。 采用具有计算能力的设备作为存储系统的存储节点。 去除所有的中心点。 解决单点故障点和当系统规模扩大时出现的规模和性能瓶颈问题。 Ceph的设计哲学 每个组件必须可扩展 不存在单点故障 解决方案必须是基于软件的 可摆脱专属硬件的束缚即可运行在常规硬件上 推崇自我管理 Ceph体系结构首先作为一个存储系统，Ceph在物理上必然包含一个存储集群，以及这个存储集群的应用或客户端。Ceph客户端又需要一定的协议与Ceph存储集群进行交互，Ceph的逻辑层次演化如图所示。OSD：主要功能包括存储数据，处理数据的复制、恢复、回补、平衡数据分布，并将一些相关数据提供给Ceph Monitor。一个Ceph的存储集群，至少需要两个Ceph OSD来实现active+clean健康状态和有效的保存数据的双副本。一旦应用程序向ceph集群发出写操作，数据就以对象的形式存储在OSD中，OSD是Ceph集群中存储实际用户数据的唯一组件。通常，一个OSD守护进程绑定到集群中的一个物理磁盘。因此，通常来说，Ceph集群中物理磁盘的总数与在每个物理磁盘上存储用户数据的OSD守护进程的总数相同。 MON：Ceph的监控器，主要功能是维护整个集群健康状态，提供一致性的决策。 MDS：主要保存的是Ceph文件系统的元数据。（Ceph的块存储和对象存储都不需要Ceph MDS） RADOS：Ceph基于可靠的、自动化的、分布式的对象存储(Reliabl,Autonomous,Distributed Object Storage, RADOS )提供了一个可无限扩展的存储集群，RADOS是Ceph最为关键的技术，它是一个支持海量存储对象的分布式对象存储系统。RADOS层本身就是一个完整的对象存储系统，事实上，所有存储在Ceph系统中的用户数据最终都是由这一层来存储。RADOS层确保数据始终保持一致，他执行数据复制、故障检测和恢复，以及跨集群节点的数据迁移和再平衡。 RADOS集群主要由两种节点组成：为数众多的OSD，负责完成数据存储和维护；若干个Monitor，负责完成系统状态检测和维护。OSD和Monion之间互相传递节点的状态信息，共同得出系统的总体运行状态，并保存在一个全局数据结构中，即所谓的集群运行图(Cluster Map )里。集群运行图与RADOS提供的特定算法相配合，便实现了Ceph的许多优秀特性。 Librados：Librados库实际上是对RADOS进行抽象和封装，并向上层提供API，支持PHP、Ruby、Java、Python、C和C++编程语言。它为Ceph存储集群（RADOS）提供了本机接口，并为其他服务提供基础，如RBD、RGW和CephFS，这些服务构建在Librados之上，Librados还支持从应用程序直接访问RADOS，没有HTTP开销。 RBD：RBD提供了一个标准的块设备接口，常用于在虚拟化的场景下为虚拟机创建存储卷，Red Hat已经将RBD驱动集成在QEMU&#x2F;KVM中，以提高虚拟机的访问性能。 RADOS GW：Ceph对象网关RADOS GW提供对象存储服务，是一个构建在Librados库之上的对象存储接口，为应用访问Ceph集群提供了一个与Amazon S3和OpenStack Swift兼容的RESTful风格的 网关。 Ceph FS：Ceph文件系统提供了一个符合posix标准的文件系统，它使用Ceph存储集群在文件系统上存储用户数据。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"块存储_文件系统存储_对象存储的区别","slug":"块存储-文件存储-对象存储的区别","date":"2024-07-13T15:29:15.000Z","updated":"2024-07-13T15:48:21.670Z","comments":true,"path":"块存储-文件存储-对象存储的区别/","permalink":"https://watsonlu6.github.io/%E5%9D%97%E5%AD%98%E5%82%A8-%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"定义角度 块存储指以扇区为基础，一个或者连续的扇区组成一个块，也叫物理块。它是在文件系统与块设备(例如：磁盘驱动器)之间。利用多个物理硬盘的并发能力。关注的是写入偏移位置。 文件系统存储文件系统存储也称为文件级存储或基于文件的存储，数据会以单条信息的形式存储在文件夹中。当需要访问该数据时，计算机需要知道相应的查找路径，存储在文件中的数据会根据数量有限的元数据来进行整理和检索，这些元数据会告诉计算机文件所在的确切位置。它就像是数据文件的库卡目录。 对象存储对象存储，也称为基于对象的存储，是一种扁平结构，其中的文件被拆分成多个部分并散布在多个硬件间。在对象存储中，数据会被分解为称为“对象”的离散单元，并保存在单个存储库中，而不是作为文件夹中的文件或服务器上的块来保存。 使用角度 块存储生活中常见的块存储设备（也叫“块设备”）比如，插在你本地电脑上的U盘、硬盘，你电脑连接的iSCSI等。从使用上来说，块级的存储如果是第一次使用，那么必须需要进行一次格式化的操作，创建出一个文件系统，然后才可以使用。例如新买的U盘、硬盘、或者新发现的iSCSI设备等，首次使用的时候都需要进行一次格式化操作，创建出一个文件系统，然后才可以将你的文件拷贝到U盘、硬盘、或者新发现的iSCSI设备中。 文件系统存储文件系统存储是最常见的一种文件内系统，我们日常对操作系使用中，基本上能够直接接触到的都就是这种，能够直接访问的C、D、E盘，电脑里的一个目录，网上邻居的空间都是文件级的存储。块级的存储设备经过格式化以及挂载（win 会自动挂载）之后，你就将一个块级的存储变成了文件级的存储。 对象存储对象存储一般来说并不是给我们人直接去使用的，从使用者角度来说，它更适合用用于给程序使用。平时最常见的一般就是百度网盘，其后端对接的就是对象存储。还有就网页上的图片、视频，其本身也是存储在对象存储的文件系统中的。如果要直接使用对象级的存储，你会发现对象级的存储本身是非常的简单的（但是对人来说不方便），它只有简单的几种命令如上传、下载、删除，并且你只需要知道某个文件的编号（如：”d5t35e6tdud725dgs6u2hdsh27dh27d7” 这不是名字）就可以直接对它进行上传、下载、删除等操作，不需要像文件级那样，直到文件的具体的路径（如:D:\\photo\\1.jpg），并且他也只有这几种操作，如果你想编辑文件，那只能将文件下载下来编辑好之后在进行上传（这也是它对人来说不方便的原因之一） 技术角度块级、文件级、对象级技术上的区别，首先要明白两个概念第一，无论是那个级别的存储系统，其数据都是会存储在物理的存储设备上的，这些存储设备现在常见的基本上就两种机械硬盘、固态硬盘。第二，任何数据都是由两部”数据“分组成的，一部分是”数据本身”(下文中“数据”指”数据本身“)，另一部分就是这些“数据”的”元数据“。所谓的”元数据”就是用来描述”数据”的”数据”。包括数据所在的位置，文件的长度（大小），文件的访问权限、文件的时间戳（创建时间、修改时间….），元数据本身也是数据。 块存储对于块级来说，如果要通过块设备来访问一段数据的话，你自己需要知道这些数据具体是存在于那个存储设备上的位置上，例如如果你要从块设备上读取一张照片，你就要高速存储设备：我要从第2块硬盘中的从A位置开始到B位置的数据，硬盘的驱动就会将这个数据给你。读取照片的过程中照片的具体位置就是元数据，也就是说块级的存储中要求程序自己保存元数据。 文件系统存储如果需要自己保存元数据的话就太麻烦了，上文也说了，元数据本身也是数据，实际上元数据也是存储在硬盘上的，那么如何访问元数据这个数据呢其实，文件级的元数据是存储在固定位置的，存储的位置和方式是大家事先约定好的，这个约定就叫做文件系统，例如EXT4、FAT32、XFS、NTFS等。借助于这些约定，我们就不用自己去维护一个表去记录每一份数据的具体存储位置了。我们只需要直到我们存储的文件的路径和名字就好了，例如我们想要 D:\\1.jpg 这个文件，那么你只需要告诉文件系统 D:\\1.jpg 这个位置就可以了，去硬盘的哪里找D:\\1.jpg 数据的真身，就是文件系统的工作了 对象存储对象级存储，文件级的元数据实际上是和数据放在一起的，就像一本书每本书都有一个目录，这个目录描述的是这本书上内容的索引，目录就是书内容的“元数据”，而对象存储，会有一本书只放目录（元数据），其他更多的书只有内容，并且内容都是被拆分好的一段一段的，就是说你会看每本书上面的内容完全是混在在一起的，这一页的前两行是书A的某句话，后面就跟的是书D的某句话，如果只放目录（元数据）那本书，你根本不知道这里写的是啥。对象及存储将一切的文件都视作对象，并且将对象按照固定的”形式”组合或拆分的存储在存储设备中，并且将数据的元数据部分完全的独立出来，进行单独的管理。 对比 从距离（io路径） 上来说（相对于传统的存储），块存储的使用者距离最底层实际存储数据的存储设备是最近的，对象级是最远的。 从使用上来说，块存储需要使用者自己直到数据的真是位置，需要自己管理记录这些数据，所以使用上是最复杂的，而对象存储的接口最简单，基本上只有上传、下载、删除，并且不需要自己保存元数据，也不需要直到文件的索引路径，所以使用上是最简单的。但是从方便角度来讲还是文件存储最方便。 从性能上来说，综合的来讲（在特定的应用场合）性能最好的是块存储，它主要用在数据库、对延时要求非常高的场景中，对象存储多用于互联网，因为扩展性好，容量可以做的非常的大。对于人类来说，如果不借助特定的客户端、APP，使用文件存储是最友好最简单的。 应用场景 块存储： 要求高性能的应用，如数据库需要高IO，用块存储比较合适。 文件系统存储： 需局域网共享的应用，如文件共享，视频处理，动画渲染&#x2F;高性能计算。 对象存储： 互联网领域的存储，如点播&#x2F;视频监控的视频存储、图片存储、网盘存储、静态网页存储等，以及异地备份存储&#x2F;归档等。 为什么块级的存储性能最好&emsp;&emsp;首先要明确一点，要明确，每次在发生数据读取访问的时候，实际上对应系统的底层是发生了多次IO的（主要是要对元数据进行访问），例如，你要打开文件1.txt ，操作系统回去进行文件是否存在的查询，以及读写权限的查询等操作，这些操作实际上都是对于元数据的访问。&emsp;&emsp;然后，相对于其它的存储方式，块存储的元数据是有操作系统自己管理的，也就是说整个文件系统（元数据）是存在在操做系统的内存中的，这样操作系统在进行元数据管理的时候可以直和自己的内存打交道。而文件系统存储和对象存储，它的文件系统是存在于另一台服务器上的，这样在进行元数据访问时就需要从网络进行访问，这样要比从内存访问慢得多。&emsp;&emsp;总结来讲，就是块级存储的元数据在系统本机中，在进行元数据访问（每次读写文件实际都会在操作系统底层发生），会更快，因为其它的级别的存储元数据都要通过网络访问。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"存储基础","slug":"云存储/存储基础","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"存储基础","slug":"存储基础","permalink":"https://watsonlu6.github.io/tags/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}]},{"title":"云存储概述","slug":"云存储概述","date":"2024-07-11T15:09:26.000Z","updated":"2024-07-13T15:53:41.544Z","comments":true,"path":"云存储概述/","permalink":"https://watsonlu6.github.io/%E4%BA%91%E5%AD%98%E5%82%A8%E6%A6%82%E8%BF%B0/","excerpt":"","text":"云存储概述 云存储的概述云存储是指通过网络，将分布在不同地方的多种存储设备，通过应用软件集合起来共同对外提供数据存储和业务访问功能的一个系统。云存储是云计算系统中的一种新型网络存储技术。云存储对外提供的存储能力以统一、简单的数据服务接口来提供和展现。用户不用关心数据具体存放在哪个设备、哪个区域，甚至不知道数据到底是怎么保存的，他们只需要关心需要多少存储空间、什么时间能够拿到数据、数据存放的安全性和可用性如何即可。 云存储的实现模式云存储的实现模式有多种，云存储的架构可以由传统的存储架构延伸而来，也可以采用全新的云计算架构。云存储的实现可以是软件的，也可以是硬件的。云存储的实现模式可以是块存储、文件存储和对象存储。 块存储 块存储：最接近底层的存储，可以对数据进行任意格式化操作，可以在上面运行数据库等性能要求高的应用。可以给虚拟机使用。 文件存储 文件存储：对数据以文件的形式进行存储，支持复杂的文件操作，适合共享文件和协作工作。典型应用场景：NAS。 对象存储 对象存储：将数据以对象形式存储，通过唯一的对象ID进行访问，适合存储大规模非结构化数据，支持RESTful API接口。典型应用场景：云存储服务，视频、图片存储。 为什么需要多元化存储？由于不同的应用场景对存储的需求不同，单一的存储类型无法满足所有需求。大规模存储系统需要支持多种存储类型和多种存储协议，比如NFS、iSCSI、HDFS、S3等。多元化存储可以更好地适应各种应用场景，提高存储系统的灵活性和适应性。 文件如何通过分布式存储在许多服务器中分布式存储系统将数据分散存储在多个物理设备上。文件被切分成多个小块，存储在不同的服务器上。通过分布式哈希表（DHT）等算法确定数据块的位置，实现数据的快速定位和访问。通过数据复制和纠删码技术提高数据的可靠性和可用性。 文件被读取时如何快速找到数据块，确保大目标的组合数据不会丢失？元数据服务器（MDS）存储文件系统的元数据，包括文件名、文件大小、数据块位置等信息。客户端请求文件时，首先查询MDS获取元数据，然后根据元数据访问对应的数据块。分布式文件系统中常用的元数据管理技术包括分布式哈希表（DHT）、目录树、名称节点（NameNode）等。 如果文件丢失怎么办？由于分布式存储系统的特性，单一数据副本的丢失不会导致数据不可恢复。分布式存储系统采用数据冗余和副本机制，常见的冗余技术包括数据复制和纠删码。数据复制是将同一份数据存储在多个节点上，副本数通常为3个或更多。纠删码是一种冗余编码技术，通过增加校验数据，在数据块丢失的情况下，可以通过校验数据恢复原始数据。分布式存储系统在后台自动检测数据块的健康状态，发现数据丢失或损坏时，自动启动数据恢复机制，确保数据的完整性和可用性。 存储系统的数据可靠性（就像RAID）以及可用性（如高可用性）是如何解决的？存储系统的数据可靠性和可用性通过多种技术手段来保证。RAID技术通过数据条带化、镜像、奇偶校验等方法，提高单一存储设备的数据可靠性。分布式存储系统通过数据复制和纠删码技术，在多个节点上存储数据副本，提高数据的可靠性和可用性。高可用性通过冗余设计实现，常见的高可用架构包括双机热备、集群等。通过负载均衡技术，将用户请求分散到多个节点上，提高系统的可用性。 写入的数据是如何被保护的？数据写入时，采用多副本机制，确保数据的一致性和可靠性。写时复制（Copy-On-Write，COW）是一种常见的技术，通过在写入数据前复制一份旧数据，确保数据写入过程中的一致性。在分布式存储系统中，数据写入时，通常会先写入多个副本，只有所有副本写入成功后，才算写入成功。数据写入过程中的故障检测和处理机制，确保数据的可靠性和一致性。 多人多设备协作时，如何保证远程协作时数据的一致性？分布式存储系统通过分布式一致性协议（如Paxos、Raft）确保数据的一致性。在多个节点之间进行数据写入时，一致性协议保证数据的一致性和正确性。冲突检测和处理机制，在多人协作时，检测并解决数据冲突。分布式锁和事务机制，确保数据的一致性和完整性。 节省存储空间存储系统采用数据压缩和数据去重技术，减少存储空间占用。数据压缩通过减少数据的冗余，提高存储空间的利用率。数据去重通过检测和删除重复数据，节省存储空间。在大规模存储系统中，数据压缩和去重技术可以显著降低存储成本，提高存储效率。 避免存储固定的文件存储系统采用分级存储和冷热数据分离策略，提高存储资源的利用率。根据数据的访问频率和重要性，将数据存储在不同的存储介质上。频繁访问的数据存储在高速存储设备上，减少访问延迟。较少访问的数据存储在低成本存储设备上，降低存储成本。通过冷热数据分离，优化存储资源的使用，提高存储系统的性能和效率。 IO速度要有保证存储系统通过多种技术手段保证IO速度。使用高速缓存技术，将热点数据缓存到内存或SSD中，减少数据访问延迟。采用预取技术，在数据请求到达前提前加载数据，提高数据访问速度。使用QoS（Quality of Service）技术，为不同的应用场景和用户提供不同的IO优先级和带宽保障。负载均衡技术，将IO请求分散到多个存储节点上，避免单点瓶颈，提高IO性能。 版本控制存储系统提供版本控制功能，允许用户对数据进行版本管理。在数据修改前，保存一份旧版本的数据，用户可以根据需要回滚到旧版本。版本控制功能确保数据的可追溯性和可恢复性，防止数据丢失和误操作。在分布式存储系统中，版本控制功能通过元数据管理和数据快照技术实现。元数据管理记录数据的版本信息和变更历史，数据快照技术保存数据的不同版本。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"存储基础","slug":"云存储/存储基础","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"存储基础","slug":"存储基础","permalink":"https://watsonlu6.github.io/tags/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}]},{"title":"Linux存储栈","slug":"Linux存储栈","date":"2024-07-10T12:24:56.000Z","updated":"2024-07-13T15:50:12.827Z","comments":true,"path":"Linux存储栈/","permalink":"https://watsonlu6.github.io/Linux%E5%AD%98%E5%82%A8%E6%A0%88/","excerpt":"","text":"Linux存储栈Linux存储系统包括用户接口和存储设备接口两个部分，前者以流形式处理数据，后者以块形式处理数据，文件系统在中间起到承上启下的作用。应用程序通过系统调用发出写请求，文件系统定位请求位置并转换成块设备所需的块，然后发送到设备上。内存在此过程中作为磁盘缓冲，将上下两部分隔离成异步运行的两个过程，避免频繁的磁盘同步。当数据需要从页面缓存同步到磁盘时，请求被包装成包含多个bio的request，每个bio包含需要同步的数据页。磁盘在执行写操作时，需要通过IO请求调度合理安排顺序，减少磁头的频繁移动，提高磁盘性能。 用户视角的数据流接口 应用程序通过系统调用（如write、read等）与操作系统交互。这些调用使得数据以流的形式被处理。 存储设备的块接口 数据在底层存储设备（如硬盘、SSD等）中以块（通常是512字节或4096字节）为单位进行读写操作。 文件系统的中间角色 位置定位：文件系统负责将用户的读写请求定位到存储设备的具体块位置。 数据转换：将数据流转换为存储设备所需的块结构，并将这些块组织成bio（block I&#x2F;O）请求。 内存作为缓冲 页面缓存：内存中的页面缓存（Page Cache）用于暂时存储数据，以减少频繁的磁盘I&#x2F;O操作。 异步运行：将用户操作与底层存储设备的实际写操作异步化，提升系统效率。对于用户态程序来说，数据尽量保留在内存中，这样可以减少频繁的数据同步。 I&#x2F;O请求调度 请求封装：从页面缓存同步到磁盘的请求被封装成request，每个request包含多个bio，而每个bio又包含具体的数据页。 调度策略：操作系统会对I&#x2F;O请求进行调度，优化执行顺序，尽量减少磁盘磁头的来回移动，提高磁盘的读写效率。 Linux数据写入流程 应用程序发出写请求：比如，应用程序通过write系统调用写入数据。 文件系统处理：文件系统接收请求，找到对应的文件位置，将数据写入页面缓存。 内存缓冲处理：数据暂存在内存的页面缓存中，以等待后续的写入操作。 请求调度与封装：页面缓存的数据需要同步到磁盘时，被封装成bio和request。 I&#x2F;O调度执行：调度器优化I&#x2F;O请求的执行顺序，减少磁头移动，提高写入效率。 数据写入磁盘：最终，数据从页面缓存同步到磁盘的指定位置，完成写操作。通过以上流程，Linux存储系统在保证数据一致性的同时，最大限度地提高了性能和效率。 系统调用&emsp;&emsp;用户应用程序访问并使用内核所提供的各种服务的途径即是系统调用。在内核和应用程序交叉的地方，内核提供了一组系统调用接口，通过这组接口，应用程序可以访问系统硬件和各种操作系统资源。用户可以通过文件系统相关的调用请求系统打开问价、关闭文件和读写文件。&emsp;&emsp;内核提供的这组系统调用称为系统调用接口层。系统调用接口把应用程序的请求传达给内核，待内核处理完请求后再将处理结果返回给应用程序。&emsp;&emsp;32位Linux，CPU能访问4GB的虚拟空间，其中低3GB的地址是应用层的地址空间，高地址的1GB是留给内核使用的。内核中所有线程共享这1GB的地址空间，而每个进程可以有自己的独立的3GB的虚拟空间，互不干扰。&emsp;&emsp;当一个进程运行的时候，其用到文件的代码段，数据段等都是映射到内存地址区域的，这个功能是通过系统调用mmap()来完成的。mmap()将文件（由文件句柄fd所指定）从偏移offset的位置开始的长度为length的一个块映射到内存区域中，从而把文件的某一段映射到进程的地址空间，这样程序就可以通过访问内存的方式访问文件了。与read()&#x2F;write()相比，使用mmap的方式对文件进行访问，带来的一个显著好处就是可以减少一次用户空间到内核空间的复制，在某些场景下，如音频、视频等大文件，可以带来性能的提升。 文件系统&emsp;&emsp;Linux文件系统的体系结构是一个对复杂系统进行抽象化，通过使用一组通用的API函数，Linux就可以在多种存储设备上支持多种文件系统，使得它拥有了与其他操作系统和谐共存的能力。&emsp;&emsp;Linux中文件的概念并不局限于普通的磁盘文件，而是由字节序列构成的信息载体，I&#x2F;O设备、socket等也被包括在内。因为有了文件的存在，所以需要衍生文件系统去进行组织和管理文件，而为了支持各种各样的文件系统，所以有了虚拟文件系统的出现。文件系统是一种对存储设备上的文件、数据进行存储和组织的机制。&emsp;&emsp;虚拟文件系统通过在各种具体的文件系统上建立了一个抽象层，屏蔽了不同文件系统间的差异，通过虚拟文件系统分层架构，在对文件进行操作时，便不需要去关心相关文件所在的具体文件系统细节。通过系统调用层，可以在不同文件系统之间复制和移动数据，正是虚拟文件系统使得这种跨越不同存储设备和不同文件系统的操作成为了可能。 虚拟文件系统象类型 超级块（Super Block）超级块对象代表了一个已经安装的文件系统，用于存储该文件系统的相关信息，如文件系统的类型、大小、状态等。对基于磁盘的文件系统， 这类对象通常存放在磁盘特定的扇区上。对于并非基于磁盘的文件系统，它们会现场创建超级块对象并将其保存在内存中。 索引节点（Inode）索引节点对象代表存储设备上的一个实际物理文件，用于存储该文件的有关信息。Linux将文件的相关信息（如访问权限、大小、创建时间等）与文件本身区分开。文件的相关信息又被称为文件的元数据。 目录项（Dentry) 目录项对象描述了文件系统的层次结构，一个路径的各个组成部分，不管是目录（虚拟文件系统将目录当作文件来处理）还是普通文件，都是一个目录项对象。 文件 文件对象代表已经被进程打开的文件，主要用于建立进程和文件之间的对应关系。它由open()系统调用创建，由close()系统调用销毁，当且仅当进程访问文件期间存在于内存中，同一个物理文件可能存在多个对应的文件对象，但其对应的索引节点对象是唯一的。 Page Cache&emsp;&emsp;Page Cache，通常也称为文件缓存，使用内存Page Cache文件的逻辑内容，从而提高对磁盘文件的访问速度。Page Cache是以物理页为单位对磁盘文件进行缓存的。&emsp;&emsp;应用程序尝试读取某块数据的时候，会首先查找Page Cache，如果这块数据已经存放在Page Cache中，那么就可以立即返回给应用程序，而不需要再进行实际的物理磁盘操作。如果不能在Page Cache中发现要读取的数据，那么就需要先将数据从磁盘读取到Page Cache中，同样，对于写操作来说，应用程序也会将数据写到Page Cache中，再根据所采用的写操作机制，判断数据是否应该立即被写到磁盘上 Direct I&#x2F;O和Buffered I&#x2F;O进程产生的IO路径主要有Direct I&#x2F;O和Buffered I&#x2F;O两条 标准I&#x2F;O： 也称为Buffered I&#x2F;O；Linux会将I&#x2F;O的数据缓存在Page Cache中，也就是说，数据会先被复制到内核的缓冲区，再从内核的缓冲区复制到应用程序的用户地址空间。在Buffered I&#x2F;O机制中，在没有CPU干预的情况下，可以通过DMA操作在磁盘和Page Cache之间直接进行数据的传输，在一定程度上分离了应用程序和物理设备，但是没有方法能直接在应用程序的地址空间和磁盘之间进行数据传输，数据在传输过程中需要在用户空间和Page Cache之间进行多次数据复制操作，这将带来较大的CPU开销。 Direct I&#x2F;O： 可以省略使用Buffered I&#x2F;O中的内核缓冲区，数据可以直接在用户空间和磁盘之间进行传输，从而使得缓存应用程序可以避开复杂系统级别的缓存结构，执行自定义的数据读写管理，从而降低系统级别的管理对应用程序访问数据的影响。如果在块设备中执行Direct I&#x2F;O，那么进程必须在打开文件的时候将对文件的访问模式设置为O_DIRECT，这样就等于告诉Linux进程在接下来将使用Direct I&#x2F;O方式去读写文件，且传输的数据不经过内核中的Page Cache。Direct I&#x2F;O最主要的优点就是通过减少内核缓冲区和用户空间的数据复制次数，降低文件读写时所带来的CPU负载能力及内存带宽的占用率。如果传输的数据量很大，使用Direct IO的方式将会大大提高性能。然而，不经过内湖缓冲区直接进行磁盘的读写，必然会引起阻塞，因此通常Direct IO和AIO（异步IO）一起使用。 块层（Block Layer）&emsp;&emsp;块设备访问时，需要在介质的不同区间前后移动，对于内核来说，管理块设备要比管理字符设备复杂得多。&emsp;&emsp;系统调用read()触发相应的虚拟文件系统函数，虚拟文件系统判断请求是否已经在内核缓冲区里，如果不在，则判断如何执行读操作。如果内核必须从块设备上读取数据，就必须要确定数据在物理设备上的位置。这由映射层，即磁盘文件系统来完成。文件系统将文件访问映射为设备访问。在通用块层中，使用bio结构体来描述一个I&#x2F;O请求在上层文件系统与底层物理磁盘之间的关系。而到了Linux驱动，则是使用request结构体来描述向块设备发出的I&#x2F;O请求的。对于慢速的磁盘而言，请求的处理速度很慢，这是内核就提供一种队列的机制把这些I&#x2F;O请求添加到队列中，使用request_queue结构体来描述。&emsp;&emsp;bio和request是块层最核心的两个数据结构，其中，bio描述了磁盘里要真实操作的位置和Page Cache中的映射关系。作为Linux I&#x2F;O请求的基本单元，bio结构贯穿块层对I&#x2F;O请求处理的始终，每个bio对应磁盘里面一块连续的位置，bio结构中的bio_vec是一个bio的数据容器，专门用来保存bio的数据，包含一块数据所在页，以及页内的偏移及长度信息，通过这些信息就可以很清晰地描述数据具体什么位置。request用来描述单次I&#x2F;O请求，request_queue用来描述与设备相关的请求队列，每个块设备在块层都有一个request_queue与之对应，所有对该块设备的I&#x2F;O请求最后都会流经request_queue。块层正是借助bio、bio_vec、request、request_queue这几个结构将I&#x2F;O请求在内核I&#x2F;O子系统各个层次的处理过程联系起来。 I&#x2F;O调度算法： noop算法（不调度算法）、deadline算法（改良的电梯算法）、CFQ算法（完全公平调度算法，对于通用的服务器来说，CFQ是较好的选择，从Linux2.6.18版本开始，CFQ成为了默认的IO调度算法）。 I&#x2F;O合并： 将符合条件的多个IO请求合并成单个IO请求进行一并处理，从而提升IO请求的效率。进程产生的IO路径主要有Direct I&#x2F;O和Buffered I&#x2F;O两条，无论哪一条路径，在bio结构转换为request结构进行IO调度前都需要进入Plug队列进行蓄流（部分Direct IO产生的请求不需要经过蓄流），所以对IO请求来说，能够进行合并的位置主要有Page Cache、Plug List、IO调度器3个。而在块层中，Plug将块层的IO请求聚集起来，使得零散的请求有机会进行合并和排序，最终达到高效利用存储设备的目的。每个进程都有一个私有的Plug队列，进程在向通用块层派发IO派发请求之前如果开始蓄流的功能，那么IO请求在被发送给IO调度器之前都被保存在Plug队列中，直到泄流的时候才被批量交给调度器。蓄流主要是为了增加请求合并的机会，bio在进入Plug队列之前会尝试与Plug队列保存的request进行合并。当应用需要发多个bio请求的时候，比较好的办法是先蓄流，而不是一个个单独发给最终的硬盘。 LVM&emsp;&emsp;LVM，即Logical Volume Manager，逻辑卷管理器，是一种硬盘的虚拟化技术，可以允许用户的硬盘资源进行灵活的调整和动态管理。&emsp;&emsp;LVM是Linux系统对于硬盘分区管理的一种机制，诞生是为了解决硬盘设备在创建分区后不易修改分区大小的缺陷。尽管对硬盘的强制性扩容和缩容理论上是可行的，但是却可能造成数据丢失。LVM技术是通过在硬盘分区和文件系统之间增加一个逻辑层，提供了一个抽象的卷组，就可以把多块硬盘设备、硬盘分区，甚至RAID整体进行卷则合并。并可以根据情况进行逻辑上的虚拟分割，这样一来，用户不用关心物理硬盘设备的底层架构和布局，就可以实现对硬盘分区设备的动态调整。&emsp;&emsp;LVM通过在操作系统与物理存储资源之间引入逻辑卷（Logical Volume）的抽象，来解决传统磁盘分区管理工具的问题。LVM将众多不同的物理存储器资源（物理卷，Physical Volume）组成卷组（Volume Group），该卷组可以理解为普通系统的物理磁盘，但是卷组上不能创建或者安装文件系统，而是需要LVM先在卷组中创建一个逻辑卷，然后将ext3等文件系统安装在这个逻辑卷上，可以在不重新引导系统的前提下通过在卷组划分额外的空间，来为这个逻辑卷动态扩容。LVM的架构体系中，有三个很重要的概念： PV，物理卷，即实际存在的硬盘、分区或者RAID VG，卷组，是由多个物理卷组合形成的大的整体的卷组 LV，逻辑卷，是从卷组上分割出来的，可以使用使用的逻辑存储设备 条带化&emsp;&emsp;大多数磁盘系统都对访问次数（每秒的 I&#x2F;O 操作，IOPS）和数据传输率（每秒传输的数据量，TPS）有限制。当达到这些限制时，后面需要访问磁盘的进程就需要等待，这时就是所谓的磁盘冲突。避免磁盘冲突是优化 I&#x2F;O 性能的一个重要目标，而 I&#x2F;O 性能的优化与其他资源（如CPU和内存）的优化有着很大的区别 ,I&#x2F;O 优化最有效的手段是将 I&#x2F;O 最大限度的进行平衡。条带化技术就是一种自动的将 I&#x2F;O 的负载均衡到多个物理磁盘上的技术，条带化技术就是将一块连续的数据分成很多小部分并把他们分别存储到不同磁盘上去。这就能使多个进程同时访问数据的多个不同部分而不会造成磁盘冲突，而且在需要对这种数据进行顺序访问的时候可以获得最大程度上的 I&#x2F;O 并行能力，从而获得非常好的性能。很多操作系统、磁盘设备供应商、各种第三方软件都能做到条带化。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"存储基础","slug":"云存储/存储基础","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"存储基础","slug":"存储基础","permalink":"https://watsonlu6.github.io/tags/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}]},{"title":"Ceph相关数据结构","slug":"Ceph相关数据结构","date":"2023-07-26T15:33:02.000Z","updated":"2024-07-26T16:21:44.610Z","comments":true,"path":"Ceph相关数据结构/","permalink":"https://watsonlu6.github.io/Ceph%E7%9B%B8%E5%85%B3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","excerpt":"","text":"Ceph 相关数据结构要想深入到Ceph的源码底层，就必须对代码通用库里的一些关键，常见的数据结构进行学习，这样才能更好的理解源代码。从最高的逻辑层次为Pool的概念，然后是PG的概念。其次是OSDＭap记录了集群的所有的配置信息。数据结构OSDOp是一个操作上下文的封装。结构object_info_t保存了一个元数据信息和访问信息。对象ObjectState是在object_info_t基础上添加了一些内存的状态信息。SnapSetContext和ObjectContext分别保存了快照和对象上下文相关的信息。Session保存了一个端到端的链接相关的上下文。 PoolPool是整个集群层面定义的一个逻辑的存储池。对一个Pool可以设置相应的数据冗余类型，目前有副本和纠删码两种实现。数据结构pg_pool_t用于保存Pool的相关信息。Pool的数据结构如下：（src&#x2F;osd&#x2F;osd_types.h） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150struct pg_pool_t &#123; static const char *APPLICATION_NAME_CEPHFS; static const char *APPLICATION_NAME_RBD; static const char *APPLICATION_NAME_RGW; enum &#123; TYPE_REPLICATED = 1, // replication 副本 //TYPE_RAID4 = 2, // raid4 (never implemented) 从来没实现的raid4 TYPE_ERASURE = 3, // erasure-coded 纠删码 &#125;; enum &#123; FLAG_HASHPSPOOL = 1&lt;&lt;0, // hash pg seed and pool together (instead of adding) FLAG_FULL = 1&lt;&lt;1, // pool is full FLAG_EC_OVERWRITES = 1&lt;&lt;2, // enables overwrites, once enabled, cannot be disabled FLAG_INCOMPLETE_CLONES = 1&lt;&lt;3, // may have incomplete clones (bc we are/were an overlay) FLAG_NODELETE = 1&lt;&lt;4, // pool can&#x27;t be deleted FLAG_NOPGCHANGE = 1&lt;&lt;5, // pool&#x27;s pg and pgp num can&#x27;t be changed FLAG_NOSIZECHANGE = 1&lt;&lt;6, // pool&#x27;s size and min size can&#x27;t be changed FLAG_WRITE_FADVISE_DONTNEED = 1&lt;&lt;7, // write mode with LIBRADOS_OP_FLAG_FADVISE_DONTNEED FLAG_NOSCRUB = 1&lt;&lt;8, // block periodic scrub FLAG_NODEEP_SCRUB = 1&lt;&lt;9, // block periodic deep-scrub FLAG_FULL_QUOTA = 1&lt;&lt;10, // pool is currently running out of quota, will set FLAG_FULL too FLAG_NEARFULL = 1&lt;&lt;11, // pool is nearfull FLAG_BACKFILLFULL = 1&lt;&lt;12, // pool is backfillfull FLAG_SELFMANAGED_SNAPS = 1&lt;&lt;13, // pool uses selfmanaged snaps FLAG_POOL_SNAPS = 1&lt;&lt;14, // pool has pool snaps FLAG_CREATING = 1&lt;&lt;15, // initial pool PGs are being created &#125;; utime_t create_time; //Pool创建时间 uint64_t flags; ///&lt; FLAG_* Pool的相关标志 __u8 type; ///&lt; TYPE_* 类型 __u8 size, min_size; ///&lt;Pool的size和min_size，即副本数和至少保证的副本数 __u8 crush_rule; ///&lt; crush placement rule rule的编号 __u8 object_hash; ///&lt; hash mapping object name to ps 对象映射的hash函数 __u8 pg_autoscale_mode; ///&lt; PG_AUTOSCALE_MODE_ PG数自动增减模式private: __u32 pg_num = 0, pgp_num = 0; ///&lt; pg、pgp的数量 __u32 pg_num_pending = 0; ///&lt; pg_num we are about to merge down to __u32 pg_num_target = 0; ///&lt; pg_num we should converge toward __u32 pgp_num_target = 0; ///&lt; pgp_num we should converge towardpublic: map&lt;string,string&gt; properties; ///&lt; OBSOLETE string erasure_code_profile; ///&lt; name of the erasure code profile in OSDMap epoch_t last_change; ///&lt; most recent epoch changed, exclusing snapshot changes /// last epoch that forced clients to resend epoch_t last_force_op_resend = 0; /// last epoch that forced clients to resend (pre-nautilus clients only) epoch_t last_force_op_resend_prenautilus = 0; /// last epoch that forced clients to resend (pre-luminous clients only) epoch_t last_force_op_resend_preluminous = 0; /// metadata for the most recent PG merge pg_merge_meta_t last_pg_merge_meta; snapid_t snap_seq; ///&lt; seq for per-pool snapshot epoch_t snap_epoch; ///&lt; osdmap epoch of last snap uint64_t auid; ///&lt; who owns the pg uint64_t quota_max_bytes; ///&lt; maximum number of bytes for this pool uint64_t quota_max_objects; ///&lt; maximum number of objects for this pool /* * Pool snaps (global to this pool). These define a SnapContext for * the pool, unless the client manually specifies an alternate * context. */ map&lt;snapid_t, pool_snap_info_t&gt; snaps; /* * Alternatively, if we are defining non-pool snaps (e.g. via the * Ceph MDS), we must track @removed_snaps (since @snaps is not * used). Snaps and removed_snaps are to be used exclusive of each * other! */ interval_set&lt;snapid_t&gt; removed_snaps; unsigned pg_num_mask, pgp_num_mask; // Tier cache : Base Storage = N : 1 // ceph osd tier add &#123;data_pool&#125; &#123;cache pool&#125; set&lt;uint64_t&gt; tiers; ///&lt; pools that are tiers of us int64_t tier_of; ///&lt; pool for which we are a tier // Note that write wins for read+write ops // WriteBack mode, read_tier is same as write_tier. Both are cache pool. // Diret mode. cache pool is read_tier, not write_tier. // ceph osd tier set-overlay &#123;data_pool&#125; &#123;cache_pool&#125; int64_t read_tier; ///&lt; pool/tier for objecter to direct reads to int64_t write_tier; ///&lt; pool/tier for objecter to direct writes to // Set cache mode // ceph osd tier cache-mode &#123;cache-pool&#125; &#123;cache-mode&#125; cache_mode_t cache_mode; ///&lt; cache pool mode uint64_t target_max_bytes; ///&lt; tiering: target max pool size uint64_t target_max_objects; ///&lt; tiering: target max pool size // 目标脏数据率：当脏数据比例达到这个值，后台 agent 开始 flush 数据 uint32_t cache_target_dirty_ratio_micro; ///&lt; cache: fraction of target to leave dirty // 高目标脏数据率：当脏数据比例达到这个值，后台 agent 开始高速 flush 数据 uint32_t cache_target_dirty_high_ratio_micro; ///&lt; cache: fraction of target to flush with high speed // 数据满的比率：当数据达到这个比例时，认为数据已满，需要进行缓存淘汰 uint32_t cache_target_full_ratio_micro; ///&lt; cache: fraction of target to fill before we evict in earnest // 对象在 cache 中被刷入到 storage 层的最小时间 uint32_t cache_min_flush_age; ///&lt; minimum age (seconds) before we can flush // 对象在 cache 中被淘汰的最小时间 uint32_t cache_min_evict_age; ///&lt; minimum age (seconds) before we can evict // HitSet 相关参数 HitSet::Params hit_set_params; ///&lt; The HitSet params to use on this pool // 每间隔 hit_set_period 一段时间，系统重新产生一个新的 hit_set 对象来记录对象的h缓存统计信息 uint32_t hit_set_period; ///&lt; periodicity of HitSet segments (seconds) // 记录系统保存最近的多少个 hit_set 记录 uint32_t hit_set_count; ///&lt; number of periods to retain // hitset archive 对象的命名规则 bool use_gmt_hitset; ///&lt; use gmt to name the hitset archive object uint32_t min_read_recency_for_promote; ///&lt; minimum number of HitSet to check before promote on read uint32_t min_write_recency_for_promote; ///&lt; minimum number of HitSet to check before promote on write uint32_t hit_set_grade_decay_rate; ///&lt; current hit_set has highest priority on objects ///&lt; temperature count,the follow hit_set&#x27;s priority decay ///&lt; by this params than pre hit_set //当前hit_set在对象温度计数上具有最高优先级，后续hit_set的优先级比预hit_set衰减此参数 uint32_t hit_set_search_last_n; ///&lt; accumulate atmost N hit_sets for temperature 为温度累积最多N次hit_sets uint32_t stripe_width; ///&lt; erasure coded stripe size in bytes uint64_t expected_num_objects; ///&lt; expected number of objects on this pool, a value of 0 indicates ///&lt; user does not specify any expected value bool fast_read; ///&lt; whether turn on fast read on the pool or not pool_opts_t opts; ///&lt; options /// application -&gt; key/value metadata map&lt;string, std::map&lt;string, string&gt;&gt; application_metadata;private: vector&lt;uint32_t&gt; grade_table;public: uint32_t get_grade(unsigned i) const &#123; if (grade_table.size() &lt;= i) return 0; return grade_table[i]; &#125; void calc_grade_table() &#123; unsigned v = 1000000; grade_table.resize(hit_set_count); // hit_set_count记录系统保存最近的多少个 hit_set 记录 for (unsigned i = 0; i &lt; hit_set_count; i++) &#123; v = v * (1 - (hit_set_grade_decay_rate / 100.0)); grade_table[i] = v; &#125; &#125;&#125;; 数据结构pg_pool_t的成员变量和方法较多，不一一介绍了。 PGPG可以认为是一组对象的集合，该集合里的对象有共同特征：副本都分布在相同的OSD列表中。结构体pg_t只是一个PG的静态描述信息（只有三个成员变量），类PG及其子类ReplicatedPG都是和PG相关的处理。pg_t的数据结构如下：（src&#x2F;osd&#x2F;osd_types.h） 12345678struct pg_t &#123; uint64_t m_pool; //pg所在的pool uint32_t m_seed; //pg的序号 static const uint8_t calc_name_buf_size = 36; // max length for max values len(&quot;18446744073709551615.ffffffff&quot;) + future suffix len(&quot;_head&quot;) + &#x27;\\0&#x27; hobject_t get_hobj_start() const; hobject_t get_hobj_end(unsigned pg_num) const; static void generate_test_instances(list&lt;pg_t*&gt;&amp; o);&#125;; OSDMapOSDMap类定义了Ceph整个集群的全局信息。它由Monitor实现管理，并以全量或者增量的方式向整个集群扩散。每一个epoch对应的OSDMap都需要持久化保存在meta下对应对象的omap属性中。内部类Incremental以增量的形式保存了OSDMap新增的信息。OSDMap包含了四类信息：首先是集群的信息，其次是pool的信息，然后是临时PG相关信息，最后就是所有OSD的状态信息。OSDMap类的数据结构如下：（src&#x2F;osd&#x2F;OSDMap.h） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889class OSDMap &#123;public: MEMPOOL_CLASS_HELPERS(); typedef interval_set&lt; snapid_t, mempool::osdmap::flat_map&lt;snapid_t,snapid_t&gt;&gt; snap_interval_set_t; class Incremental &#123; public: MEMPOOL_CLASS_HELPERS(); //系统相关的信息 /// feature bits we were encoded with. the subsequent OSDMap /// encoding should match. uint64_t encode_features; uuid_d fsid; //当前集群的fsid值 epoch_t epoch; //当前集群的epoch值 new epoch; we are a diff from epoch-1 to epoch utime_t modified; //创建修改的时间戳 int64_t new_pool_max; //incremented by the OSDMonitor on each pool create int32_t new_flags; int8_t new_require_osd_release = -1; // full (rare) bufferlist fullmap; // in lieu of below. bufferlist crush;......private: //集群相关的信息 uuid_d fsid; //当前集群的fsid值 epoch_t epoch; //当前集群的epoch值 what epoch of the osd cluster descriptor is this utime_t created, modified; //创建、修改的时间戳 epoch start time int32_t pool_max; //最大的pool数量 the largest pool num, ever uint32_t flags; //一些标志信息 //OSD相关的信息 int num_osd; //OSD的总数量 not saved; see calc_num_osds int num_up_osd; //处于up状态的OSD的数量 not saved; see calc_num_osds int num_in_osd; //处于in状态的OSD的数量 not saved; see calc_num_osds int32_t max_osd; //OSD的最大数目 vector&lt;uint32_t&gt; osd_state; //OSD的状态 mempool::osdmap::map&lt;int32_t,uint32_t&gt; crush_node_flags; // crush node -&gt; CEPH_OSD_* flags mempool::osdmap::map&lt;int32_t,uint32_t&gt; device_class_flags; // device class -&gt; CEPH_OSD_* flags utime_t last_up_change, last_in_change; // These features affect OSDMap[::Incremental] encoding, or the // encoding of some type embedded therein (CrushWrapper, something // from osd_types, etc.). static constexpr uint64_t SIGNIFICANT_FEATURES = CEPH_FEATUREMASK_PGID64 | CEPH_FEATUREMASK_PGPOOL3 | CEPH_FEATUREMASK_OSDENC | CEPH_FEATUREMASK_OSDMAP_ENC | CEPH_FEATUREMASK_OSD_POOLRESEND | CEPH_FEATUREMASK_NEW_OSDOP_ENCODING | CEPH_FEATUREMASK_MSG_ADDR2 | CEPH_FEATUREMASK_CRUSH_TUNABLES5 | CEPH_FEATUREMASK_CRUSH_CHOOSE_ARGS | CEPH_FEATUREMASK_SERVER_LUMINOUS | CEPH_FEATUREMASK_SERVER_MIMIC | CEPH_FEATUREMASK_SERVER_NAUTILUS; struct addrs_s &#123; mempool::osdmap::vector&lt;std::shared_ptr&lt;entity_addrvec_t&gt; &gt; client_addrs; mempool::osdmap::vector&lt;std::shared_ptr&lt;entity_addrvec_t&gt; &gt; cluster_addrs; mempool::osdmap::vector&lt;std::shared_ptr&lt;entity_addrvec_t&gt; &gt; hb_back_addrs; mempool::osdmap::vector&lt;std::shared_ptr&lt;entity_addrvec_t&gt; &gt; hb_front_addrs; &#125;; std::shared_ptr&lt;addrs_s&gt; osd_addrs; //OSD的地址 entity_addrvec_t _blank_addrvec; mempool::osdmap::vector&lt;__u32&gt; osd_weight; //OSD的权重 16.16 fixed point, 0x10000 = &quot;in&quot;, 0 = &quot;out&quot; mempool::osdmap::vector&lt;osd_info_t&gt; osd_info; //OSD 的基本信息 std::shared_ptr&lt; mempool::osdmap::vector&lt;uuid_d&gt; &gt; osd_uuid; //OSD对应的uuid mempool::osdmap::vector&lt;osd_xinfo_t&gt; osd_xinfo; //OSD一些扩展信息 //PG相关的信息 std::shared_ptr&lt;PGTempMap&gt; pg_temp; // temp pg mapping (e.g. while we rebuild) std::shared_ptr&lt; mempool::osdmap::map&lt;pg_t,int32_t &gt; &gt; primary_temp; // temp primary mapping (e.g. while we rebuild) std::shared_ptr&lt; mempool::osdmap::vector&lt;__u32&gt; &gt; osd_primary_affinity; ///&lt; 16.16 fixed point, 0x10000 = baseline // remap (post-CRUSH, pre-up) mempool::osdmap::map&lt;pg_t,mempool::osdmap::vector&lt;int32_t&gt;&gt; pg_upmap; ///&lt; remap pg mempool::osdmap::map&lt;pg_t,mempool::osdmap::vector&lt;pair&lt;int32_t,int32_t&gt;&gt;&gt; pg_upmap_items; ///&lt; remap osds in up set //pool的相关信息 mempool::osdmap::map&lt;int64_t,pg_pool_t&gt; pools; //pool的id到pg_pool_t的映射 mempool::osdmap::map&lt;int64_t,string&gt; pool_name; //pool的id到pool的名字的映射 mempool::osdmap::map&lt;string,map&lt;string,string&gt; &gt; erasure_code_profiles; //pool的EC相关信息 mempool::osdmap::map&lt;string,int64_t&gt; name_pool; //pool的名字到pool的id的映射 Op结构体Op封装了完成一个操作的相关上下文信息，包括target地址信息(op_target_t)、链接信息(session)等 123456789101112131415161718192021222324252627282930313233343536//Op封装了完成一个操作的相关的上下文信息，包括target地址信息、链接信息等。 struct Op : public RefCountedObject &#123; OSDSession *session; //OSD相关的Session信息 int incarnation; //引用次数 op_target_t target; //地址信息 ConnectionRef con; // for rx buffer only uint64_t features; // explicitly specified op features vector&lt;OSDOp&gt; ops; // 对应多个操作的封装 snapid_t snapid; //快照的ID SnapContext snapc; //pool层级的快照信息 ceph::real_time mtime; bufferlist *outbl; //输出的bufferlist vector&lt;bufferlist*&gt; out_bl; //每个操作对应的bufferlist vector&lt;Context*&gt; out_handler; //每个操作对应的回调函数 vector&lt;int*&gt; out_rval; //每个操作对应的输出结果 int priority; Context *onfinish; uint64_t ontimeout; ceph_tid_t tid; int attempts; version_t *objver; epoch_t *reply_epoch; ceph::coarse_mono_time stamp; epoch_t map_dne_bound; int budget; /// true if we should resend this message on failure bool should_resend; /// true if the throttle budget is get/put on a series of OPs, /// instead of per OP basis, when this flag is set, the budget is /// acquired before sending the very first OP of the series and /// released upon receiving the last OP reply. bool ctx_budgeted; int *data_offset; osd_reqid_t reqid; // explicitly setting reqid ZTracer::Trace trace; op_target_t数据结构op_target_t封装了对象所在的PG，以及PG对应的OSD列表等地址信息。 12345678910111213141516171819202122232425262728293031//封装了对象所在的PG，以及PG对应的OSD列表等地址信息 struct op_target_t &#123; int flags = 0; //标志 epoch_t epoch = 0; ///&lt; latest epoch we calculated the mapping object_t base_oid; //读取的对象 object_locator_t base_oloc; //对象的pool信息 object_t target_oid; //最终读取的目标对象 object_locator_t target_oloc; //最终目标对象的pool信息 ///&lt; true if we are directed at base_pgid, not base_oid bool precalc_pgid = false; ///&lt; true if we have ever mapped to a valid pool bool pool_ever_existed = false; ///&lt; explcit pg target, if any pg_t base_pgid; pg_t pgid; ///&lt; last (raw) pg we mapped to spg_t actual_pgid; ///&lt; last (actual) spg_t we mapped to unsigned pg_num = 0; ///&lt; last pg_num we mapped to unsigned pg_num_mask = 0; ///&lt; last pg_num_mask we mapped to unsigned pg_num_pending = 0; ///&lt; last pg_num we mapped to vector&lt;int&gt; up; ///&lt; set of up osds for last pg we mapped to vector&lt;int&gt; acting; ///&lt; set of acting osds for last pg we mapped to int up_primary = -1; ///&lt; last up_primary we mapped to int acting_primary = -1; ///&lt; last acting_primary we mapped to int size = -1; ///&lt; the size of the pool when were were last mapped int min_size = -1; ///&lt; the min size of the pool when were were last mapped bool sort_bitwise = false; ///&lt; whether the hobject_t sort order is bitwise bool recovery_deletes = false; ///&lt; whether the deletes are performed during recovery instead of peering bool used_replica = false; bool paused = false; int osd = -1; ///&lt; the final target osd, or -1 epoch_t last_force_resend = 0; CRUSH Map123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146struct crush_rule_step &#123; __u32 op; //操作类型 __s32 arg1; //操作数1 __s32 arg2; //操作数2&#125;;enum crush_opcodes &#123; CRUSH_RULE_NOOP = 0, CRUSH_RULE_TAKE = 1, /* arg1 = value to start with */ CRUSH_RULE_CHOOSE_FIRSTN = 2, /* arg1 = num items to pick */ /* arg2 = type */ CRUSH_RULE_CHOOSE_INDEP = 3, /* same */ CRUSH_RULE_EMIT = 4, /* no args */ CRUSH_RULE_CHOOSELEAF_FIRSTN = 6, CRUSH_RULE_CHOOSELEAF_INDEP = 7, CRUSH_RULE_SET_CHOOSE_TRIES = 8, /* override choose_total_tries */ CRUSH_RULE_SET_CHOOSELEAF_TRIES = 9, /* override chooseleaf_descend_once */ CRUSH_RULE_SET_CHOOSE_LOCAL_TRIES = 10, CRUSH_RULE_SET_CHOOSE_LOCAL_FALLBACK_TRIES = 11, CRUSH_RULE_SET_CHOOSELEAF_VARY_R = 12, CRUSH_RULE_SET_CHOOSELEAF_STABLE = 13&#125;;/* * 用于指定相对于传递给 do_rule 的 max 参数的选择 num (arg1) */#define CRUSH_CHOOSE_N 0#define CRUSH_CHOOSE_N_MINUS(x) (-(x))/* * 规则掩码用于描述规则的用途。 * 给定规则集和输出集的大小，我们在规则列表中搜索匹配的 rule_mask。 */struct crush_rule_mask &#123; __u8 ruleset; //ruleId __u8 type; //多副本还是纠删码 __u8 min_size; //副本数大于等于时适用 __u8 max_size; //副本数小于等于时适用&#125;;struct crush_rule &#123; __u32 len; //steps数组的长度 struct crush_rule_mask mask; //releset相关的配置参数 struct crush_rule_step steps[0]; //step集合&#125;;#define crush_rule_size(len) (sizeof(struct crush_rule) + \\ (len)*sizeof(struct crush_rule_step))/* * A bucket is a named container of other items (either devices or * other buckets). * 桶是其他item（设备或其他存储桶）的命名容器 *//** * 使用三种算法中的一种来选择的，这些算法代表了性能和重组效率之间的权衡。 * 如果您不确定要使用哪种存储桶类型，我们建议您使用 ::CRUSH_BUCKET_STRAW2。 * 该表总结了在添加或删除item时每个选项的速度如何与映射稳定性相比较。 * Bucket Alg Speed Additions Removals * ------------------------------------------------ * uniform O(1) poor poor * list O(n) optimal poor * straw2 O(n) optimal optimal */enum crush_algorithm &#123; CRUSH_BUCKET_UNIFORM = 1, CRUSH_BUCKET_LIST = 2, CRUSH_BUCKET_TREE = 3, CRUSH_BUCKET_STRAW = 4, CRUSH_BUCKET_STRAW2 = 5,&#125;;extern const char *crush_bucket_alg_name(int alg);#define CRUSH_LEGACY_ALLOWED_BUCKET_ALGS ( \\ (1 &lt;&lt; CRUSH_BUCKET_UNIFORM) | \\ (1 &lt;&lt; CRUSH_BUCKET_LIST) | \\ (1 &lt;&lt; CRUSH_BUCKET_STRAW))struct crush_bucket &#123; __s32 id; //bucket的编号。小于0 /*!&lt; bucket identifier, &lt; 0 and unique within a crush_map */ __u16 type; //bucket的类型/*!&lt; &gt; 0 bucket type, defined by the caller */ __u8 alg; //使用的crush算法/*!&lt; the item selection ::crush_algorithm */ __u8 hash; //使用的hash算法/* which hash function to use, CRUSH_HASH_* */ __u32 weight; //权重 /*!&lt; 16.16 fixed point cumulated children weight */ __u32 size; //items的数量/*!&lt; size of the __items__ array */ __s32 *items; //子bucket/*!&lt; array of children: &lt; 0 are buckets, &gt;= 0 items */&#125;;struct crush_weight_set &#123; __u32 *weights; /*!&lt; 16.16 fixed point weights in the same order as items */ __u32 size; /*!&lt; size of the __weights__ array */&#125;;struct crush_choose_arg &#123; __s32 *ids; /*!&lt; values to use instead of items */ __u32 ids_size; /*!&lt; size of the __ids__ array */ struct crush_weight_set *weight_set; /*!&lt; weight replacements for a given position */ __u32 weight_set_positions; /*!&lt; size of the __weight_set__ array */&#125;;struct crush_choose_arg_map &#123; struct crush_choose_arg *args; /*!&lt; replacement for each bucket in the crushmap */ __u32 size; /*!&lt; size of the __args__ array */&#125;;struct crush_bucket_uniform &#123; struct crush_bucket h; /*!&lt; generic bucket information */ __u32 item_weight; /*!&lt; 16.16 fixed point weight for each item */&#125;;struct crush_bucket_list &#123; struct crush_bucket h; /*!&lt; generic bucket information */ __u32 *item_weights; /*!&lt; 16.16 fixed point weight for each item */ __u32 *sum_weights; /*!&lt; 16.16 fixed point sum of the weights */&#125;;struct crush_bucket_tree &#123; struct crush_bucket h; /* note: h.size is _tree_ size, not number of actual items */ __u8 num_nodes; __u32 *node_weights;&#125;;struct crush_bucket_straw &#123; struct crush_bucket h; __u32 *item_weights; /* 16-bit fixed point */ __u32 *straws; /* 16-bit fixed point */&#125;;struct crush_bucket_straw2 &#123; struct crush_bucket h; /*!&lt; generic bucket information */ __. /*!&lt; 16.16 fixed point weight for each item */&#125;;struct crush_map &#123; struct crush_bucket **buckets; **类型，所有的bucket都存在这里 /*! 一个大小为__max_rules__ 的crush_rule 指针数组。 * 如果规则被删除，数组的一个元素可能为NULL（没有API 可以这样做，但将来可能会有一个）。 * 规则必须使用crunch_add_rule() 添加。 */ struct crush_rule **rules; //**类型，多层嵌套的rules __s32 max_buckets; /*!&lt; the size of __buckets__ */ // bucket的总数 __u32 max_rules; /*!&lt; the size of __rules__ */ // rule的总数 __s32 max_devices; // osd的总数 __u32 choose_local_tries; //选择的总次数 __u32 choose_local_fallback_tries; __u32 choose_total_tries; __u32 chooseleaf_descend_once; __u8 chooseleaf_vary_r; __u8 chooseleaf_stable; /* 该值是在构建器解码或构建后计算的。 它在此处公开（而不是具有“构建 CRUSH 工作空间”功能），以便调用者可以保留静态缓冲区、在堆栈上分配空间，或者在需要时避免调用堆分配器。 工作空间的大小取决于映射，而传递给映射器的临时向量的大小取决于所需结果集的大小。尽管如此，没有什么能阻止调用者在一个膨胀 foop 中分配两个点并传递两个点。 */ size_t working_size;","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Hello World","slug":"hello-world","date":"2023-07-11T15:09:26.000Z","updated":"2024-07-13T15:58:33.946Z","comments":true,"path":"hello-world/","permalink":"https://watsonlu6.github.io/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"},{"name":"存储基础","slug":"云存储/存储基础","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"},{"name":"存储基础","slug":"存储基础","permalink":"https://watsonlu6.github.io/tags/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}]}